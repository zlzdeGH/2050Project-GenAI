{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61230e8e",
   "metadata": {},
   "source": [
    "## Keyword Strategy Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d8a82fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Found 2 AI Policy Paragraphs ---\n",
      "\n",
      "1. Paragraph:\n",
      "   'The use of generative AI tools like ChatGPT is permitted for brainstorming, but all submitted work must be original.'\n",
      "   Reason for extraction: Matched a primary keyword.\n",
      "\n",
      "2. Paragraph:\n",
      "   'Students must cite any assistance from AI, as failure to do so is a violation of academic integrity.'\n",
      "   Reason for extraction: Matched the combination of AI-context and policy keywords.\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "PRIMARY_KEYWORDS = [\n",
    "    'ai policy', 'aigenerated content', 'ai-generated', 'ai-assisted',\n",
    "    'generative ai', 'llm',\n",
    "]\n",
    "AI_CONTEXT_WORDS = [\n",
    "    'ai', 'artificial intelligence', 'generative', 'chatgpt', 'llm', 'copilot', 'bard', \n",
    "    'large language model' ,\n",
    "    'gemini', 'dall-e', 'gpt', 'midjourney', 'stable diffusion', 'ai tool', 'ai tools'\n",
    "]\n",
    "POLICY_KEYWORDS = [\n",
    "    'academic integrity', 'academic dishonesty', 'plagiarism', 'cheating', 'unauthorized',\n",
    "    'unauthorized use', 'unauthorized assistance', 'citation', 'cite', 'attribution',\n",
    "    'acknowledge', 'permitted', 'allowed', 'prohibited', 'forbidden', 'disclosure', \n",
    "    'ethical use', 'responsible use', 'use', 'using', 'assistance', 'help', 'guidance', \n",
    "    'policy', 'rule'\n",
    "]\n",
    "\n",
    "def find_ai_policy_paragraphs(paragraphs):\n",
    "    \"\"\"\n",
    "    KEEP a paragraph if it contains\n",
    "    a PRIMARY_KEYWORD\n",
    "    OR\n",
    "    (an AI_CONTEXT_WORD AND a POLICY_KEYWORD)\n",
    "    \"\"\"\n",
    "    found_policies = []\n",
    "    \n",
    "    # --- Pre-compile regex patterns for efficiency ---\n",
    "    ai_context_patterns = [\n",
    "        re.compile(r'\\b' + re.escape(kw) + r'\\b', re.IGNORECASE)\n",
    "        for kw in AI_CONTEXT_WORDS\n",
    "    ]\n",
    "\n",
    "    for para in paragraphs:\n",
    "        # Rule A: check PRIMARY keywords (mostly multi-word phrases) ---\n",
    "        para_lower = para.lower()\n",
    "        if any(kw in para_lower for kw in PRIMARY_KEYWORDS):\n",
    "            found_policies.append({\n",
    "                'text': para,\n",
    "                'reason': 'Matched a primary keyword.'\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Rule B: require an AI-context word via regex ---\n",
    "        contains_ai_context = any(pattern.search(para) for pattern in ai_context_patterns)\n",
    "        if not contains_ai_context:\n",
    "            continue\n",
    "            \n",
    "        # Policy keywords check (simple substring search is fine here)\n",
    "        contains_policy_context = any(kw in para_lower for kw in POLICY_KEYWORDS)\n",
    "\n",
    "        if contains_ai_context and contains_policy_context:\n",
    "            found_policies.append({\n",
    "                'text': para,\n",
    "                'reason': 'Matched the combination of AI-context and policy keywords.'\n",
    "            })\n",
    "            \n",
    "    return found_policies\n",
    "\n",
    "sample_paragraphs = [\n",
    "    \"The use of generative AI tools like ChatGPT is permitted for brainstorming, but all submitted work must be original.\",\n",
    "    \"Academic dishonesty, including plagiarism and cheating on exams, will result in a failing grade for the course.\",\n",
    "    \"Students must cite any assistance from AI, as failure to do so is a violation of academic integrity.\",\n",
    "    \"This course will include a lecture on the history of artificial intelligence and its impact on modern society.\",\n",
    "    \"All sources must be properly cited in APA format. Failure to attribute your sources constitutes plagiarism.\"\n",
    "]\n",
    "\n",
    "extracted_policies = find_ai_policy_paragraphs(sample_paragraphs)\n",
    "\n",
    "# --- Print results ---\n",
    "print(f\"--- Found {len(extracted_policies)} AI Policy Paragraphs ---\")\n",
    "for i, policy in enumerate(extracted_policies, 1):\n",
    "    print(f\"\\n{i}. Paragraph:\")\n",
    "    print(f\"   '{policy['text']}'\")\n",
    "    print(f\"   Reason for extraction: {policy['reason']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef37f20",
   "metadata": {},
   "source": [
    "## Final version from Qinjunjie\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eabb2eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Analyzing Syllabus: 8C2WFwQPrq6vcFU0Yml3vHcL1CYb5HnYcxp5s7A5.docx ====================\n",
      "Error: File not found.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==================== Analyzing Syllabus: UMeRwRyvXAKAH6DwCkTmFyIunq3ti97bJPARlu7C.docx ====================\n",
      "Error: File not found.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==================== Analyzing Syllabus: 0Fslu7lGZ8dJG0OYQGf1TgzFMPyFDEv0n5Q96BNq.pdf ====================\n",
      "Error: File not found.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==================== Analyzing Syllabus: 7Y6H93I4L6F5bcplrUpPbf5AjQcypbJfrevTSiNf.doc ====================\n",
      "Error: File not found.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import sys \n",
    "from docx import Document\n",
    "import pdfplumber\n",
    "# import pytesseract\n",
    "# from pdf2image import convert_from_path\n",
    "\n",
    "# --- For Windows users, you may need to specify the path to the Tesseract executable ---\n",
    "# Uncomment and update the line below if you are on Windows and Tesseract is not in your PATH\n",
    "# pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "# Conditionally import the library for .doc files, only on Windows\n",
    "if sys.platform == 'win32':\n",
    "    try:\n",
    "        import win32com.client as win32\n",
    "    except ImportError:\n",
    "        print(\"Warning: The 'pywin32' library is not installed. .doc files cannot be processed.\")\n",
    "        print(\"To enable .doc support on Windows, run: pip install pywin32\")\n",
    "        win32 = None\n",
    "else:\n",
    "    win32 = None\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 1: KEYWORD DEFINITIONS & MAPPINGS\n",
    "# ==============================================================================\n",
    "PRIMARY_KEYWORDS = ['ai policy', 'aigenerated content', 'ai-generated', 'ai-assisted', 'generative ai']\n",
    "AI_CONTEXT_WORDS = ['ai', 'artificial intelligence', 'generative', 'chatgpt', 'llm', 'copilot', 'bard', 'large language model', 'gemini', 'dall-e', 'gpt', 'midjourney', 'stable diffusion', 'ai tool', 'ai tools']\n",
    "POLICY_KEYWORDS = ['academic integrity', 'academic dishonesty', 'plagiarism', 'cheating', 'unauthorized', 'unauthorized use', 'unauthorized assistance', 'citation', 'cite', 'attribution', 'acknowledge', 'permitted', 'allowed', 'prohibited', 'forbidden', 'disclosure', 'ethical use', 'responsible use', 'use', 'using', 'assistance', 'help', 'guidance', 'policy', 'rule']\n",
    "\n",
    "DEPARTMENT_MAP = {\n",
    "    'Africana Studies': 'AFRI', 'American Studies': ['AMST', 'ETHN'], 'Anthropology': 'ANTH',\n",
    "    'Economics': 'ECON', 'Computer Science': 'CSCI', 'Applied Mathematics': 'APMA',\n",
    "    'Public Health': 'PHP', \n",
    "    'Cognitive, Linguistic, and Psychological Sciences': 'CLPS',\n",
    "}\n",
    "\n",
    "def build_checkers():\n",
    "    primary_checker = re.compile(r'|'.join([kw.replace(' ', r'\\s*') for kw in PRIMARY_KEYWORDS]), re.IGNORECASE)\n",
    "    ai_checker = re.compile(r'\\b(' + '|'.join(AI_CONTEXT_WORDS) + r')\\b', re.IGNORECASE)\n",
    "    policy_checker = re.compile(r'\\b(' + '|'.join(POLICY_KEYWORDS) + r')\\b', re.IGNORECASE)\n",
    "    return primary_checker, ai_checker, policy_checker\n",
    "\n",
    "PRIMARY_CHECKER, AI_CHECKER, POLICY_CHECKER = build_checkers()\n",
    "\n",
    "def is_policy_text(text):\n",
    "    if PRIMARY_CHECKER.search(text): return 'Matched a primary keyword.'\n",
    "    if AI_CHECKER.search(text) and POLICY_CHECKER.search(text): return 'Matched the combination of AI-context and policy keywords.'\n",
    "    return None\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 2: TEXT EXTRACTION (NOW WITH .DOC SUPPORT)\n",
    "# ==============================================================================\n",
    "def extract_paragraphs_from_doc(doc_path):\n",
    "    \"\"\"\n",
    "    Extracts paragraphs from a .doc file using MS Word automation (Windows only).\n",
    "    \"\"\"\n",
    "    if not win32:\n",
    "        print(f\"Skipping .doc file '{os.path.basename(doc_path)}' as 'pywin32' is not available on this system.\")\n",
    "        print(\"Please manually save it as .docx or .pdf to analyze.\")\n",
    "        return []\n",
    "    \n",
    "    word = None\n",
    "    doc = None\n",
    "    try:\n",
    "        word = win32.Dispatch(\"Word.Application\")\n",
    "        word.Visible = False\n",
    "        # Get the full absolute path, which COM objects often require\n",
    "        abs_path = os.path.abspath(doc_path)\n",
    "        doc = word.Documents.Open(abs_path)\n",
    "        paragraphs = [p.Range.Text.strip() for p in doc.Paragraphs if p.Range.Text.strip()]\n",
    "        return paragraphs\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing .doc file with MS Word: {e}\")\n",
    "        return []\n",
    "    finally:\n",
    "        if doc:\n",
    "            doc.Close(False) # Close the document without saving changes\n",
    "        if word:\n",
    "            word.Quit() # Quit the Word application\n",
    "\n",
    "def extract_paragraphs_from_docx(doc_path):\n",
    "    try:\n",
    "        doc = Document(doc_path); blocks = [p.text for p in doc.paragraphs if p.text.strip()]\n",
    "        for table in doc.tables:\n",
    "            for row in table.rows:\n",
    "                for cell in row.cells:\n",
    "                    if cell.text.strip(): blocks.append(cell.text)\n",
    "        return blocks\n",
    "    except Exception as e: print(f\"Error reading DOCX file {doc_path}: {e}\"); return []\n",
    "\n",
    "def _reconstruct_paragraphs_from_page(page):\n",
    "    \"\"\"Helper function to reconstruct paragraphs on a single page using coordinates.\"\"\"\n",
    "    words = page.extract_words(keep_blank_chars=False, x_tolerance=2)\n",
    "    if not words: return []\n",
    "    lines = {};\n",
    "    for word in words:\n",
    "        line_top = round(word['top'], 2)\n",
    "        if line_top not in lines: lines[line_top] = []\n",
    "        lines[line_top].append(word)\n",
    "    for line_top in lines: lines[line_top].sort(key=lambda w: w['x0'])\n",
    "    sorted_lines = sorted(lines.items(), key=lambda item: item[0])\n",
    "    reconstructed_lines = []; line_heights = []; last_top = None\n",
    "    for top, words_in_line in sorted_lines:\n",
    "        text = \" \".join(w['text'] for w in words_in_line)\n",
    "        reconstructed_lines.append({'top': top, 'text': text})\n",
    "        if last_top is not None: line_heights.append(top - last_top)\n",
    "        last_top = top\n",
    "    if not reconstructed_lines: return []\n",
    "    avg_line_height = sum(line_heights) / len(line_heights) if line_heights else 12\n",
    "    paragraph_break_threshold = avg_line_height * 1.5\n",
    "    page_paragraphs = []; current_paragraph = reconstructed_lines[0]['text']\n",
    "    for i in range(1, len(reconstructed_lines)):\n",
    "        prev_line, curr_line = reconstructed_lines[i-1], reconstructed_lines[i]\n",
    "        if (curr_line['top'] - prev_line['top']) > paragraph_break_threshold:\n",
    "            page_paragraphs.append(current_paragraph)\n",
    "            current_paragraph = curr_line['text']\n",
    "        else:\n",
    "            current_paragraph += \" \" + curr_line['text']\n",
    "    page_paragraphs.append(current_paragraph)\n",
    "    return page_paragraphs\n",
    "\n",
    "def extract_paragraphs_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    FINAL PDF METHOD: Uses coordinate geometry and cross-page stitching\n",
    "    to perfectly reconstruct all paragraphs, even those split across pages.\n",
    "    \"\"\"\n",
    "    all_paragraphs = []\n",
    "    carry_over_paragraph = \"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                paragraphs_on_page = _reconstruct_paragraphs_from_page(page)\n",
    "                if not paragraphs_on_page: continue\n",
    "\n",
    "                # If there's a carry-over from the previous page, stitch it to the first paragraph\n",
    "                if carry_over_paragraph:\n",
    "                    paragraphs_on_page[0] = carry_over_paragraph + \" \" + paragraphs_on_page[0]\n",
    "                    carry_over_paragraph = \"\"\n",
    "\n",
    "                # Check if the last paragraph on THIS page is incomplete\n",
    "                last_para = paragraphs_on_page[-1]\n",
    "                # A simple but effective heuristic: if it doesn't end with punctuation, it's likely incomplete.\n",
    "                if not last_para.strip().endswith(('.', '?', '!', '\"', \"'\", ')', ':', ';')):\n",
    "                    carry_over_paragraph = paragraphs_on_page.pop()\n",
    "\n",
    "                all_paragraphs.extend(paragraphs_on_page)\n",
    "        \n",
    "        # Add any final carry-over from the very last page\n",
    "        if carry_over_paragraph:\n",
    "            all_paragraphs.append(carry_over_paragraph)\n",
    "\n",
    "        if all_paragraphs: print(\"Successfully extracted paragraphs using coordinate-based method.\"); return all_paragraphs\n",
    "        print(\"Coordinate-based method failed. Attempting OCR as last resort...\")\n",
    "    except Exception as e: print(f\"Coordinate-based parsing failed: {e}. Attempting OCR...\")\n",
    "    # try:\n",
    "    #     images = convert_from_path(pdf_path); full_text = \"\"\n",
    "    #     for image in images: full_text += pytesseract.image_to_string(image) + \"\\n\\n\"\n",
    "    #     return [p.strip().replace('\\n', ' ') for p in full_text.split('\\n\\n') if p.strip()]\n",
    "    # except Exception as e: print(f\"OCR processing failed: {e}\"); return []\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 3: ANALYSIS LOGIC\n",
    "# ==============================================================================\n",
    "def find_course_code(paragraphs, search_limit=30):\n",
    "    for long_name, short_code in DEPARTMENT_MAP.items():\n",
    "        pattern = re.compile(r'\\b' + re.escape(long_name) + r'\\s*(\\d{3,4}[A-Z]?)\\b', re.IGNORECASE)\n",
    "        for para in paragraphs[:search_limit]:\n",
    "            if match := pattern.search(para): return f\"{short_code} {match.group(1)}\"\n",
    "    fallback_pattern = re.compile(r'\\b(([A-Z]{2,4}(\\s*/\\s*[A-Z]{2,4})*))\\s*(\\d{3,4}[A-Z]?)\\b')\n",
    "    for para in paragraphs[:search_limit]:\n",
    "        if match := fallback_pattern.search(para):\n",
    "            dept_part = match.group(1).replace(\" \", \"\"); num_part = match.group(4)\n",
    "            return f\"{dept_part} {num_part}\"\n",
    "    return None\n",
    "\n",
    "def analyze_ai_policy(paragraphs):\n",
    "    found_policies = []\n",
    "    for para in paragraphs:\n",
    "        if reason := is_policy_text(para):\n",
    "            found_policies.append({'text': para, 'reason': reason})\n",
    "    return found_policies\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 4: MAIN CONTROLLER\n",
    "# ==============================================================================\n",
    "def analyze_syllabus(file_path):\n",
    "    print(f\"\\n{'='*20} Analyzing Syllabus: {os.path.basename(file_path)} {'='*20}\")\n",
    "    if not os.path.exists(file_path): print(\"Error: File not found.\"); return\n",
    "\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    paragraphs = []\n",
    "    if ext == '.docx': paragraphs = extract_paragraphs_from_docx(file_path)\n",
    "    elif ext == '.pdf': paragraphs = extract_paragraphs_from_pdf(file_path)\n",
    "    elif ext == '.doc': paragraphs = extract_paragraphs_from_doc(file_path) \n",
    "    else: print(f\"Error: Unsupported file type '{ext}'.\"); return\n",
    "\n",
    "    if not paragraphs: print(\"Could not extract any usable text.\"); return\n",
    "    \n",
    "    print(f\"Extracted {len(paragraphs)} distinct paragraphs. Analyzing...\")\n",
    "    course_code = find_course_code(paragraphs)\n",
    "    print(f\"--- Course Code: {course_code if course_code else 'Not Found'} ---\")\n",
    "\n",
    "    ai_policy_sections = analyze_ai_policy(paragraphs)\n",
    "    if not ai_policy_sections:\n",
    "        print(\"\\n--- No AI policy paragraphs were found in this document. ---\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n--- SUCCESS: Found {len(ai_policy_sections)} AI Policy Paragraph(s) ---\\n\")\n",
    "    unique_policies = {p['text']: p for p in ai_policy_sections}.values()\n",
    "    for i, policy in enumerate(unique_policies, 1):\n",
    "        print(f\"--- Relevant Paragraph {i} ---\"); print(policy['text']); print(f\"(Reason: {policy['reason']})\\n\")\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 5: EXECUTION\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    docx_file1 = \"8C2WFwQPrq6vcFU0Yml3vHcL1CYb5HnYcxp5s7A5.docx\" \n",
    "    analyze_syllabus(docx_file1)\n",
    "    print(\"\\n\" * 3)\n",
    "\n",
    "    docx_file2 = \"UMeRwRyvXAKAH6DwCkTmFyIunq3ti97bJPARlu7C.docx\"\n",
    "    analyze_syllabus(docx_file2)\n",
    "    print(\"\\n\" * 3) \n",
    "\n",
    "    pdf_file = \"0Fslu7lGZ8dJG0OYQGf1TgzFMPyFDEv0n5Q96BNq.pdf\" \n",
    "    analyze_syllabus(pdf_file)\n",
    "    print(\"\\n\" * 3)\n",
    "\n",
    "    doc_file = \"7Y6H93I4L6F5bcplrUpPbf5AjQcypbJfrevTSiNf.doc\"\n",
    "    analyze_syllabus(doc_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2570793a",
   "metadata": {},
   "source": [
    "## ADD Department (My First Version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ba2483d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Analyzing Syllabus: 8C2WFwQPrq6vcFU0Yml3vHcL1CYb5HnYcxp5s7A5.docx ====================\n",
      "Error: File not found.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==================== Analyzing Syllabus: UMeRwRyvXAKAH6DwCkTmFyIunq3ti97bJPARlu7C.docx ====================\n",
      "Error: File not found.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==================== Analyzing Syllabus: 0Fslu7lGZ8dJG0OYQGf1TgzFMPyFDEv0n5Q96BNq.pdf ====================\n",
      "Error: File not found.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==================== Analyzing Syllabus: 7Y6H93I4L6F5bcplrUpPbf5AjQcypbJfrevTSiNf.doc ====================\n",
      "Error: File not found.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "from docx import Document\n",
    "import pdfplumber\n",
    "\n",
    "# Conditionally import the library for .doc files, only on Windows\n",
    "if sys.platform == 'win32':\n",
    "    try:\n",
    "        import win32com.client as win32\n",
    "    except ImportError:\n",
    "        print(\"Warning: The 'pywin32' library is not installed. .doc files cannot be processed.\")\n",
    "        print(\"To enable .doc support on Windows, run: pip install pywin32\")\n",
    "        win32 = None\n",
    "else:\n",
    "    win32 = None\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 1: KEYWORD DEFINITIONS & MAPPINGS\n",
    "# ==============================================================================\n",
    "PRIMARY_KEYWORDS = ['ai policy', 'aigenerated content', 'ai-generated', 'ai-assisted', 'generative ai']\n",
    "AI_CONTEXT_WORDS = [\n",
    "    'ai', 'artificial intelligence', 'generative', 'chatgpt', 'llm', 'copilot',\n",
    "    'bard', 'large language model', 'gemini', 'dall-e', 'gpt', 'midjourney',\n",
    "    'stable diffusion', 'ai tool', 'ai tools'\n",
    "]\n",
    "POLICY_KEYWORDS = [\n",
    "    'academic integrity', 'academic dishonesty', 'plagiarism', 'cheating', 'unauthorized',\n",
    "    'unauthorized use', 'unauthorized assistance', 'citation', 'cite', 'attribution',\n",
    "    'acknowledge', 'permitted', 'allowed', 'prohibited', 'forbidden', 'disclosure',\n",
    "    'ethical use', 'responsible use', 'use', 'using', 'assistance', 'help', 'guidance',\n",
    "    'policy', 'rule'\n",
    "]\n",
    "\n",
    "DEPARTMENT_MAP = {\n",
    "    'Africana Studies': 'AFRI',\n",
    "    'American Studies': ['AMST', 'ETHN'],\n",
    "    'Anthropology': 'ANTH',\n",
    "    'Economics': 'ECON',\n",
    "    'Computer Science': 'CSCI',\n",
    "    'Applied Mathematics': 'APMA',\n",
    "    'Public Health': 'PHP',\n",
    "    'Cognitive, Linguistic, and Psychological Sciences': 'CLPS',\n",
    "}\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# build reverse lookup: short code → full department name\n",
    "CODE_TO_DEPT = {}\n",
    "for long_name, short in DEPARTMENT_MAP.items():\n",
    "    if isinstance(short, list):\n",
    "        for s in short:\n",
    "            CODE_TO_DEPT[s] = long_name\n",
    "    else:\n",
    "        CODE_TO_DEPT[short] = long_name\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "def build_checkers():\n",
    "    primary_checker = re.compile(\n",
    "        r'|'.join([kw.replace(' ', r'\\s*') for kw in PRIMARY_KEYWORDS]),\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    ai_checker = re.compile(\n",
    "        r'\\b(' + '|'.join(AI_CONTEXT_WORDS) + r')\\b',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    policy_checker = re.compile(\n",
    "        r'\\b(' + '|'.join(POLICY_KEYWORDS) + r')\\b',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    return primary_checker, ai_checker, policy_checker\n",
    "\n",
    "PRIMARY_CHECKER, AI_CHECKER, POLICY_CHECKER = build_checkers()\n",
    "\n",
    "def is_policy_text(text):\n",
    "    if PRIMARY_CHECKER.search(text):\n",
    "        return 'Matched a primary keyword.'\n",
    "    if AI_CHECKER.search(text) and POLICY_CHECKER.search(text):\n",
    "        return 'Matched the combination of AI-context and policy keywords.'\n",
    "    return None\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 2: TEXT EXTRACTION\n",
    "# ==============================================================================\n",
    "\n",
    "def extract_paragraphs_from_doc(doc_path):\n",
    "    if not win32:\n",
    "        print(f\"Skipping .doc file '{os.path.basename(doc_path)}' as 'pywin32' is not available on this system.\")\n",
    "        return []\n",
    "    word = None\n",
    "    doc = None\n",
    "    try:\n",
    "        word = win32.Dispatch(\"Word.Application\")\n",
    "        word.Visible = False\n",
    "        abs_path = os.path.abspath(doc_path)\n",
    "        doc = word.Documents.Open(abs_path)\n",
    "        paragraphs = [p.Range.Text.strip() for p in doc.Paragraphs if p.Range.Text.strip()]\n",
    "        return paragraphs\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing .doc file: {e}\")\n",
    "        return []\n",
    "    finally:\n",
    "        if doc:\n",
    "            doc.Close(False)\n",
    "        if word:\n",
    "            word.Quit()\n",
    "\n",
    "\n",
    "def extract_paragraphs_from_docx(doc_path):\n",
    "    try:\n",
    "        doc = Document(doc_path)\n",
    "        blocks = [p.text for p in doc.paragraphs if p.text.strip()]\n",
    "        for table in doc.tables:\n",
    "            for row in table.rows:\n",
    "                for cell in row.cells:\n",
    "                    if cell.text.strip():\n",
    "                        blocks.append(cell.text)\n",
    "        return blocks\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading DOCX file {doc_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def _reconstruct_paragraphs_from_page(page):\n",
    "    words = page.extract_words(keep_blank_chars=False, x_tolerance=2)\n",
    "    if not words:\n",
    "        return []\n",
    "    lines = {}\n",
    "    for word in words:\n",
    "        line_top = round(word['top'], 2)\n",
    "        lines.setdefault(line_top, []).append(word)\n",
    "    for line_top in lines:\n",
    "        lines[line_top].sort(key=lambda w: w['x0'])\n",
    "    sorted_lines = sorted(lines.items(), key=lambda item: item[0])\n",
    "    reconstructed_lines = []\n",
    "    last_top = None\n",
    "    line_heights = []\n",
    "    for top, ws in sorted_lines:\n",
    "        text = \" \".join(w['text'] for w in ws)\n",
    "        reconstructed_lines.append({'top': top, 'text': text})\n",
    "        if last_top is not None:\n",
    "            line_heights.append(top - last_top)\n",
    "        last_top = top\n",
    "    avg_height = sum(line_heights) / len(line_heights) if line_heights else 12\n",
    "    threshold = avg_height * 1.5\n",
    "    paragraphs = []\n",
    "    current = reconstructed_lines[0]['text']\n",
    "    for prev, curr in zip(reconstructed_lines, reconstructed_lines[1:]):\n",
    "        if (curr['top'] - prev['top']) > threshold:\n",
    "            paragraphs.append(current)\n",
    "            current = curr['text']\n",
    "        else:\n",
    "            current += ' ' + curr['text']\n",
    "    paragraphs.append(current)\n",
    "    return paragraphs\n",
    "\n",
    "\n",
    "def extract_paragraphs_from_pdf(pdf_path):\n",
    "    all_paras = []\n",
    "    carry = \"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                paras = _reconstruct_paragraphs_from_page(page)\n",
    "                if not paras:\n",
    "                    continue\n",
    "                if carry:\n",
    "                    paras[0] = carry + ' ' + paras[0]\n",
    "                    carry = \"\"\n",
    "                last = paras[-1]\n",
    "                if not last.strip().endswith(('.', '?', '!', '\"', \"'\", ')', ':', ';')):\n",
    "                    carry = paras.pop()\n",
    "                all_paras.extend(paras)\n",
    "        if carry:\n",
    "            all_paras.append(carry)\n",
    "        if all_paras:\n",
    "            print(\"Successfully extracted paragraphs using coordinate-based method.\")\n",
    "            return all_paras\n",
    "        print(\"Coordinate-based method failed. No text extracted.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Coordinate-based parsing failed: {e}\")\n",
    "    return []\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 3: ANALYSIS LOGIC\n",
    "# ==============================================================================\n",
    "def find_course_code(paragraphs, search_limit=30):\n",
    "    for long_name, short_code in DEPARTMENT_MAP.items():\n",
    "        pattern = re.compile(r'\\b' + re.escape(long_name) + r'\\s*(\\d{3,4}[A-Z]?)\\b', re.IGNORECASE)\n",
    "        for para in paragraphs[:search_limit]:\n",
    "            if m := pattern.search(para):\n",
    "                return f\"{short_code} {m.group(1)}\"\n",
    "    fallback = re.compile(r\"\\b(([A-Z]{2,4}(\\s*/\\s*[A-Z]{2,4})*))\\s*(\\d{3,4}[A-Z]?)\\b\")\n",
    "    for para in paragraphs[:search_limit]:\n",
    "        if m := fallback.search(para):\n",
    "            dept = m.group(1).replace(\" \", \"\")\n",
    "            num = m.group(4)\n",
    "            return f\"{dept} {num}\"\n",
    "    return None\n",
    "\n",
    "\n",
    "def analyze_ai_policy(paragraphs):\n",
    "    results = []\n",
    "    for para in paragraphs:\n",
    "        if reason := is_policy_text(para):\n",
    "            results.append({'text': para, 'reason': reason})\n",
    "    return results\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 4: MAIN CONTROLLER\n",
    "# ==============================================================================\n",
    "def analyze_syllabus(file_path):\n",
    "    print(f\"\\n{'='*20} Analyzing Syllabus: {os.path.basename(file_path)} {'='*20}\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(\"Error: File not found.\"); return\n",
    "\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    if ext == '.docx':\n",
    "        paragraphs = extract_paragraphs_from_docx(file_path)\n",
    "    elif ext == '.pdf':\n",
    "        paragraphs = extract_paragraphs_from_pdf(file_path)\n",
    "    elif ext == '.doc':\n",
    "        paragraphs = extract_paragraphs_from_doc(file_path)\n",
    "    else:\n",
    "        print(f\"Error: Unsupported file type '{ext}'.\"); return\n",
    "\n",
    "    if not paragraphs:\n",
    "        print(\"Could not extract any usable text.\"); return\n",
    "\n",
    "    print(f\"Extracted {len(paragraphs)} distinct paragraphs. Analyzing...\")\n",
    "    course_code = find_course_code(paragraphs)\n",
    "    print(f\"--- Course Code: {course_code if course_code else 'Not Found'} ---\")\n",
    "\n",
    "    # NEW: print department name\n",
    "    if course_code:\n",
    "        prefix = course_code.split()[0]\n",
    "        dept_name = CODE_TO_DEPT.get(prefix, 'Unknown')\n",
    "        print(f\"--- Department: {dept_name} ---\")\n",
    "\n",
    "    policies = analyze_ai_policy(paragraphs)\n",
    "    if not policies:\n",
    "        print(\"\\n--- No AI policy paragraphs were found in this document. ---\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n--- SUCCESS: Found {len(policies)} AI Policy Paragraph(s) ---\\n\")\n",
    "    unique = {p['text']: p for p in policies}.values()\n",
    "    for i, p in enumerate(unique, 1):\n",
    "        print(f\"--- Relevant Paragraph {i} ---\")\n",
    "        print(p['text'])\n",
    "        print(f\"(Reason: {p['reason']})\\n\")\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 5: EXECUTION\n",
    "# ==============================================================================\n",
    "if __name__ == '__main__':\n",
    "    files = [\n",
    "        '8C2WFwQPrq6vcFU0Yml3vHcL1CYb5HnYcxp5s7A5.docx',\n",
    "        'UMeRwRyvXAKAH6DwCkTmFyIunq3ti97bJPARlu7C.docx',\n",
    "        '0Fslu7lGZ8dJG0OYQGf1TgzFMPyFDEv0n5Q96BNq.pdf',\n",
    "        '7Y6H93I4L6F5bcplrUpPbf5AjQcypbJfrevTSiNf.doc'\n",
    "    ]\n",
    "    for f in files:\n",
    "        analyze_syllabus(f)\n",
    "        print(\"\\n\" * 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd059dc3",
   "metadata": {},
   "source": [
    "## ADD Knowledge Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecedd196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Analyzing Syllabus: 8C2WFwQPrq6vcFU0Yml3vHcL1CYb5HnYcxp5s7A5.docx ====================\n",
      "Error: File not found.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==================== Analyzing Syllabus: UMeRwRyvXAKAH6DwCkTmFyIunq3ti97bJPARlu7C.docx ====================\n",
      "Error: File not found.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==================== Analyzing Syllabus: 0Fslu7lGZ8dJG0OYQGf1TgzFMPyFDEv0n5Q96BNq.pdf ====================\n",
      "Error: File not found.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==================== Analyzing Syllabus: 7Y6H93I4L6F5bcplrUpPbf5AjQcypbJfrevTSiNf.doc ====================\n",
      "Error: File not found.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "from docx import Document\n",
    "import pdfplumber\n",
    "\n",
    "# Conditionally import the library for .doc files, only on Windows\n",
    "if sys.platform == 'win32':\n",
    "    try:\n",
    "        import win32com.client as win32\n",
    "    except ImportError:\n",
    "        print(\"Warning: The 'pywin32' library is not installed. .doc files cannot be processed.\")\n",
    "        print(\"To enable .doc support on Windows, run: pip install pywin32\")\n",
    "        win32 = None\n",
    "else:\n",
    "    win32 = None\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 1: KEYWORD DEFINITIONS & MAPPINGS\n",
    "# ==============================================================================\n",
    "PRIMARY_KEYWORDS = ['ai policy', 'aigenerated content', 'ai-generated', 'ai-assisted', 'generative ai']\n",
    "AI_CONTEXT_WORDS = [\n",
    "    'ai', 'artificial intelligence', 'generative', 'chatgpt', 'llm', 'copilot',\n",
    "    'bard', 'large language model', 'gemini', 'dall-e', 'gpt', 'midjourney',\n",
    "    'stable diffusion', 'ai tool', 'ai tools'\n",
    "]\n",
    "POLICY_KEYWORDS = [\n",
    "    'academic integrity', 'academic dishonesty', 'plagiarism', 'cheating', 'unauthorized',\n",
    "    'unauthorized use', 'unauthorized assistance', 'citation', 'cite', 'attribution',\n",
    "    'acknowledge', 'permitted', 'allowed', 'prohibited', 'forbidden', 'disclosure',\n",
    "    'ethical use', 'responsible use', 'use', 'using', 'assistance', 'help', 'guidance',\n",
    "    'policy', 'rule'\n",
    "]\n",
    "\n",
    "DEPARTMENT_MAP = {\n",
    "    'Africana Studies': 'AFRI',\n",
    "    'American Studies': ['AMST', 'ETHN'],\n",
    "    'Anthropology': 'ANTH',\n",
    "    'Economics': 'ECON',\n",
    "    'Computer Science': 'CSCI',\n",
    "    'Applied Mathematics': 'APMA',\n",
    "    'Public Health': 'PHP',\n",
    "    'Cognitive, Linguistic, and Psychological Sciences': 'CLPS',\n",
    "}\n",
    "\n",
    "# build reverse lookup: short code → full department name\n",
    "CODE_TO_DEPT = {}\n",
    "for long_name, short in DEPARTMENT_MAP.items():\n",
    "    if isinstance(short, list):\n",
    "        for s in short:\n",
    "            CODE_TO_DEPT[s] = long_name\n",
    "    else:\n",
    "        CODE_TO_DEPT[short] = long_name\n",
    "\n",
    "# map each department to a knowledge area\n",
    "KNOWLEDGE_AREA_MAP = {\n",
    "    'Africana Studies': 'Humanities',\n",
    "    'American Studies': 'Humanities',\n",
    "    'Anthropology': 'Social Science',\n",
    "    'Economics': 'Social Science',\n",
    "    'Computer Science': 'Physical Sciences',\n",
    "    'Applied Mathematics': 'Physical Sciences',\n",
    "    'Public Health': 'Life Sciences',\n",
    "    'Cognitive, Linguistic, and Psychological Sciences': 'Social Science',\n",
    "}\n",
    "\n",
    "# compile keyword checkers\n",
    "def build_checkers():\n",
    "    primary_checker = re.compile(\n",
    "        r'|'.join([kw.replace(' ', r'\\s*') for kw in PRIMARY_KEYWORDS]),\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    ai_checker = re.compile(\n",
    "        r'\\b(' + '|'.join(AI_CONTEXT_WORDS) + r')\\b',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    policy_checker = re.compile(\n",
    "        r'\\b(' + '|'.join(POLICY_KEYWORDS) + r')\\b',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    return primary_checker, ai_checker, policy_checker\n",
    "\n",
    "PRIMARY_CHECKER, AI_CHECKER, POLICY_CHECKER = build_checkers()\n",
    "\n",
    "def is_policy_text(text):\n",
    "    if PRIMARY_CHECKER.search(text):\n",
    "        return 'Matched a primary keyword.'\n",
    "    if AI_CHECKER.search(text) and POLICY_CHECKER.search(text):\n",
    "        return 'Matched the combination of AI-context and policy keywords.'\n",
    "    return None\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 2: TEXT EXTRACTION\n",
    "# ==============================================================================\n",
    "\n",
    "def extract_paragraphs_from_doc(doc_path):\n",
    "    if not win32:\n",
    "        print(f\"Skipping .doc file '{os.path.basename(doc_path)}' as 'pywin32' is not available on this system.\")\n",
    "        return []\n",
    "    word = None\n",
    "    doc = None\n",
    "    try:\n",
    "        word = win32.Dispatch(\"Word.Application\")\n",
    "        word.Visible = False\n",
    "        abs_path = os.path.abspath(doc_path)\n",
    "        doc = word.Documents.Open(abs_path)\n",
    "        return [p.Range.Text.strip() for p in doc.Paragraphs if p.Range.Text.strip()]\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing .doc file: {e}\")\n",
    "        return []\n",
    "    finally:\n",
    "        if doc: doc.Close(False)\n",
    "        if word: word.Quit()\n",
    "\n",
    "\n",
    "def extract_paragraphs_from_docx(doc_path):\n",
    "    try:\n",
    "        doc = Document(doc_path)\n",
    "        blocks = [p.text for p in doc.paragraphs if p.text.strip()]\n",
    "        for table in doc.tables:\n",
    "            for row in table.rows:\n",
    "                for cell in row.cells:\n",
    "                    if cell.text.strip(): blocks.append(cell.text)\n",
    "        return blocks\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading DOCX file {doc_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def _reconstruct_paragraphs_from_page(page):\n",
    "    words = page.extract_words(x_tolerance=2)\n",
    "    if not words: return []\n",
    "    lines = {}\n",
    "    for w in words:\n",
    "        top = round(w['top'],2)\n",
    "        lines.setdefault(top, []).append(w)\n",
    "    for top in lines: lines[top].sort(key=lambda x: x['x0'])\n",
    "    items = sorted(lines.items(), key=lambda kv: kv[0])\n",
    "    texts, heights = [], []\n",
    "    for idx,(top, ws) in enumerate(items):\n",
    "        texts.append({'top': top, 'text': ' '.join(w['text'] for w in ws)})\n",
    "        if idx>0: heights.append(top - items[idx-1][0])\n",
    "    avg = sum(heights)/len(heights) if heights else 12\n",
    "    threshold = avg*1.5\n",
    "    paras, cur = [], texts[0]['text']\n",
    "    for prev, curr in zip(texts, texts[1:]):\n",
    "        if (curr['top'] - prev['top'])>threshold:\n",
    "            paras.append(cur); cur = curr['text']\n",
    "        else:\n",
    "            cur += ' ' + curr['text']\n",
    "    paras.append(cur)\n",
    "    return paras\n",
    "\n",
    "\n",
    "def extract_paragraphs_from_pdf(pdf_path):\n",
    "    all_p, carry = [], ''\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                paras = _reconstruct_paragraphs_from_page(page)\n",
    "                if not paras: continue\n",
    "                if carry: paras[0] = carry + ' ' + paras[0]; carry = ''\n",
    "                if not paras[-1].strip().endswith(('.', '?', '!', '\"', \"'\", ')', ':', ';')):\n",
    "                    carry = paras.pop()\n",
    "                all_p.extend(paras)\n",
    "        if carry: all_p.append(carry)\n",
    "        if all_p:\n",
    "            print(\"Successfully extracted paragraphs using coordinate-based method.\")\n",
    "            return all_p\n",
    "        print(\"Coordinate-based method failed. No text extracted.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Coordinate-based parsing failed: {e}\")\n",
    "    return []\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 3: ANALYSIS LOGIC\n",
    "# ==============================================================================\n",
    "\n",
    "def find_course_code(paragraphs, limit=30):\n",
    "    for lname, scode in DEPARTMENT_MAP.items():\n",
    "        pat = re.compile(r'\\b'+re.escape(lname)+r'\\s*(\\d{3,4}[A-Z]?)\\b', re.IGNORECASE)\n",
    "        for para in paragraphs[:limit]:\n",
    "            if m:=pat.search(para): return f\"{scode if isinstance(scode,str) else scode[0]} {m.group(1)}\"\n",
    "    fb = re.compile(r\"\\b(([A-Z]{2,4}(\\s*/\\s*[A-Z]{2,4})*))\\s*(\\d{3,4}[A-Z]?)\\b\")\n",
    "    for para in paragraphs[:limit]:\n",
    "        if m:=fb.search(para): return f\"{m.group(1).replace(' ','')} {m.group(4)}\"\n",
    "    return None\n",
    "\n",
    "\n",
    "def analyze_ai_policy(paragraphs):\n",
    "    return [\n",
    "        {'text':p,'reason': reason}\n",
    "        for p in paragraphs if (reason:=is_policy_text(p))\n",
    "    ]\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 4: MAIN CONTROLLER\n",
    "# ==============================================================================\n",
    "\n",
    "def analyze_syllabus(path):\n",
    "    print(f\"\\n{'='*20} Analyzing Syllabus: {os.path.basename(path)} {'='*20}\")\n",
    "    if not os.path.exists(path):\n",
    "        print(\"Error: File not found.\"); return\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    paras = []\n",
    "    if ext=='.docx': paras=extract_paragraphs_from_docx(path)\n",
    "    elif ext=='.pdf': paras=extract_paragraphs_from_pdf(path)\n",
    "    elif ext=='.doc': paras=extract_paragraphs_from_doc(path)\n",
    "    else:\n",
    "        print(f\"Error: Unsupported file type '{ext}'.\"); return\n",
    "    if not paras:\n",
    "        print(\"Could not extract any usable text.\"); return\n",
    "    print(f\"Extracted {len(paras)} distinct paragraphs. Analyzing...\")\n",
    "    code = find_course_code(paras)\n",
    "    print(f\"--- Course Code: {code if code else 'Not Found'} ---\")\n",
    "    if code:\n",
    "        prefix = code.split()[0]\n",
    "        dept = CODE_TO_DEPT.get(prefix, 'Unknown')\n",
    "        print(f\"--- Department: {dept} ---\")\n",
    "        area = KNOWLEDGE_AREA_MAP.get(dept, 'Unknown')\n",
    "        print(f\"--- Knowledge Area: {area} ---\")\n",
    "    policies = analyze_ai_policy(paras)\n",
    "    if not policies:\n",
    "        print(\"\\n--- No AI policy paragraphs were found in this document. ---\")\n",
    "        return\n",
    "    print(f\"\\n--- SUCCESS: Found {len(policies)} AI Policy Paragraph(s) ---\\n\")\n",
    "    unique = {p['text']: p for p in policies}.values()\n",
    "    for i,p in enumerate(unique,1):\n",
    "        print(f\"--- Relevant Paragraph {i} ---\")\n",
    "        print(p['text'])\n",
    "        print(f\"(Reason: {p['reason']})\\n\")\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 5: EXECUTION\n",
    "# ==============================================================================\n",
    "if __name__=='__main__':\n",
    "    files=[\n",
    "        '8C2WFwQPrq6vcFU0Yml3vHcL1CYb5HnYcxp5s7A5.docx',\n",
    "        'UMeRwRyvXAKAH6DwCkTmFyIunq3ti97bJPARlu7C.docx',\n",
    "        '0Fslu7lGZ8dJG0OYQGf1TgzFMPyFDEv0n5Q96BNq.pdf',\n",
    "        '7Y6H93I4L6F5bcplrUpPbf5AjQcypbJfrevTSiNf.doc'\n",
    "    ]\n",
    "    for f in files:\n",
    "        analyze_syllabus(f)\n",
    "        print(\"\\n\"*2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa85bd2",
   "metadata": {},
   "source": [
    "## Create a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7773fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Analyzing Syllabus: 8C2WFwQPrq6vcFU0Yml3vHcL1CYb5HnYcxp5s7A5.docx ====================\n",
      "Error: File not found.\n",
      "\n",
      "==================== Analyzing Syllabus: UMeRwRyvXAKAH6DwCkTmFyIunq3ti97bJPARlu7C.docx ====================\n",
      "Error: File not found.\n",
      "\n",
      "==================== Analyzing Syllabus: 0Fslu7lGZ8dJG0OYQGf1TgzFMPyFDEv0n5Q96BNq.pdf ====================\n",
      "Error: File not found.\n",
      "\n",
      "==================== Analyzing Syllabus: 7Y6H93I4L6F5bcplrUpPbf5AjQcypbJfrevTSiNf.doc ====================\n",
      "Error: File not found.\n",
      "\n",
      "Summary CSV written to syllabus_summary.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "from docx import Document\n",
    "import pdfplumber\n",
    "\n",
    "# Conditionally import the library for .doc files, only on Windows\n",
    "if sys.platform == 'win32':\n",
    "    try:\n",
    "        import win32com.client as win32\n",
    "    except ImportError:\n",
    "        print(\"Warning: The 'pywin32' library is not installed. .doc files cannot be processed.\")\n",
    "        print(\"To enable .doc support on Windows, run: pip install pywin32\")\n",
    "        win32 = None\n",
    "else:\n",
    "    win32 = None\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 1: KEYWORD DEFINITIONS & MAPPINGS\n",
    "# ==============================================================================\n",
    "PRIMARY_KEYWORDS = ['ai policy', 'aigenerated content', 'ai-generated', 'ai-assisted', 'generative ai']\n",
    "AI_CONTEXT_WORDS = [\n",
    "    'ai', 'artificial intelligence', 'generative', 'chatgpt', 'llm', 'copilot',\n",
    "    'bard', 'large language model', 'gemini', 'dall-e', 'gpt', 'midjourney',\n",
    "    'stable diffusion', 'ai tool', 'ai tools'\n",
    "]\n",
    "POLICY_KEYWORDS = [\n",
    "    'academic integrity', 'academic dishonesty', 'plagiarism', 'cheating', 'unauthorized',\n",
    "    'unauthorized use', 'unauthorized assistance', 'citation', 'cite', 'attribution',\n",
    "    'acknowledge', 'permitted', 'allowed', 'prohibited', 'forbidden', 'disclosure',\n",
    "    'ethical use', 'responsible use', 'use', 'using', 'assistance', 'help', 'guidance',\n",
    "    'policy', 'rule'\n",
    "]\n",
    "DEPARTMENT_MAP = {\n",
    "    'Africana Studies': 'AFRI',\n",
    "    'American Studies': ['AMST', 'ETHN'],\n",
    "    'Anthropology': 'ANTH',\n",
    "    'Economics': 'ECON',\n",
    "    'Computer Science': 'CSCI',\n",
    "    'Applied Mathematics': 'APMA',\n",
    "    'Public Health': 'PHP',\n",
    "    'Cognitive, Linguistic, and Psychological Sciences': 'CLPS',\n",
    "}\n",
    "\n",
    "# build reverse lookup: short code → full department name\n",
    "CODE_TO_DEPT = {}\n",
    "for long_name, short in DEPARTMENT_MAP.items():\n",
    "    if isinstance(short, list):\n",
    "        for s in short:\n",
    "            CODE_TO_DEPT[s] = long_name\n",
    "    else:\n",
    "        CODE_TO_DEPT[short] = long_name\n",
    "\n",
    "# map each department to a knowledge area\n",
    "KNOWLEDGE_AREA_MAP = {\n",
    "    'Africana Studies': 'Humanities',\n",
    "    'American Studies': 'Humanities',\n",
    "    'Anthropology': 'Social Science',\n",
    "    'Economics': 'Social Science',\n",
    "    'Computer Science': 'Physical Sciences',\n",
    "    'Applied Mathematics': 'Physical Sciences',\n",
    "    'Public Health': 'Life Sciences',\n",
    "    'Cognitive, Linguistic, and Psychological Sciences': 'Social Science',\n",
    "}\n",
    "\n",
    "# compile keyword checkers\n",
    "def build_checkers():\n",
    "    primary_checker = re.compile(\n",
    "        '|'.join([kw.replace(' ', r'\\s*') for kw in PRIMARY_KEYWORDS]),\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    ai_checker = re.compile(\n",
    "        r'\\b(' + '|'.join(AI_CONTEXT_WORDS) + r')\\b',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    policy_checker = re.compile(\n",
    "        r'\\b(' + '|'.join(POLICY_KEYWORDS) + r')\\b',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    return primary_checker, ai_checker, policy_checker\n",
    "\n",
    "PRIMARY_CHECKER, AI_CHECKER, POLICY_CHECKER = build_checkers()\n",
    "\n",
    "def is_policy_text(text):\n",
    "    if PRIMARY_CHECKER.search(text):\n",
    "        return 'Matched a primary keyword.'\n",
    "    if AI_CHECKER.search(text) and POLICY_CHECKER.search(text):\n",
    "        return 'Matched the combination of AI-context and policy keywords.'\n",
    "    return None\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 2: TEXT EXTRACTION\n",
    "# ==============================================================================\n",
    "def extract_paragraphs_from_doc(doc_path):\n",
    "    if not win32:\n",
    "        print(f\"Skipping .doc file '{os.path.basename(doc_path)}' as 'pywin32' is not available on this system.\")\n",
    "        return []\n",
    "    word = None\n",
    "    doc = None\n",
    "    try:\n",
    "        word = win32.Dispatch(\"Word.Application\")\n",
    "        word.Visible = False\n",
    "        abs_path = os.path.abspath(doc_path)\n",
    "        doc = word.Documents.Open(abs_path)\n",
    "        return [p.Range.Text.strip() for p in doc.Paragraphs if p.Range.Text.strip()]\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing .doc file: {e}\")\n",
    "        return []\n",
    "    finally:\n",
    "        if doc: doc.Close(False)\n",
    "        if word: word.Quit()\n",
    "\n",
    "def extract_paragraphs_from_docx(doc_path):\n",
    "    try:\n",
    "        doc = Document(doc_path)\n",
    "        blocks = [p.text for p in doc.paragraphs if p.text.strip()]\n",
    "        for table in doc.tables:\n",
    "            for row in table.rows:\n",
    "                for cell in row.cells:\n",
    "                    if cell.text.strip(): blocks.append(cell.text)\n",
    "        return blocks\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading DOCX file {doc_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def _reconstruct_paragraphs_from_page(page):\n",
    "    words = page.extract_words(x_tolerance=2)\n",
    "    if not words: return []\n",
    "    lines = {}\n",
    "    for w in words:\n",
    "        top = round(w['top'], 2)\n",
    "        lines.setdefault(top, []).append(w)\n",
    "    for top, ws in lines.items():\n",
    "        ws.sort(key=lambda x: x['x0'])\n",
    "    items = sorted(lines.items(), key=lambda kv: kv[0])\n",
    "    texts, heights = [], []\n",
    "    for idx, (top, ws) in enumerate(items):\n",
    "        texts.append({'top': top, 'text': ' '.join(w['text'] for w in ws)})\n",
    "        if idx > 0:\n",
    "            heights.append(top - items[idx-1][0])\n",
    "    avg_height = sum(heights) / len(heights) if heights else 12\n",
    "    threshold = avg_height * 1.5\n",
    "    paras, current = [], texts[0]['text']\n",
    "    for prev, curr in zip(texts, texts[1:]):\n",
    "        if (curr['top'] - prev['top']) > threshold:\n",
    "            paras.append(current)\n",
    "            current = curr['text']\n",
    "        else:\n",
    "            current += ' ' + curr['text']\n",
    "    paras.append(current)\n",
    "    return paras\n",
    "\n",
    "def extract_paragraphs_from_pdf(pdf_path):\n",
    "    all_paras, carry = [], ''\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                paras = _reconstruct_paragraphs_from_page(page)\n",
    "                if not paras:\n",
    "                    continue\n",
    "                if carry:\n",
    "                    paras[0] = carry + ' ' + paras[0]\n",
    "                    carry = ''\n",
    "                if not paras[-1].strip().endswith(('.', '?', '!', '\"', \"'\", ')', ':', ';')):\n",
    "                    carry = paras.pop()\n",
    "                all_paras.extend(paras)\n",
    "        if carry:\n",
    "            all_paras.append(carry)\n",
    "        if all_paras:\n",
    "            print(\"Successfully extracted paragraphs using coordinate-based method.\")\n",
    "            return all_paras\n",
    "        print(\"Coordinate-based method failed. No text extracted.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Coordinate-based parsing failed: {e}\")\n",
    "    return []\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 3: ANALYSIS LOGIC\n",
    "# ==============================================================================\n",
    "def find_course_code(paragraphs, limit=30):\n",
    "    for long_name, short_code in DEPARTMENT_MAP.items():\n",
    "        pattern = re.compile(r'\\b' + re.escape(long_name) + r'\\s*(\\d{3,4}[A-Z]?)\\b', re.IGNORECASE)\n",
    "        for para in paragraphs[:limit]:\n",
    "            match = pattern.search(para)\n",
    "            if match:\n",
    "                code = short_code if isinstance(short_code, str) else short_code[0]\n",
    "                return f\"{code} {match.group(1)}\"\n",
    "    # fallback: any CODE ### pattern\n",
    "    fallback = re.compile(r\"\\b(([A-Z]{2,4}(?:\\s*/\\s*[A-Z]{2,4})*))\\s*(\\d{3,4}[A-Z]?)\\b\")\n",
    "    for para in paragraphs[:limit]:\n",
    "        match = fallback.search(para)\n",
    "        if match:\n",
    "            dept_part = match.group(1).replace(' ', '')\n",
    "            num = match.group(3)\n",
    "            return f\"{dept_part} {num}\"\n",
    "    return None\n",
    "\n",
    "\n",
    "def analyze_ai_policy(paragraphs):\n",
    "    findings = []\n",
    "    for para in paragraphs:\n",
    "        reason = is_policy_text(para)\n",
    "        if reason:\n",
    "            findings.append({'text': para, 'reason': reason})\n",
    "    return findings\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 4: MAIN CONTROLLER & CSV OUTPUT\n",
    "# ==============================================================================\n",
    "results = []  # accumulate summary rows\n",
    "\n",
    "def analyze_syllabus(path):\n",
    "    print(f\"\\n{'='*20} Analyzing Syllabus: {os.path.basename(path)} {'='*20}\")\n",
    "    if not os.path.exists(path):\n",
    "        print(\"Error: File not found.\"); return\n",
    "\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    paragraphs = []\n",
    "    if ext == '.docx':\n",
    "        paragraphs = extract_paragraphs_from_docx(path)\n",
    "    elif ext == '.pdf':\n",
    "        paragraphs = extract_paragraphs_from_pdf(path)\n",
    "    elif ext == '.doc':\n",
    "        paragraphs = extract_paragraphs_from_doc(path)\n",
    "    else:\n",
    "        print(f\"Error: Unsupported file type '{ext}'.\"); return\n",
    "\n",
    "    if not paragraphs:\n",
    "        print(\"Could not extract any usable text.\"); return\n",
    "\n",
    "    print(f\"Extracted {len(paragraphs)} distinct paragraphs. Analyzing...\")\n",
    "    code = find_course_code(paragraphs)\n",
    "    print(f\"--- Course Code: {code if code else 'Not Found'} ---\")\n",
    "\n",
    "    dept = 'Unknown'\n",
    "    area = 'Unknown'\n",
    "    if code:\n",
    "        prefix = code.split()[0]\n",
    "        dept = CODE_TO_DEPT.get(prefix, 'Unknown')\n",
    "        area = KNOWLEDGE_AREA_MAP.get(dept, 'Unknown')\n",
    "        print(f\"--- Department: {dept} ---\")\n",
    "        print(f\"--- Knowledge Area: {area} ---\")\n",
    "\n",
    "    policies = analyze_ai_policy(paragraphs)\n",
    "    flag = 1 if policies else 0\n",
    "    if not policies:\n",
    "        print(\"\\n--- No AI policy paragraphs were found in this document. ---\")\n",
    "    else:\n",
    "        print(f\"\\n--- SUCCESS: Found {len(policies)} AI Policy Paragraph(s) ---\\n\")\n",
    "        unique = {p['text']: p for p in policies}.values()\n",
    "        for i, p in enumerate(unique, 1):\n",
    "            print(f\"--- Relevant Paragraph {i} ---\")\n",
    "            print(p['text'])\n",
    "            print(f\"(Reason: {p['reason']})\\n\")\n",
    "\n",
    "    # prepare CSV row\n",
    "    if policies:\n",
    "        unique = {p['text']: p for p in policies}.values()\n",
    "        para_texts = [f\"{i}:{p['text']} ({p['reason']})\" for i, p in enumerate(unique, 1)]\n",
    "        para_field = ' || '.join(para_texts)\n",
    "    else:\n",
    "        para_field = ''\n",
    "\n",
    "    results.append({\n",
    "        'Course Code': code or '',\n",
    "        'Department': dept,\n",
    "        'Knowledge Area': area,\n",
    "        'AI Policy': flag,\n",
    "        'AI Policy Paragraphs': para_field\n",
    "    })\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    files = [\n",
    "        '8C2WFwQPrq6vcFU0Yml3vHcL1CYb5HnYcxp5s7A5.docx',\n",
    "        'UMeRwRyvXAKAH6DwCkTmFyIunq3ti97bJPARlu7C.docx',\n",
    "        '0Fslu7lGZ8dJG0OYQGf1TgzFMPyFDEv0n5Q96BNq.pdf',\n",
    "        '7Y6H93I4L6F5bcplrUpPbf5AjQcypbJfrevTSiNf.doc'\n",
    "    ]\n",
    "    for f in files:\n",
    "        analyze_syllabus(f)\n",
    "\n",
    "    # write CSV summary\n",
    "    csv_file = 'syllabus_summary.csv'\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8-sig') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\n",
    "            'Course Code', 'Department', 'Knowledge Area', 'AI Policy', 'AI Policy Paragraphs'\n",
    "        ])\n",
    "        writer.writeheader()\n",
    "        for row in results:\n",
    "            writer.writerow(row)\n",
    "    print(f\"\\nSummary CSV written to {csv_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b12b8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
