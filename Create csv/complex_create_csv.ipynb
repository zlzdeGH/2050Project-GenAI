{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57049977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import time\n",
    "from docx import Document\n",
    "import pdfplumber\n",
    "import ollama\n",
    "\n",
    "# --- For Windows users... ---\n",
    "if sys.platform == 'win32':\n",
    "    try: import win32com.client as win32\n",
    "    except ImportError: print(\"Warning: .doc support disabled.\"); win32 = None\n",
    "else: win32 = None\n",
    "\n",
    "# ================================================================================\n",
    "# PART 0: DEPARTMENT & KNOWLEDGE AREA MAPPINGS\n",
    "# ================================================================================\n",
    "DEPARTMENT_MAP = {\n",
    "    'Africana Studies': 'AFRI',\n",
    "    'American Studies': ['AMST', 'ETHN', 'NAIS', 'PHUM', 'STS'],\n",
    "    'Anthropology': 'ANTH',\n",
    "    'Applied Mathematics': 'APMA',\n",
    "    'Archaeology and the Ancient World': 'ARCH',\n",
    "    'Bio-Medical (PLME & MED)': 'MED',\n",
    "    'Biology': 'BIOL',\n",
    "    'Brown Arts Institute': 'ARTS',\n",
    "    'Business, Entrepreneurship, Organizations': 'BEO',\n",
    "    'Center for Language Studies': ['ARAB', 'EINT', 'HNDI', 'LANG', 'PRSN', 'SIGN', 'TKSH', 'YORU'],\n",
    "    'Chemistry': 'CHEM',\n",
    "    'Classics': ['CLAS', 'CREK', 'LATN', 'MGRK', 'SANS'],\n",
    "    'Cognitive, Linguistic, and Psychological Sciences': ['CLPS', 'LING'],\n",
    "    'Cognitive and Psychological Sciences': 'CPSY',\n",
    "    'Cogut Institute for the Humanities': 'HMAN',\n",
    "    'Comparative Literature': 'COLT',\n",
    "    'Computer Science': 'CSCI',\n",
    "    'Data Science Initiative': ['DATA', 'DSIO'],\n",
    "    'Early Modern World': 'EMOW',\n",
    "    'Earth, Environmental and Planetary Sciences': 'EEPS',\n",
    "    'East Asian Studies': ['CHIN', 'EAST', 'JAPN', 'KREA', 'VIET'],\n",
    "    'Economics': 'ENCO',\n",
    "    'Education': 'EDUC',\n",
    "    'Egyptology and Assyriology': ['ASYR', 'EGYT'],\n",
    "    'Engineering': 'ENGN',\n",
    "    'English': 'ENGL',\n",
    "    'Environmental Studies': 'ENVS',\n",
    "    'French Studies': 'FREN',\n",
    "    'German Studies': 'GRMN',\n",
    "    'Hispanic Studies': 'HISP',\n",
    "    'History': 'HIST',\n",
    "    'History of Art and Architecture': 'HIAA',\n",
    "    'Italian Studies': 'ITAL',\n",
    "    'Judaic Studies': ['HEBR', 'JUDS'],\n",
    "    'Linguistics': 'LING',\n",
    "    'Literary Arts': 'LITR',\n",
    "    'Mathematics': 'MATH',\n",
    "    'Medieval Studies': 'MDVL',\n",
    "    'Middle East Studies': 'MES',\n",
    "    'Modern Culture and Media': 'MCM',\n",
    "    'Music': 'MUSC',\n",
    "    'Neuroscience': 'NEUR',\n",
    "    'Pembroke Center Teach and Rearch': 'GNSS',\n",
    "    'Philosophy': 'PHIL',\n",
    "    'Physics': 'PHYS',\n",
    "    'Political Science': 'POLS',\n",
    "    'Portuguese and Brazilian Studies': 'POBS',\n",
    "    'Public Health': ['BHDS', 'GPHP', 'HCL', 'PHP'],\n",
    "    'Religious Studies': ['COST', 'RELS'],\n",
    "    'Slavic Studies': ['CZCH','PLSH','RUSS', 'SLAV'],\n",
    "    'Sociology': 'SOC',\n",
    "    'Theatre Arts and Performance Studies': 'TAPS',\n",
    "    'Urban Studies': 'URBN',\n",
    "    'Visual Art': 'VISA',\n",
    "    'Watson Institute': ['IAPA', 'MPA'],\n",
    "}\n",
    "# reverse lookup: code >> dept name\n",
    "CODE_TO_DEPT = {}\n",
    "for dept, codes in DEPARTMENT_MAP.items():\n",
    "    if isinstance(codes, list):\n",
    "        for c in codes:\n",
    "            CODE_TO_DEPT[c] = dept\n",
    "    else:\n",
    "        CODE_TO_DEPT[codes] = dept\n",
    "# map each department to a knowledge area\n",
    "\n",
    "KNOWLEDGE_AREA_MAP = {\n",
    "    'Africana Studies': 'Social Sciences',\n",
    "    'American Studies': 'Social Sciences',\n",
    "    'Anthropology': 'Social Sciences',\n",
    "    'Applied Mathematics': 'Physical Sciences',\n",
    "    'Archaeology and the Ancient World': 'Humanities',\n",
    "    'Bio-Medical (PLME & MED)': 'Life Sciences',\n",
    "    'Biology': 'Life Sciences',\n",
    "    'Brown Arts Institute': 'Arts',\n",
    "    'Business, Entrepreneurship, Organizations': 'Social Sciences',\n",
    "    'Center for Language Studies': 'Humanities',\n",
    "    'Chemistry': 'Physical Sciences',\n",
    "    'Classics': 'Humanities',\n",
    "    'Cognitive and Psychological Sciences': 'Life Sciences',\n",
    "    'Cogut Institute for the Humanities': 'Humanities',\n",
    "    'Comparative Literature': 'Humanities',\n",
    "    'Computer Science': 'Physical Sciences',\n",
    "    'Data Science Initiative': 'Physical Sciences',\n",
    "    'Early Modern World': 'Humanities',\n",
    "    'Earth, Environmental and Planetary Sciences': 'Physical Sciences',\n",
    "    'East Asian Studies': 'Humanities',\n",
    "    'Economics': 'Social Sciences',\n",
    "    'Education': 'Social Sciences',\n",
    "    'Egyptology and Assyriology': 'Humanities',\n",
    "    'Engineering': 'Physical Sciences',\n",
    "    'English': 'Humanities',\n",
    "    'Environmental Studies': 'Physical Sciences',\n",
    "    'French Studies': 'Humanities',\n",
    "    'German Studies': 'Humanities',\n",
    "    'Hispanic Studies': 'Humanities',\n",
    "    'History': 'Social Sciences',\n",
    "    'History of Art and Architecture': 'Humanities',\n",
    "    'Italian Studies': 'Humanities',\n",
    "    'Judaic Studies': 'Humanities',\n",
    "    'Linguistics': 'Social Sciences',\n",
    "    'Literary Arts': 'Humanities',\n",
    "    'Mathematics': 'Physical Sciences',\n",
    "    'Medieval Studies': 'Humanities',\n",
    "    'Middle East Studies': 'Humanities',\n",
    "    'Modern Culture and Media': 'Humanities',\n",
    "    'Music': 'Humanities',\n",
    "    'Neuroscience': 'Life Sciences',\n",
    "    'Pembroke Center Teach and Rearch': 'Humanities',\n",
    "    'Philosophy': 'Humanities',\n",
    "    'Physics': 'Physical Sciences',\n",
    "    'Political Science': 'Social Sciences',\n",
    "    'Portuguese and Brazilian Studies': 'Humanities',\n",
    "    'Public Health': 'Life Sciences',\n",
    "    'Religious Studies': 'Humanities',\n",
    "    'Slavic Studies': 'Humanities',\n",
    "    'Sociology': 'Social Sciences',\n",
    "    'Theatre Arts and Performance Studies': 'Humanities',\n",
    "    'Urban Studies': 'Social Sciences',\n",
    "    'Visual Art': 'Humanities',\n",
    "    'Watson Institute': 'Social Sciences',\n",
    "}\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 1: DEFINITIONS\n",
    "# ==============================================================================\n",
    "AI_CONTEXT_WORDS = ['ai', 'artificial intelligence', 'generative', 'chatgpt', 'llm', 'copilot', 'bard', 'large language model', 'gemini', 'dall-e', 'gpt', 'midjourney', 'stable diffusion', 'ai tool', 'ai tools']\n",
    "POLICY_KEYWORDS = ['academic integrity', 'academic dishonesty', 'plagiarism', 'cheating', 'unauthorized', 'unauthorized use', 'unauthorized assistance', 'citation', 'cite', 'attribution', 'acknowledge', 'permitted', 'allowed', 'prohibited', 'forbidden', 'disclosure', 'ethical use', 'responsible use', 'use', 'using', 'assistance', 'help', 'guidance', 'policy', 'rule']\n",
    "COURSE_CODE_MAP = {\n",
    "    'Africana Studies': 'AFRI',\n",
    "    'American Studies': 'AMST', \n",
    "    'American Studies 1': 'ETHN', \n",
    "    'American Studies 2': 'NAIS', \n",
    "    'American Studies 3': 'PHUM', \n",
    "    'American Studies 4': 'STS',\n",
    "    'Anthropology': 'ANTH',\n",
    "    'Applied Mathematics': 'APMA',\n",
    "    'Archaeology and the Ancient World': 'ARCH',\n",
    "    'Bio-Medical (PLME & MED)': 'MED',\n",
    "    'Biology': 'BIOL',\n",
    "    'Brown Arts Institute': 'ARTS',\n",
    "    'Business, Entrepreneurship, Organizations': 'BEO',\n",
    "    'Center for Language Studies': 'ARAB', \n",
    "    'Center for Language Studies 1': 'EINT', \n",
    "    'Center for Language Studies 2': 'HNDI', \n",
    "    'Center for Language Studies 3': 'LANG', \n",
    "    'Center for Language Studies 4': 'PRSN', \n",
    "    'Center for Language Studies 5': 'SIGN', \n",
    "    'Center for Language Studies 6': 'TKSH', \n",
    "    'Center for Language Studies 7': 'YORU',\n",
    "    'Chemistry': 'CHEM',\n",
    "    'Classics': 'CLAS', \n",
    "    'Classics 1': 'CREK', \n",
    "    'Classics 2': 'LATN', \n",
    "    'Classics 3':'MGRK', \n",
    "    'Classics 4':'SANS',\n",
    "    'Cognitive, Linguistic, and Psychological Sciences': 'CLPS', \n",
    "    'Cognitive, Linguistic, and Psychological Sciences 1': 'LING',\n",
    "    'Cognitive and Psychological Sciences': 'CPSY',\n",
    "    'Cogut Institute for the Humanities': 'HMAN',\n",
    "    'Comparative Literature': 'COLT',\n",
    "    'Computer Science': 'CSCI',\n",
    "    'Data Science Initiative': 'DATA', \n",
    "    'Data Science Initiative 1': 'DSIO',\n",
    "    'Early Modern World': 'EMOW',\n",
    "    'Earth, Environmental and Planetary Sciences': 'EEPS',\n",
    "    'East Asian Studies': 'EAST', \n",
    "    'East Asian Studies 1': 'CHIN',\n",
    "    'East Asian Studies 2': 'JAPN', \n",
    "    'East Asian Studies 3': 'KREA', \n",
    "    'East Asian Studies 4': 'VIET',\n",
    "    'Economics': 'ENCO',\n",
    "    'Education': 'EDUC',\n",
    "    'Assyriology': 'ASYR',\n",
    "    'Egyptology': 'EGYT',\n",
    "    'Engineering': 'ENGN',\n",
    "    'English': 'ENGL',\n",
    "    'Environmental Studies': 'ENVS',\n",
    "    'French Studies': 'FREN',\n",
    "    'German Studies': 'GRMN',\n",
    "    'Hispanic Studies': 'HISP',\n",
    "    'History': 'HIST',\n",
    "    'History of Art and Architecture': 'HIAA',\n",
    "    'Italian Studies': 'ITAL',\n",
    "    'Judaic Studies': 'JUDS',\n",
    "    'Judaic Studies 1': 'HEBR',\n",
    "    'Linguistics': 'LING',\n",
    "    'Literary Arts': 'LITR',\n",
    "    'Mathematics': 'MATH',\n",
    "    'Medieval Studies': 'MDVL',\n",
    "    'Middle East Studies': 'MES',\n",
    "    'Modern Culture and Media': 'MCM',\n",
    "    'Music': 'MUSC',\n",
    "    'Neuroscience': 'NEUR',\n",
    "    'Pembroke Center Teach and Rearch': 'GNSS',\n",
    "    'Philosophy': 'PHIL',\n",
    "    'Physics': 'PHYS',\n",
    "    'Political Science': 'POLS',\n",
    "    'Portuguese and Brazilian Studies': 'POBS',\n",
    "    'Public Health': 'BHDS', \n",
    "    'Public Health 1': 'GPHP', \n",
    "    'Public Health 2': 'HCL', \n",
    "    'Public Health 3': 'PHP',\n",
    "    'Religious Studies': 'RELS',\n",
    "    'Religious Studies 1': 'COST',\n",
    "    'Slavic Studies': 'SLAV',\n",
    "    'Slavic Studies 1': 'CZCH',\n",
    "    'Slavic Studies 2': 'PLSH',\n",
    "    'Slavic Studies 3': 'RUSS',\n",
    "    'Sociology': 'SOC',\n",
    "    'Theatre Arts and Performance Studies': 'TAPS',\n",
    "    'Urban Studies': 'URBN',\n",
    "    'Visual Art': 'VISA',\n",
    "    'Watson Institute': 'IAPA', \n",
    "    'Watson Institute 1': 'MPA',\n",
    "}\n",
    "\n",
    "HEADER_PATTERN = re.compile(\n",
    "    r'.*\\b('\n",
    "    r'(ai|artificial\\sintelligence|generative\\s+ai)\\s+.*\\bpolicy'\n",
    "    r'|'\n",
    "    r'policy\\s+.*\\b(ai|artificial\\sintelligence|generative\\s+ai)'\n",
    "    r')\\b.*',\n",
    "    re.IGNORECASE\n",
    ")\n",
    "AI_TRIGGER_PATTERN = re.compile(r'\\b(' + '|'.join(AI_CONTEXT_WORDS) + r')\\b', re.IGNORECASE)\n",
    "POLICY_PATTERN = re.compile(r'\\b(' + '|'.join(POLICY_KEYWORDS) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 2: TEXT EXTRACTION \n",
    "# ==============================================================================\n",
    "\n",
    "def extract_paragraphs_from_doc(doc_path):\n",
    "    if not win32:\n",
    "        print(f\"Skipping .doc file '{os.path.basename(doc_path)}' as 'pywin32' is not available on this system.\")\n",
    "        return []\n",
    "    word = None; doc = None\n",
    "    try:\n",
    "        word = win32.Dispatch(\"Word.Application\")\n",
    "        word.Visible = False\n",
    "        doc = word.Documents.Open(os.path.abspath(doc_path))\n",
    "        return [p.Range.Text.strip() for p in doc.Paragraphs if p.Range.Text.strip()]\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing .doc file with MS Word: {e}\")\n",
    "        return []\n",
    "    finally:\n",
    "        if doc: doc.Close(False)\n",
    "        if word: word.Quit()\n",
    "\n",
    "def extract_paragraphs_from_docx(doc_path):\n",
    "    try:\n",
    "        doc = Document(doc_path)\n",
    "        blocks = [p.text for p in doc.paragraphs if p.text.strip()]\n",
    "        for table in doc.tables:\n",
    "            for row in table.rows:\n",
    "                for cell in row.cells:\n",
    "                    if cell.text.strip():\n",
    "                        blocks.append(cell.text)\n",
    "        return blocks\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading DOCX file {doc_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def _reconstruct_paragraphs_from_page(page):\n",
    "    words = page.extract_words(keep_blank_chars=False, x_tolerance=2)\n",
    "    if not words: return []\n",
    "    \n",
    "    lines = {}\n",
    "    for word in words:\n",
    "        line_top = round(word['top'], 2)\n",
    "        if line_top not in lines:\n",
    "            lines[line_top] = []\n",
    "        lines[line_top].append(word)\n",
    "\n",
    "    for line_top in lines:\n",
    "        lines[line_top].sort(key=lambda w: w['x0'])\n",
    "        \n",
    "    sorted_lines = sorted(lines.items(), key=lambda item: item[0])\n",
    "    \n",
    "    reconstructed_lines = []\n",
    "    line_heights = []\n",
    "    last_top = None\n",
    "    for top, words_in_line in sorted_lines:\n",
    "        text = \" \".join(w['text'] for w in words_in_line)\n",
    "        reconstructed_lines.append({'top': top, 'text': text})\n",
    "        if last_top is not None:\n",
    "            line_heights.append(top - last_top)\n",
    "        last_top = top\n",
    "\n",
    "    if not reconstructed_lines: return []\n",
    "\n",
    "    avg_line_height = sum(line_heights) / len(line_heights) if line_heights else 12\n",
    "    paragraph_break_threshold = avg_line_height * 1.5\n",
    "    \n",
    "    page_paragraphs = []\n",
    "    current_paragraph = reconstructed_lines[0]['text']\n",
    "    for i in range(1, len(reconstructed_lines)):\n",
    "        prev_line, curr_line = reconstructed_lines[i-1], reconstructed_lines[i]\n",
    "        if (curr_line['top'] - prev_line['top']) > paragraph_break_threshold:\n",
    "            page_paragraphs.append(current_paragraph)\n",
    "            current_paragraph = curr_line['text']\n",
    "        else:\n",
    "            current_paragraph += \" \" + curr_line['text']\n",
    "    page_paragraphs.append(current_paragraph)\n",
    "    \n",
    "    return page_paragraphs\n",
    "\n",
    "def extract_paragraphs_from_pdf(pdf_path):\n",
    "    all_paragraphs = []\n",
    "    carry_over_paragraph = \"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page_num, page in enumerate(pdf.pages):\n",
    "                paragraphs_on_page = _reconstruct_paragraphs_from_page(page)\n",
    "                if not paragraphs_on_page: continue\n",
    "                if carry_over_paragraph:\n",
    "                    paragraphs_on_page[0] = carry_over_paragraph + \" \" + paragraphs_on_page[0]\n",
    "                    carry_over_paragraph = \"\"\n",
    "                last_para = paragraphs_on_page[-1]\n",
    "                if not last_para.strip().endswith(('.', '?', '!', '\"', \"'\", ')', ':', ';')):\n",
    "                    carry_over_paragraph = paragraphs_on_page.pop()\n",
    "                all_paragraphs.extend(paragraphs_on_page)\n",
    "        if carry_over_paragraph:\n",
    "            all_paragraphs.append(carry_over_paragraph)\n",
    "        print(\"Successfully extracted paragraphs using coordinate-based method.\")\n",
    "        return all_paragraphs\n",
    "    except Exception as e:\n",
    "        print(f\"Coordinate-based parsing failed: {e}. No OCR fallback implemented.\")\n",
    "        return []\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 3: ANALYSIS LOGIC\n",
    "# ==============================================================================\n",
    "\n",
    "def find_course_code_rule_based(paragraphs, search_limit=30):\n",
    "    for long_name, short_code in COURSE_CODE_MAP.items():\n",
    "        pattern = re.compile(r'\\b' + re.escape(long_name) + r'\\s*(\\d{3,4}[A-Z]?)\\b', re.IGNORECASE)\n",
    "        for para in paragraphs[:search_limit]:\n",
    "            if match := pattern.search(para):\n",
    "                return f\"{short_code} {match.group(1)}\"\n",
    "\n",
    "    SEMESTER_TERMS = ['FALL', 'SPRING', 'WINTER', 'SUMMER']\n",
    "    exclude_pattern = '|'.join(SEMESTER_TERMS)\n",
    "    \n",
    "    fallback_pattern = re.compile(\n",
    "        r'\\b((?!' + exclude_pattern + r'\\b)[A-Z]{2,4}(\\s*/\\s*[A-Z]{2,4})*)\\s*(\\d{3,4}[A-Z]?)\\b'\n",
    "    )\n",
    "    for para in paragraphs[:search_limit]:\n",
    "        if match := fallback_pattern.search(para.upper()):\n",
    "            original_match_text = para[match.start():match.end()]\n",
    "            sub_match = fallback_pattern.match(original_match_text)\n",
    "            if sub_match:\n",
    "                 dept_part = sub_match.group(1).replace(\" \", \"\")\n",
    "                 num_part = sub_match.group(3)\n",
    "                 return f\"{dept_part} {num_part}\"\n",
    "    return None\n",
    "\n",
    "def _normalize_for_comparison(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts text to a 'canonical' form for robust comparison by lowercasing\n",
    "    and removing all non-alphanumeric characters.\n",
    "    \"\"\"\n",
    "    return \"\".join(char.lower() for char in text if char.isalnum())\n",
    "\n",
    "def refine_policy_with_ollama(context_block, model_name=\"deepseek-r1:1.5b\"):\n",
    "    \"\"\"\n",
    "    Asks the LLM to extract the core policy with retries, progressive prompting,\n",
    "    and robust, normalized verbatim quote verification.\n",
    "    Returns:\n",
    "        tuple: (result, flags)\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Handing off to Ollama model '{model_name}' to refine the policy block... ---\")\n",
    "    \n",
    "    SEPARATOR_START = \"[---POLICY_TEXT_START---]\"\n",
    "    SEPARATOR_END = \"[---POLICY_TEXT_END---]\"\n",
    "    MAX_RETRIES = 3\n",
    "\n",
    "    prompts = [\n",
    "        f\"\"\"Analyze the syllabus section to find the policy on \"Artificial Intelligence\".\n",
    "Instructions:\n",
    "1. Provide a brief, one-sentence explanation.\n",
    "2. Output the start separator: {SEPARATOR_START}\n",
    "3. Quote the complete AI policy verbatim.\n",
    "4. Output the end separator: {SEPARATOR_END}\n",
    "If no policy is found, respond ONLY with \"None\".\n",
    "Syllabus Section:\\n---\\n{context_block}\\n---\"\"\",\n",
    "\n",
    "        f\"\"\"RE-EVALUATION: The previous analysis was likely wrong. The text IS KNOWN to contain a policy on \"Artificial Intelligence\". Locate and extract it.\n",
    "Instructions:\n",
    "1. Explain your corrected finding in one sentence.\n",
    "2. Output the start separator: {SEPARATOR_START}\n",
    "3. Quote the policy verbatim. DO NOT summarize.\n",
    "4. Output the end separator: {SEPARATOR_END}\n",
    "Do not respond \"None\". Find the policy.\n",
    "Syllabus Section:\\n---\\n{context_block}\\n---\"\"\",\n",
    "\n",
    "        f\"\"\"FINAL ATTEMPT: You MUST extract the policy about \"Artificial Intelligence\". It is there. Your task is to EXTRACT it, not to decide if it exists.\n",
    "Instructions:\n",
    "1. Find the rules for using AI.\n",
    "2. Output: {SEPARATOR_START}\n",
    "3. Copy the paragraph(s) exactly.\n",
    "4. Output: {SEPARATOR_END}\n",
    "Extract it now.\n",
    "Syllabus Section:\\n---\\n{context_block}\\n---\"\"\"\n",
    "    ]\n",
    "    \n",
    "    normalized_context = _normalize_for_comparison(context_block)\n",
    "\n",
    "    try:\n",
    "        for attempt in range(MAX_RETRIES):\n",
    "            current_prompt = prompts[attempt]\n",
    "            print(f\"INFO: Attempt {attempt + 1} of {MAX_RETRIES} to query LLM...\")\n",
    "            \n",
    "            response = ollama.chat(model=model_name, messages=[{'role': 'user', 'content': current_prompt}])\n",
    "            llm_response = response['message']['content'].strip()\n",
    "\n",
    "            if (llm_response.strip().upper() == \"NONE\"):\n",
    "                print(f\"WARN: Attempt {attempt + 1} failed. LLM responded 'None'.\")\n",
    "                if attempt < MAX_RETRIES - 1: time.sleep(1)\n",
    "                continue\n",
    "\n",
    "            if SEPARATOR_START in llm_response and SEPARATOR_END in llm_response:\n",
    "                after_start = llm_response.split(SEPARATOR_START, 1)[1]\n",
    "                policy_text = after_start.split(SEPARATOR_END, 1)[0].strip()\n",
    "                \n",
    "                if not policy_text:\n",
    "                     print(f\"WARN: Attempt {attempt + 1} failed. LLM returned an empty policy.\")\n",
    "                     if attempt < MAX_RETRIES - 1: time.sleep(1)\n",
    "                     continue\n",
    "\n",
    "                normalized_policy = _normalize_for_comparison(policy_text)\n",
    "                if normalized_policy not in normalized_context:\n",
    "                    print(f\"WARN: Attempt {attempt + 1} failed. Normalized quote not in context (likely a summary).\")\n",
    "                    if attempt < MAX_RETRIES - 1: time.sleep(1)\n",
    "                    continue\n",
    "\n",
    "                print(\"INFO: LLM refinement successful and passed verification.\")\n",
    "                result = [{'text': policy_text, 'reason': f'Refined by {model_name} (Attempt {attempt+1})'}]\n",
    "                return (result, [])\n",
    "            else:\n",
    "                print(f\"WARN: Attempt {attempt + 1} failed. LLM did not use required separators.\")\n",
    "                if attempt < MAX_RETRIES - 1: time.sleep(1)\n",
    "        \n",
    "        return ([], ['LLM_FINAL_FAILURE'])\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"--- ERROR: Could not connect to Ollama: {e} ---\")\n",
    "        return ([], ['LLM_CONNECTION_ERROR'])\n",
    "\n",
    "def analyze_policy_with_clustering(paragraphs):\n",
    "    \"\"\"\n",
    "    Clusters mentions, finds the best block using a two-tier scoring system,\n",
    "    and returns it along with a flag if multiple clusters were found.\n",
    "    Returns:\n",
    "        tuple: (result, flags)\n",
    "    \"\"\"\n",
    "    flags = []\n",
    "    triggers = []\n",
    "    \n",
    "    for i, para in enumerate(paragraphs):\n",
    "        is_header = HEADER_PATTERN.match(para) and len(para.split()) < 15\n",
    "        contains_ai = AI_TRIGGER_PATTERN.search(para)\n",
    "        contains_policy = POLICY_PATTERN.search(para)\n",
    "        if is_header:\n",
    "            triggers.append({'index': i, 'type': 'header', 'weight': 10}) \n",
    "        elif contains_ai and contains_policy:\n",
    "            triggers.append({'index': i, 'type': 'strong_mention', 'weight': 3})\n",
    "        elif contains_ai:\n",
    "            triggers.append({'index': i, 'type': 'weak_mention', 'weight': 1})\n",
    "    \n",
    "    if not triggers:\n",
    "        return ([], [])\n",
    "\n",
    "    clusters = []\n",
    "    if triggers:\n",
    "        current_cluster = [triggers[0]]\n",
    "        for i in range(1, len(triggers)):\n",
    "            if triggers[i]['index'] - current_cluster[-1]['index'] <= 3:\n",
    "                current_cluster.append(triggers[i])\n",
    "            else:\n",
    "                clusters.append(current_cluster)\n",
    "                current_cluster = [triggers[i]]\n",
    "        clusters.append(current_cluster)\n",
    "\n",
    "    if len(clusters) > 1:\n",
    "        print(f\"INFO: Found {len(clusters)} distinct AI-related clusters. Flagging for review.\")\n",
    "        flags.append('MULTIPLE_CLUSTERS')\n",
    "    \n",
    "    # === MODIFICATION START: Two-tier scoring system ===\n",
    "    best_cluster_info = {'score': -1, 'policy_density': -1, 'block': []}\n",
    "    \n",
    "    for cluster in clusters:\n",
    "        min_index = min(t['index'] for t in cluster)\n",
    "        max_index = max(t['index'] for t in cluster)\n",
    "        \n",
    "        # --- Primary Score Calculation ---\n",
    "        has_header = any(t['type'] == 'header' for t in cluster) or (min_index > 0 and HEADER_PATTERN.match(paragraphs[min_index - 1]))\n",
    "        score = sum(t['weight'] for t in cluster) + (20 if has_header else 0)\n",
    "        \n",
    "        # --- Tie-breaker (Secondary Score) Calculation ---\n",
    "        start_idx_for_text = min_index - 1 if has_header and not any(t['type'] == 'header' for t in cluster) else min_index\n",
    "        end_idx_for_text = min(len(paragraphs), max_index + 1)\n",
    "        cluster_block_text = \"\\n\\n\".join(paragraphs[start_idx_for_text:end_idx_for_text])\n",
    "        num_policy_words = len(POLICY_PATTERN.findall(cluster_block_text))\n",
    "        policy_density = num_policy_words / (len(cluster_block_text.split()) + 1e-6) # Add epsilon to avoid division by zero\n",
    "        \n",
    "        # --- Update Best Cluster based on two-tier logic ---\n",
    "        is_best = False\n",
    "        if score > best_cluster_info['score']:\n",
    "            is_best = True\n",
    "        elif score == best_cluster_info['score'] and policy_density > best_cluster_info['policy_density']:\n",
    "            print(f\"INFO: Tie-breaker activated. New cluster with density {policy_density:.4f} is better than previous {best_cluster_info['policy_density']:.4f}.\")\n",
    "            is_best = True\n",
    "        \n",
    "        if is_best:\n",
    "            start_index = start_idx_for_text\n",
    "            end_index = end_idx_for_text\n",
    "            while end_index < len(paragraphs) and len(paragraphs[end_index].split()) > 5:\n",
    "                end_index += 1\n",
    "                \n",
    "            best_cluster_info = {\n",
    "                'score': score, \n",
    "                'policy_density': policy_density, \n",
    "                'block': paragraphs[start_index:end_index]\n",
    "            }\n",
    "    # === MODIFICATION END ===\n",
    "\n",
    "    if best_cluster_info['block']:\n",
    "        final_text = \"\\n\\n\".join(best_cluster_info['block'])\n",
    "        # Add policy_density to the reason for better debugging\n",
    "        reason_str = (\n",
    "            f\"Clustered Policy Block (Score: {best_cluster_info['score']}, \"\n",
    "            f\"Density: {best_cluster_info['policy_density']:.4f})\"\n",
    "        )\n",
    "        result = [{'text': final_text, 'reason': reason_str}]\n",
    "        return (result, flags)\n",
    "\n",
    "    return ([], flags)\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 4: MAIN CONTROLLER \n",
    "# ==============================================================================\n",
    "def analyze_syllabus(file_path):\n",
    "    \"\"\"\n",
    "    Analyzes a syllabus and returns a structured list with the result and binary flags.\n",
    "    \n",
    "    Returns:\n",
    "        list: [course_code, policy_text, flag_multiple_clusters, flag_llm_failure]\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*20} Analyzing Syllabus: {os.path.basename(file_path)} {'='*20}\")\n",
    "    \n",
    "    flag_multiple_clusters = 0\n",
    "    flag_llm_failure = 0\n",
    "\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    paragraphs = []\n",
    "    if ext == '.doc': paragraphs = extract_paragraphs_from_doc(file_path)\n",
    "    elif ext == '.docx': paragraphs = extract_paragraphs_from_docx(file_path)\n",
    "    elif ext == '.pdf': paragraphs = extract_paragraphs_from_pdf(file_path)\n",
    "    else:\n",
    "        print(f\"Error: Unsupported file type '{ext}'.\")\n",
    "        return [None, None, 0, 0]\n",
    "\n",
    "    if not paragraphs:\n",
    "        print(\"Could not extract any usable text.\")\n",
    "        return [None, None, 0, 0]\n",
    "    \n",
    "    print(f\"Extracted {len(paragraphs)} distinct paragraphs. Analyzing with clustering engine...\")\n",
    "    course_code = find_course_code_rule_based(paragraphs)\n",
    "    \n",
    "    # ADDED: determine department & knowledge area\n",
    "    if course_code:\n",
    "        prefix = course_code.split()[0]\n",
    "        department = CODE_TO_DEPT.get(prefix, 'Unknown')\n",
    "        knowledge_area = KNOWLEDGE_AREA_MAP.get(department, 'Unknown')\n",
    "    else:\n",
    "        department = 'Unknown'\n",
    "        knowledge_area = 'Unknown'\n",
    "    #############\n",
    "    \n",
    "    ai_policy_sections, cluster_flags = analyze_policy_with_clustering(paragraphs)\n",
    "    if 'MULTIPLE_CLUSTERS' in cluster_flags:\n",
    "        flag_multiple_clusters = 1\n",
    "\n",
    "    policy_text = None\n",
    "    if ai_policy_sections:\n",
    "        policy_block = ai_policy_sections[0]['text']\n",
    "        max_word_count = 150\n",
    "\n",
    "        if len(policy_block.split()) > max_word_count:\n",
    "            print(f\"INFO: Policy block is long ({len(policy_block.split())} words). Engaging LLM.\")\n",
    "            refined_sections, llm_flags = refine_policy_with_ollama(policy_block)\n",
    "\n",
    "            if llm_flags: # This means the list is not empty, indicating a failure\n",
    "                flag_llm_failure = 1\n",
    "                policy_text = None \n",
    "            else:\n",
    "                if refined_sections:\n",
    "                    policy_text = refined_sections[0]['text']\n",
    "        else:\n",
    "            print(\"INFO: Clustered policy block passed quality checks.\")\n",
    "            policy_text = ai_policy_sections[0]['text']\n",
    "    \n",
    "    # flag = 1 if policy_text else 0\n",
    "    \n",
    "    print(\"--- Analysis processing complete. ---\")\n",
    "    return [course_code, department, knowledge_area, policy_text, flag_multiple_clusters, flag_llm_failure]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66f0d860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Analyzing Syllabus: 0Fslu7lGZ8dJG0OYQGf1TgzFMPyFDEv0n5Q96BNq.pdf ====================\n",
      "Successfully extracted paragraphs using coordinate-based method.\n",
      "Extracted 31 distinct paragraphs. Analyzing with clustering engine...\n",
      "INFO: Clustered policy block passed quality checks.\n",
      "--- Analysis processing complete. ---\n",
      "\n",
      "==================== Analyzing Syllabus: 7Y6H93I4L6F5bcplrUpPbf5AjQcypbJfrevTSiNf.doc ====================\n",
      "Extracted 101 distinct paragraphs. Analyzing with clustering engine...\n",
      "--- Analysis processing complete. ---\n",
      "\n",
      "==================== Analyzing Syllabus: 8C2WFwQPrq6vcFU0Yml3vHcL1CYb5HnYcxp5s7A5.docx ====================\n",
      "Extracted 197 distinct paragraphs. Analyzing with clustering engine...\n",
      "--- Analysis processing complete. ---\n",
      "\n",
      "==================== Analyzing Syllabus: UMeRwRyvXAKAH6DwCkTmFyIunq3ti97bJPARlu7C.docx ====================\n",
      "Extracted 182 distinct paragraphs. Analyzing with clustering engine...\n",
      "INFO: Policy block is long (422 words). Engaging LLM.\n",
      "\n",
      "--- Handing off to Ollama model 'deepseek-r1:1.5b' to refine the policy block... ---\n",
      "INFO: Attempt 1 of 3 to query LLM...\n",
      "WARN: Attempt 1 failed. Normalized quote not in context (likely a summary).\n",
      "INFO: Attempt 2 of 3 to query LLM...\n",
      "WARN: Attempt 2 failed. LLM did not use required separators.\n",
      "INFO: Attempt 3 of 3 to query LLM...\n",
      "WARN: Attempt 3 failed. Normalized quote not in context (likely a summary).\n",
      "--- Analysis processing complete. ---\n",
      "Summary CSV written to syllabus_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# Use file folder, the folder name is test, you can change it\n",
    "if __name__ == '__main__':\n",
    "    results = []\n",
    "    for fname in os.listdir('test'):\n",
    "        path = os.path.join('test', fname)\n",
    "        res = analyze_syllabus(path)\n",
    "        if res and res[0]:\n",
    "            code, dept, area, policy, multi, llm_fail = res\n",
    "            results.append({\n",
    "                'Course Code': code,\n",
    "                'Department': dept,\n",
    "                'Knowledge Area': area,\n",
    "                'AI Policy': policy or '',\n",
    "                'Multiple Clusters': multi,\n",
    "                'LLM Failure': llm_fail\n",
    "            })\n",
    "    csv_file = 'syllabus_summary.csv'\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8-sig') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\n",
    "            'Course Code', 'Department', 'Knowledge Area', 'AI Policy', 'Multiple Clusters', 'LLM Failure'\n",
    "        ])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(results)\n",
    "    print(f\"Summary CSV written to {csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5a7786d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Analyzing Syllabus: 8C2WFwQPrq6vcFU0Yml3vHcL1CYb5HnYcxp5s7A5.docx ====================\n",
      "Extracted 197 distinct paragraphs. Analyzing with clustering engine...\n",
      "--- Analysis processing complete. ---\n",
      "\n",
      "==================== Analyzing Syllabus: UMeRwRyvXAKAH6DwCkTmFyIunq3ti97bJPARlu7C.docx ====================\n",
      "Extracted 182 distinct paragraphs. Analyzing with clustering engine...\n",
      "INFO: Policy block is long (422 words). Engaging LLM.\n",
      "\n",
      "--- Handing off to Ollama model 'deepseek-r1:1.5b' to refine the policy block... ---\n",
      "INFO: Attempt 1 of 3 to query LLM...\n",
      "WARN: Attempt 1 failed. Normalized quote not in context (likely a summary).\n",
      "INFO: Attempt 2 of 3 to query LLM...\n",
      "WARN: Attempt 2 failed. Normalized quote not in context (likely a summary).\n",
      "INFO: Attempt 3 of 3 to query LLM...\n",
      "WARN: Attempt 3 failed. Normalized quote not in context (likely a summary).\n",
      "--- Analysis processing complete. ---\n",
      "\n",
      "==================== Analyzing Syllabus: 0Fslu7lGZ8dJG0OYQGf1TgzFMPyFDEv0n5Q96BNq.pdf ====================\n",
      "Successfully extracted paragraphs using coordinate-based method.\n",
      "Extracted 31 distinct paragraphs. Analyzing with clustering engine...\n",
      "INFO: Clustered policy block passed quality checks.\n",
      "--- Analysis processing complete. ---\n",
      "\n",
      "==================== Analyzing Syllabus: 7Y6H93I4L6F5bcplrUpPbf5AjQcypbJfrevTSiNf.doc ====================\n",
      "Extracted 101 distinct paragraphs. Analyzing with clustering engine...\n",
      "--- Analysis processing complete. ---\n",
      "Summary CSV written to syllabus_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# Enter the file name by yourself\n",
    "if __name__ == '__main__':\n",
    "    results = []\n",
    "    files = [\n",
    "        '8C2WFwQPrq6vcFU0Yml3vHcL1CYb5HnYcxp5s7A5.docx',\n",
    "        'UMeRwRyvXAKAH6DwCkTmFyIunq3ti97bJPARlu7C.docx',\n",
    "        '0Fslu7lGZ8dJG0OYQGf1TgzFMPyFDEv0n5Q96BNq.pdf',\n",
    "        '7Y6H93I4L6F5bcplrUpPbf5AjQcypbJfrevTSiNf.doc'\n",
    "    ]\n",
    "    for fname in files:\n",
    "        res = analyze_syllabus(fname)\n",
    "        if res and res[0]:\n",
    "            code, dept, area, policy, multi, llm_fail = res\n",
    "            results.append({\n",
    "                'Course Code': code,\n",
    "                'Department': dept,\n",
    "                'Knowledge Area': area,\n",
    "                'AI Policy': policy or '',\n",
    "                'Multiple Clusters': multi,\n",
    "                'LLM Failure': llm_fail\n",
    "            })\n",
    "    csv_file = 'syllabus_summary.csv'\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8-sig') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\n",
    "            'Course Code', 'Department', 'Knowledge Area', 'AI Policy', 'Multiple Clusters', 'LLM Failure'\n",
    "        ])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(results)\n",
    "    print(f\"Summary CSV written to {csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1af97ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Analyzing Syllabus: 0GIzUorcaUlFtsvHBQmWIgLxidFQogW1Q3jaQJlX.pdf ====================\n",
      "Successfully extracted paragraphs using coordinate-based method.\n",
      "Extracted 46 distinct paragraphs. Analyzing with clustering engine...\n",
      "INFO: Found 2 distinct AI-related clusters. Flagging for review.\n",
      "INFO: Policy block is long (366 words). Engaging LLM.\n",
      "\n",
      "--- Handing off to Ollama model 'deepseek-r1:1.5b' to refine the policy block... ---\n",
      "INFO: Attempt 1 of 3 to query LLM...\n",
      "WARN: Attempt 1 failed. Normalized quote not in context (likely a summary).\n",
      "INFO: Attempt 2 of 3 to query LLM...\n",
      "INFO: LLM refinement successful and passed verification.\n",
      "--- Analysis processing complete. ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ENGL 1901R',\n",
       " 'Unknown',\n",
       " 'Unknown',\n",
       " 'Dear Students, As we delve into the complexities of literary theory this semester, I want to emphasize the importance of engaging deeply with the material and developing your own critical insights. While tools like ChatGPT can provide useful information and summaries, they are not a substitute for your own analytical thinking and original interpretation. Literary theory involves nuanced arguments, diverse perspectives, and sophisticated analysis that go beyond surface-level responses. Relying too heavily on AI tools can lead to superficial understanding and prevent you from cultivating your own voice and critical skills. Instead, I encourage you to approach your studies with curiosity and rigor. Read the primary texts, engage with scholarly debates, and participate actively in class discussions. Your unique perspectives and interpretations are what will truly enrich your understanding of the material and contribute to your growth as a scholar. If you have questions or need guidance, please reach out to me or use the resources available through our course. Letâ€™s work together to make this a rewarding learning experience. Best regards, [Your Name]',\n",
       " 1,\n",
       " 0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_file = \"0GIzUorcaUlFtsvHBQmWIgLxidFQogW1Q3jaQJlX.pdf\" \n",
    "analyze_syllabus(pdf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39be1cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file = \"DIoCCJg4XeH5rJ0HuN9MqZiRNenyFUxa121Il04r.pdf\" \n",
    "analyze_syllabus(pdf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d2c154",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file = \"jnlQIFYUgZhHZIZyILiFAEr02KEuyWJYkquXUvJD.pdf\" \n",
    "analyze_syllabus(pdf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1992a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file = \"PeB4sO9tTTZxAYrxncoLPKoAIOtRa1ew5spr4lCw.pdf\" \n",
    "analyze_syllabus(pdf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2381482f",
   "metadata": {},
   "outputs": [],
   "source": [
    "docx_file = \"HNPK3m8hfi0bcJ1sSa2LHeMNQ3DAxjJBXkSTtomA.docx\" \n",
    "analyze_syllabus(docx_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56a5c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "docx_file = \"T8hj3wlKrzFUjYtz61tDDFxzIIW5B7MFAdgNzy4X.docx\" \n",
    "analyze_syllabus(docx_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3328ea2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_clusters_in_file(file_path):\n",
    "    \"\"\"\n",
    "    An independent function to analyze a file, find all AI-related text clusters,\n",
    "    and print them with their scores for debugging and inspection.\n",
    "    This function does NOT proceed to LLM refinement.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*20} Debugging Clusters for: {os.path.basename(file_path)} {'='*20}\")\n",
    "    \n",
    "    # --- Step 1: Extract Text  \n",
    "    if not os.path.exists(file_path):\n",
    "        print(\"Error: File not found.\")\n",
    "        return\n",
    "\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    paragraphs = []\n",
    "    if ext == '.doc': paragraphs = extract_paragraphs_from_doc(file_path)\n",
    "    elif ext == '.docx': paragraphs = extract_paragraphs_from_docx(file_path)\n",
    "    elif ext == '.pdf': paragraphs = extract_paragraphs_from_pdf(file_path)\n",
    "    else:\n",
    "        print(f\"Error: Unsupported file type '{ext}'.\")\n",
    "        return\n",
    "\n",
    "    if not paragraphs:\n",
    "        print(\"Could not extract any usable text from the file.\")\n",
    "        return\n",
    "        \n",
    "    print(f\"Successfully extracted {len(paragraphs)} paragraphs.\")\n",
    "\n",
    "    # --- Step 2: Find all clusters and their details \n",
    "    triggers = []\n",
    "    for i, para in enumerate(paragraphs):\n",
    "        is_header = HEADER_PATTERN.match(para) and len(para.split()) < 15\n",
    "        contains_ai = AI_TRIGGER_PATTERN.search(para)\n",
    "        contains_policy = POLICY_PATTERN.search(para)\n",
    "        if is_header:\n",
    "            triggers.append({'index': i, 'type': 'header', 'weight': 10}) \n",
    "        elif contains_ai and contains_policy:\n",
    "            triggers.append({'index': i, 'type': 'strong_mention', 'weight': 3})\n",
    "        elif contains_ai:\n",
    "            triggers.append({'index': i, 'type': 'weak_mention', 'weight': 1})\n",
    "    \n",
    "    if not triggers:\n",
    "        print(\"\\n--- No AI-related triggers found in this document. ---\")\n",
    "        return\n",
    "\n",
    "    clusters = []\n",
    "    if triggers:\n",
    "        current_cluster = [triggers[0]]\n",
    "        for i in range(1, len(triggers)):\n",
    "            if triggers[i]['index'] - current_cluster[-1]['index'] <= 3:\n",
    "                current_cluster.append(triggers[i])\n",
    "            else:\n",
    "                clusters.append(current_cluster)\n",
    "                current_cluster = [triggers[i]]\n",
    "        clusters.append(current_cluster)\n",
    "        \n",
    "    if not clusters:\n",
    "        print(\"\\n--- Could not form any clusters from the triggers. ---\")\n",
    "        return\n",
    "\n",
    "    # --- Step 3: Calculate scores and format for printing\n",
    "    all_clusters_details = []\n",
    "    for cluster in clusters:\n",
    "        min_index = min(t['index'] for t in cluster)\n",
    "        max_index = max(t['index'] for t in cluster)\n",
    "        has_header = any(t['type'] == 'header' for t in cluster) or (min_index > 0 and HEADER_PATTERN.match(paragraphs[min_index - 1]))\n",
    "        score = sum(t['weight'] for t in cluster) + (20 if has_header else 0)\n",
    "        \n",
    "        start_index = min_index - 1 if has_header and not any(t['type'] == 'header' for t in cluster) else min_index\n",
    "        end_index = min(len(paragraphs), max_index + 1)\n",
    "        while end_index < len(paragraphs) and len(paragraphs[end_index].split()) > 5:\n",
    "            end_index += 1\n",
    "        \n",
    "        cluster_text = \"\\n\\n\".join(paragraphs[start_index:end_index])\n",
    "        all_clusters_details.append({'score': score, 'text': cluster_text})\n",
    "\n",
    "    # --- Step 4: Print the results beautifully \n",
    "    print(f\"\\n--- Found {len(all_clusters_details)} AI-related cluster(s). Details below: ---\")\n",
    "    sorted_clusters = sorted(all_clusters_details, key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    for i, cluster_info in enumerate(sorted_clusters, 1):\n",
    "        print(f\"\\n--- Cluster #{i} (Score: {cluster_info['score']}) ---\")\n",
    "        print(cluster_info['text'])\n",
    "        print(\"-\" * (25 + len(str(i)) + len(str(cluster_info['score']))))\n",
    "\n",
    "\n",
    "pdf_file = \"0GIzUorcaUlFtsvHBQmWIgLxidFQogW1Q3jaQJlX.pdf\" \n",
    "debug_clusters_in_file(pdf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb77490",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file = \"DIoCCJg4XeH5rJ0HuN9MqZiRNenyFUxa121Il04r.pdf\" \n",
    "debug_clusters_in_file(pdf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2773cf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file = \"jnlQIFYUgZhHZIZyILiFAEr02KEuyWJYkquXUvJD.pdf\" \n",
    "debug_clusters_in_file(pdf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd657c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
