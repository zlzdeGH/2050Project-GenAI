{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57049977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import time\n",
    "from docx import Document\n",
    "import pdfplumber\n",
    "import ollama\n",
    "\n",
    "# --- For Windows users... ---\n",
    "if sys.platform == 'win32':\n",
    "    try: import win32com.client as win32\n",
    "    except ImportError: print(\"Warning: .doc support disabled.\"); win32 = None\n",
    "else: win32 = None\n",
    "\n",
    "# ================================================================================\n",
    "# PART 0: DEPARTMENT & KNOWLEDGE AREA MAPPINGS\n",
    "# ================================================================================\n",
    "DEPARTMENT_MAP = {\n",
    "    'Africana Studies': 'AFRI',\n",
    "    'American Studies': ['AMST', 'ETHN', 'NAIS', 'PHUM', 'STS'],\n",
    "    'Anthropology': 'ANTH',\n",
    "    'Applied Mathematics': 'APMA',\n",
    "    'Archaeology and the Ancient World': 'ARCH',\n",
    "    'Bio-Medical (PLME & MED)': 'MED',\n",
    "    'Biology': 'BIOL',\n",
    "    'Brown Arts Institute': 'ARTS',\n",
    "    'Business, Entrepreneurship, Organizations': 'BEO',\n",
    "    'Center for Language Studies': ['ARAB', 'EINT', 'HNDI', 'LANG', 'PRSN', 'SIGN', 'TKSH', 'YORU'],\n",
    "    'Chemistry': 'CHEM',\n",
    "    'Classics': ['CLAS', 'CREK', 'LATN', 'MGRK', 'SANS'],\n",
    "    'Cognitive, Linguistic, and Psychological Sciences': ['CLPS', 'LING'],\n",
    "    'Cognitive and Psychological Sciences': 'CPSY',\n",
    "    'Cogut Institute for the Humanities': 'HMAN',\n",
    "    'Comparative Literature': 'COLT',\n",
    "    'Computer Science': 'CSCI',\n",
    "    'Data Science Initiative': ['DATA', 'DSIO'],\n",
    "    'Early Modern World': 'EMOW',\n",
    "    'Earth, Environmental and Planetary Sciences': 'EEPS',\n",
    "    'East Asian Studies': ['CHIN', 'EAST', 'JAPN', 'KREA', 'VIET'],\n",
    "    'Economics': 'ECON',\n",
    "    'Education': 'EDUC',\n",
    "    'Egyptology and Assyriology': ['ASYR', 'EGYT'],\n",
    "    'Engineering': 'ENGN',\n",
    "    'English': 'ENGL',\n",
    "    'Environmental Studies': 'ENVS',\n",
    "    'French Studies': 'FREN',\n",
    "    'German Studies': 'GRMN',\n",
    "    'Hispanic Studies': 'HISP',\n",
    "    'History': 'HIST',\n",
    "    'History of Art and Architecture': 'HIAA',\n",
    "    'Italian Studies': 'ITAL',\n",
    "    'Judaic Studies': ['HEBR', 'JUDS'],\n",
    "    'Linguistics': 'LING',\n",
    "    'Literary Arts': 'LITR',\n",
    "    'Mathematics': 'MATH',\n",
    "    'Medieval Studies': 'MDVL',\n",
    "    'Middle East Studies': 'MES',\n",
    "    'Modern Culture and Media': 'MCM',\n",
    "    'Music': 'MUSC',\n",
    "    'Neuroscience': 'NEUR',\n",
    "    'Pembroke Center Teach and Rearch': 'GNSS',\n",
    "    'Philosophy': 'PHIL',\n",
    "    'Physics': 'PHYS',\n",
    "    'Political Science': 'POLS',\n",
    "    'Portuguese and Brazilian Studies': 'POBS',\n",
    "    'Public Health': ['BHDS', 'GPHP', 'HCL', 'PHP'],\n",
    "    'Religious Studies': ['COST', 'RELS'],\n",
    "    'Slavic Studies': ['CZCH','PLSH','RUSS', 'SLAV'],\n",
    "    'Sociology': 'SOC',\n",
    "    'Theatre Arts and Performance Studies': 'TAPS',\n",
    "    'Urban Studies': 'URBN',\n",
    "    'Visual Art': 'VISA',\n",
    "    'Watson Institute': ['IAPA', 'MPA'],\n",
    "}\n",
    "# reverse lookup: code >> dept name\n",
    "CODE_TO_DEPT = {}\n",
    "for dept, codes in DEPARTMENT_MAP.items():\n",
    "    if isinstance(codes, list):\n",
    "        for c in codes:\n",
    "            CODE_TO_DEPT[c] = dept\n",
    "    else:\n",
    "        CODE_TO_DEPT[codes] = dept\n",
    "# map each department to a knowledge area\n",
    "\n",
    "KNOWLEDGE_AREA_MAP = {\n",
    "    'Africana Studies': 'Social Sciences',\n",
    "    'American Studies': 'Social Sciences',\n",
    "    'Anthropology': 'Social Sciences',\n",
    "    'Applied Mathematics': 'Physical Sciences',\n",
    "    'Archaeology and the Ancient World': 'Humanities',\n",
    "    'Bio-Medical (PLME & MED)': 'Life Sciences',\n",
    "    'Biology': 'Life Sciences',\n",
    "    'Brown Arts Institute': 'Arts',\n",
    "    'Business, Entrepreneurship, Organizations': 'Social Sciences',\n",
    "    'Center for Language Studies': 'Humanities',\n",
    "    'Chemistry': 'Physical Sciences',\n",
    "    'Classics': 'Humanities',\n",
    "    'Cognitive and Psychological Sciences': 'Life Sciences',\n",
    "    'Cogut Institute for the Humanities': 'Humanities',\n",
    "    'Comparative Literature': 'Humanities',\n",
    "    'Computer Science': 'Physical Sciences',\n",
    "    'Data Science Initiative': 'Physical Sciences',\n",
    "    'Early Modern World': 'Humanities',\n",
    "    'Earth, Environmental and Planetary Sciences': 'Physical Sciences',\n",
    "    'East Asian Studies': 'Humanities',\n",
    "    'Economics': 'Social Sciences',\n",
    "    'Education': 'Social Sciences',\n",
    "    'Egyptology and Assyriology': 'Humanities',\n",
    "    'Engineering': 'Physical Sciences',\n",
    "    'English': 'Humanities',\n",
    "    'Environmental Studies': 'Physical Sciences',\n",
    "    'French Studies': 'Humanities',\n",
    "    'German Studies': 'Humanities',\n",
    "    'Hispanic Studies': 'Humanities',\n",
    "    'History': 'Social Sciences',\n",
    "    'History of Art and Architecture': 'Humanities',\n",
    "    'Italian Studies': 'Humanities',\n",
    "    'Judaic Studies': 'Humanities',\n",
    "    'Linguistics': 'Social Sciences',\n",
    "    'Literary Arts': 'Humanities',\n",
    "    'Mathematics': 'Physical Sciences',\n",
    "    'Medieval Studies': 'Humanities',\n",
    "    'Middle East Studies': 'Humanities',\n",
    "    'Modern Culture and Media': 'Humanities',\n",
    "    'Music': 'Humanities',\n",
    "    'Neuroscience': 'Life Sciences',\n",
    "    'Pembroke Center Teach and Rearch': 'Humanities',\n",
    "    'Philosophy': 'Humanities',\n",
    "    'Physics': 'Physical Sciences',\n",
    "    'Political Science': 'Social Sciences',\n",
    "    'Portuguese and Brazilian Studies': 'Humanities',\n",
    "    'Public Health': 'Life Sciences',\n",
    "    'Religious Studies': 'Humanities',\n",
    "    'Slavic Studies': 'Humanities',\n",
    "    'Sociology': 'Social Sciences',\n",
    "    'Theatre Arts and Performance Studies': 'Humanities',\n",
    "    'Urban Studies': 'Social Sciences',\n",
    "    'Visual Art': 'Humanities',\n",
    "    'Watson Institute': 'Social Sciences',\n",
    "}\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 1: DEFINITIONS\n",
    "# ==============================================================================\n",
    "AI_CONTEXT_WORDS = ['ai', 'artificial intelligence', 'generative', 'chatgpt', 'llm', 'copilot', 'bard', 'large language model', 'gemini', 'dall-e', 'gpt', 'midjourney', 'stable diffusion', 'ai tool', 'ai tools']\n",
    "POLICY_KEYWORDS = ['academic integrity', 'academic dishonesty', 'plagiarism', 'cheating', 'unauthorized', 'unauthorized use', 'unauthorized assistance', 'citation', 'cite', 'attribution', 'acknowledge', 'permitted', 'allowed', 'prohibited', 'forbidden', 'disclosure', 'ethical use', 'responsible use', 'use', 'using', 'assistance', 'help', 'guidance', 'policy', 'rule']\n",
    "COURSE_CODE_MAP = {\n",
    "    'Africana Studies': 'AFRI',\n",
    "    'American Sign Language': 'SIGN',\n",
    "    'American Studies': 'AMST',\n",
    "    'Anthropology': 'ANTH',\n",
    "    'Applied Mathematics': 'APMA',\n",
    "    'Arabic': 'ARAB',\n",
    "    'Archaeology and the Ancient World': 'ARCH',\n",
    "    'Assyriology': 'ASYR',\n",
    "    'Behavioral and Social Health Sciences': 'BHDS',\n",
    "    'Biology': 'BIOL',\n",
    "    'Bio-Medical (PLME & MED)': 'MED',\n",
    "    'Brown Arts Institute': 'ARTS',\n",
    "    'Business, Entrepreneurship, Organizations': 'BEO',\n",
    "    'Chemistry': 'CHEM',\n",
    "    'Chinese': 'CHIN',\n",
    "    'Classical Greek': 'CREK',\n",
    "    'Classics': 'CLAS',\n",
    "    'Cognitive and Psychological Sciences': 'CPSY',\n",
    "    'Cognitive, Linguistic, and Psychological Sciences': 'CLPS',\n",
    "    'Cogut Institute for the Humanities': 'HMAN',\n",
    "    'Comparative Literature': 'COLT',\n",
    "    'Computer Science': 'CSCI',\n",
    "    'Contemplative Studies': 'COST', \n",
    "    'Czech': 'CZCH',\n",
    "    'Data Science': 'DATA',\n",
    "    'Early Modern World': 'EMOW',\n",
    "    'Earth, Environmental and Planetary Sciences': 'EEPS',\n",
    "    'East Asian Studies': 'EAST',\n",
    "    'Economics': 'ECON', \n",
    "    'Education': 'EDUC',\n",
    "    'Egyptology': 'EGYT',\n",
    "    'Engineering': 'ENGN',\n",
    "    'English': 'ENGL',\n",
    "    'English for International Students': 'EINT', \n",
    "    'Environmental Studies': 'ENVS',\n",
    "    'Ethnic Studies': 'ETHN',\n",
    "    'French': 'FREN',\n",
    "    'Gender and Sexuality Studies': 'GNSS', \n",
    "    'German': 'GRMN',\n",
    "    'Global Public Health': 'GPHP',\n",
    "    'Health Care Leadership': 'HCL', \n",
    "    'Hebrew': 'HEBR',\n",
    "    'Hindi': 'HNDI',\n",
    "    'Hispanic Studies': 'HISP',\n",
    "    'History': 'HIST',\n",
    "    'History of Art and Architecture': 'HIAA',\n",
    "    'International and Public Affairs': 'IAPA', \n",
    "    'Italian': 'ITAL',\n",
    "    'Japanese': 'JAPN',\n",
    "    'Judaic Studies': 'JUDS',\n",
    "    'Korean': 'KREA',\n",
    "    'Language Studies': 'LANG', \n",
    "    'Latin': 'LATN',\n",
    "    'Linguistics': 'LING',\n",
    "    'Literary Arts': 'LITR',\n",
    "    'Master of Public Affairs': 'MPA', \n",
    "    'Mathematics': 'MATH',\n",
    "    'Medieval Studies': 'MDVL',\n",
    "    'Middle East Studies': 'MES',\n",
    "    'Modern Culture and Media': 'MCM',\n",
    "    'Modern Greek': 'MGRK',\n",
    "    'Music': 'MUSC',\n",
    "    'Native American and Indigenous Studies': 'NAIS',\n",
    "    'Neuroscience': 'NEUR',\n",
    "    'Persian': 'PRSN',\n",
    "    'Philosophy': 'PHIL',\n",
    "    'Physics': 'PHYS',\n",
    "    'Polish': 'PLSH',\n",
    "    'Political Science': 'POLS',\n",
    "    'Portuguese and Brazilian Studies': 'POBS',\n",
    "    'Public Health': 'PHP',\n",
    "    'Public Humanities': 'PHUM',\n",
    "    'Religious Studies': 'RELS',\n",
    "    'Russian': 'RUSS',\n",
    "    'Sanskrit': 'SANS',\n",
    "    'Science, Technology, and Society': 'STS',\n",
    "    'Slavic Studies': 'SLAV',\n",
    "    'Sociology': 'SOC',\n",
    "    'Theatre Arts and Performance Studies': 'TAPS',\n",
    "    'Turkish': 'TKSH',\n",
    "    'Urban Studies': 'URBN',\n",
    "    'Vietnamese': 'VIET',\n",
    "    'Visual Art': 'VISA',\n",
    "    'Yoruba': 'YORU'\n",
    "}\n",
    "\n",
    "HEADER_PATTERN = re.compile(\n",
    "    r'.*\\b('\n",
    "    r'(ai|artificial\\sintelligence|generative\\s+ai)\\s+.*\\bpolicy'\n",
    "    r'|'\n",
    "    r'policy\\s+.*\\b(ai|artificial\\sintelligence|generative\\s+ai)'\n",
    "    r')\\b.*',\n",
    "    re.IGNORECASE\n",
    ")\n",
    "AI_TRIGGER_PATTERN = re.compile(r'\\b(' + '|'.join(AI_CONTEXT_WORDS) + r')\\b', re.IGNORECASE)\n",
    "POLICY_PATTERN = re.compile(r'\\b(' + '|'.join(POLICY_KEYWORDS) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 2: TEXT EXTRACTION \n",
    "# ==============================================================================\n",
    "\n",
    "def extract_paragraphs_from_doc(doc_path):\n",
    "    if not win32:\n",
    "        print(f\"Skipping .doc file '{os.path.basename(doc_path)}' as 'pywin32' is not available on this system.\")\n",
    "        return []\n",
    "        \n",
    "    word = None; doc = None\n",
    "    \n",
    "    try:\n",
    "        word = win32.Dispatch(\"Word.Application\")\n",
    "        word.Visible = False\n",
    "        doc = word.Documents.Open(os.path.abspath(doc_path))\n",
    "        return [p.Range.Text.strip() for p in doc.Paragraphs if p.Range.Text.strip()]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing .doc file with MS Word: {e}\")\n",
    "        return []\n",
    "        \n",
    "    finally:\n",
    "        if doc: \n",
    "            try:\n",
    "                doc.Close(False)\n",
    "            except Exception as e_close:\n",
    "                print(f\"  - Warning: Failed to close document object: {e_close}\")\n",
    "\n",
    "        if word: \n",
    "            try:\n",
    "                word.Quit()\n",
    "            except Exception as e_quit:\n",
    "                print(f\"  - Warning: Failed to quit Word application: {e_quit}\")\n",
    "        wait_time = 5 # when it falls, this number can be set larger.\n",
    "        print(f\"  - Pausing for {wait_time} seconds to ensure Word process cleanup...\")\n",
    "        time.sleep(wait_time)\n",
    "        \n",
    "def extract_paragraphs_from_docx(doc_path):\n",
    "    try:\n",
    "        doc = Document(doc_path)\n",
    "        blocks = [p.text for p in doc.paragraphs if p.text.strip()]\n",
    "        for table in doc.tables:\n",
    "            for row in table.rows:\n",
    "                for cell in row.cells:\n",
    "                    if cell.text.strip():\n",
    "                        blocks.append(cell.text)\n",
    "        return blocks\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading DOCX file {doc_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def _reconstruct_paragraphs_from_page(page):\n",
    "    words = page.extract_words(keep_blank_chars=False, x_tolerance=2)\n",
    "    if not words: return []\n",
    "    \n",
    "    lines = {}\n",
    "    for word in words:\n",
    "        line_top = round(word['top'], 2)\n",
    "        if line_top not in lines:\n",
    "            lines[line_top] = []\n",
    "        lines[line_top].append(word)\n",
    "\n",
    "    for line_top in lines:\n",
    "        lines[line_top].sort(key=lambda w: w['x0'])\n",
    "        \n",
    "    sorted_lines = sorted(lines.items(), key=lambda item: item[0])\n",
    "    \n",
    "    reconstructed_lines = []\n",
    "    line_heights = []\n",
    "    last_top = None\n",
    "    for top, words_in_line in sorted_lines:\n",
    "        text = \" \".join(w['text'] for w in words_in_line)\n",
    "        reconstructed_lines.append({'top': top, 'text': text})\n",
    "        if last_top is not None:\n",
    "            line_heights.append(top - last_top)\n",
    "        last_top = top\n",
    "\n",
    "    if not reconstructed_lines: return []\n",
    "\n",
    "    avg_line_height = sum(line_heights) / len(line_heights) if line_heights else 12\n",
    "    paragraph_break_threshold = avg_line_height * 1.5\n",
    "    \n",
    "    page_paragraphs = []\n",
    "    current_paragraph = reconstructed_lines[0]['text']\n",
    "    for i in range(1, len(reconstructed_lines)):\n",
    "        prev_line, curr_line = reconstructed_lines[i-1], reconstructed_lines[i]\n",
    "        if (curr_line['top'] - prev_line['top']) > paragraph_break_threshold:\n",
    "            page_paragraphs.append(current_paragraph)\n",
    "            current_paragraph = curr_line['text']\n",
    "        else:\n",
    "            current_paragraph += \" \" + curr_line['text']\n",
    "    page_paragraphs.append(current_paragraph)\n",
    "    \n",
    "    return page_paragraphs\n",
    "\n",
    "def extract_paragraphs_from_pdf(pdf_path):\n",
    "    all_paragraphs = []\n",
    "    carry_over_paragraph = \"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page_num, page in enumerate(pdf.pages):\n",
    "                paragraphs_on_page = _reconstruct_paragraphs_from_page(page)\n",
    "                if not paragraphs_on_page: continue\n",
    "                if carry_over_paragraph:\n",
    "                    paragraphs_on_page[0] = carry_over_paragraph + \" \" + paragraphs_on_page[0]\n",
    "                    carry_over_paragraph = \"\"\n",
    "                last_para = paragraphs_on_page[-1]\n",
    "                if not last_para.strip().endswith(('.', '?', '!', '\"', \"'\", ')', ':', ';')):\n",
    "                    carry_over_paragraph = paragraphs_on_page.pop()\n",
    "                all_paragraphs.extend(paragraphs_on_page)\n",
    "        if carry_over_paragraph:\n",
    "            all_paragraphs.append(carry_over_paragraph)\n",
    "        print(\"Successfully extracted paragraphs using coordinate-based method.\")\n",
    "        return all_paragraphs\n",
    "    except Exception as e:\n",
    "        print(f\"Coordinate-based parsing failed: {e}. No OCR fallback implemented.\")\n",
    "        return []\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 3: ANALYSIS LOGIC\n",
    "# ==============================================================================\n",
    "\n",
    "def find_course_code_rule_based(paragraphs, search_limit=30):\n",
    "\n",
    "    for long_name, short_code in COURSE_CODE_MAP.items():\n",
    "        pattern = re.compile(\n",
    "            r'\\b' + re.escape(long_name) +\n",
    "            r'\\s+' +\n",
    "            r'(\\d{3,4}[a-zA-Z]?)\\b',\n",
    "            re.IGNORECASE\n",
    "        )\n",
    "        for para in paragraphs[:search_limit]:\n",
    "            if match := pattern.search(para):\n",
    "                return f\"{short_code} {match.group(1).upper()}\"\n",
    "\n",
    "    EXCLUDE_TERMS = ['FALL', 'SPRING', 'WINTER', 'SUMMER', 'ROOM', 'TO', 'AT', 'THE']\n",
    "    exclude_pattern = '|'.join(EXCLUDE_TERMS)\n",
    "\n",
    "    num_pattern_ext = r'(\\d{3,4}[a-zA-Z]?(?:\\s*\\/\\s*\\d{3,4}[a-zA-Z]?)?)'\n",
    "    \n",
    "    all_caps_pattern = re.compile(\n",
    "        r'\\b' +\n",
    "        r'(?!' + exclude_pattern + r'\\b)' +\n",
    "        r'([A-Z]{2,4}(?:\\s*\\/\\s*[A-Z]{2,4})*)' +\n",
    "        r'\\s*-?\\s*' +\n",
    "        num_pattern_ext + r'\\b'\n",
    "    )\n",
    "    compound_pattern = re.compile(\n",
    "        r'\\b(([A-Z]{2,4}\\d{3,4}[a-zA-Z]?)(?:\\s*/\\s*[A-Z]{2,4}\\d{3,4}[a-zA-Z]?)+)\\b'\n",
    "    )\n",
    "    mixed_case_pattern = re.compile(\n",
    "        r'\\b([A-Z][a-z]{1,3})' +\n",
    "        num_pattern_ext + r'\\b'\n",
    "    )\n",
    "    \n",
    "    for para in paragraphs[:search_limit]:\n",
    "        found_high_priority = []\n",
    "        \n",
    "        compound_match = compound_pattern.search(para)\n",
    "        caps_match = all_caps_pattern.search(para)\n",
    "\n",
    "        if compound_match:\n",
    "            raw_code = compound_match.group(1).upper()\n",
    "            parts = [p.strip() for p in raw_code.split('/')]\n",
    "            formatted_parts = [re.sub(r'([A-Z]+)(\\d)', r'\\1 \\2', p, 1) for p in parts]\n",
    "            found_high_priority.append({'start': compound_match.start(), 'code': '/'.join(formatted_parts)})\n",
    "\n",
    "        if caps_match and (not compound_match or caps_match.start() != compound_match.start()):\n",
    "            dept = re.sub(r'\\s*\\/\\s*', '/', caps_match.group(1))\n",
    "            if dept.upper() not in EXCLUDE_TERMS:\n",
    "                 num = re.sub(r'\\s*\\/\\s*', '/', caps_match.group(2).upper())\n",
    "                 found_high_priority.append({'start': caps_match.start(), 'code': f\"{dept} {num}\"})\n",
    "        \n",
    "        if found_high_priority:\n",
    "            earliest_match = min(found_high_priority, key=lambda x: x['start'])\n",
    "            return earliest_match['code']\n",
    "\n",
    "\n",
    "    for para in paragraphs[:search_limit]:\n",
    "        if match := mixed_case_pattern.search(para):\n",
    "            dept = match.group(1).upper()\n",
    "            num = re.sub(r'\\s*\\/\\s*', '/', match.group(2).upper())\n",
    "            return f\"{dept} {num}\"\n",
    "    \n",
    "    return None\n",
    "\n",
    "def find_course_code_position_based(paragraphs, search_limit=30):\n",
    "    \n",
    "    patterns = []\n",
    "    \n",
    "    patterns.append({\n",
    "        'name': 'compound',\n",
    "        'pattern': re.compile(r'\\b(([A-Z]{2,4}\\d{3,4}[a-zA-Z]?)(?:\\s*/\\s*[A-Z]{2,4}\\d{3,4}[a-zA-Z]?)+)\\b')\n",
    "    })\n",
    "\n",
    "    for long_name, short_code in COURSE_CODE_MAP.items():\n",
    "        patterns.append({\n",
    "            'name': 'long_name',\n",
    "            'pattern': re.compile(r'\\b' + re.escape(long_name) + r'\\s+(\\d{3,4}[a-zA-Z]?)\\b', re.IGNORECASE),\n",
    "            'short_code': short_code\n",
    "        })\n",
    "    \n",
    "    EXCLUDE_TERMS = ['FALL', 'SPRING', 'WINTER', 'SUMMER', 'ROOM', 'TO', 'AT', 'THE']\n",
    "    exclude_pattern_str = '|'.join(EXCLUDE_TERMS)\n",
    "    num_pattern_ext = r'(\\d{3,4}[a-zA-Z]?(?:\\s*\\/\\s*\\d{3,4}[a-zA-Z]?)?)'\n",
    "    \n",
    "    patterns.append({\n",
    "        'name': 'all_caps',\n",
    "        'pattern': re.compile(\n",
    "            r'\\b(?!' + exclude_pattern_str + r'\\b)'\n",
    "            r'([A-Z]{2,4}(?:\\s*\\/\\s*[A-Z]{2,4})*)'\n",
    "            r'\\s*-?\\s*' + num_pattern_ext + r'\\b'\n",
    "        )\n",
    "    })\n",
    "    \n",
    "    patterns.append({\n",
    "        'name': 'mixed_case',\n",
    "        'pattern': re.compile(r'\\b([A-Z][a-z]{1,3})' + num_pattern_ext + r'\\b')\n",
    "    })\n",
    "\n",
    "    for para in paragraphs[:search_limit]:\n",
    "        found_matches = []\n",
    "        for p_info in patterns:\n",
    "            for match in p_info['pattern'].finditer(para):\n",
    "                found_matches.append({\n",
    "                    'start': match.start(), 'end': match.end(),\n",
    "                    'pattern_name': p_info['name'], 'match_obj': match,\n",
    "                    'short_code': p_info.get('short_code')\n",
    "                })\n",
    "        \n",
    "        if not found_matches: continue\n",
    "            \n",
    "        final_candidates = []\n",
    "        for i, m1 in enumerate(found_matches):\n",
    "            is_submatch = False\n",
    "            for j, m2 in enumerate(found_matches):\n",
    "                if i == j: continue\n",
    "                if m2['start'] <= m1['start'] and m2['end'] >= m1['end'] and (m2['end']-m2['start'] > m1['end']-m1['start']):\n",
    "                    is_submatch = True\n",
    "                    break\n",
    "            if not is_submatch:\n",
    "                final_candidates.append(m1)\n",
    "\n",
    "        if not final_candidates: continue\n",
    "\n",
    "        earliest_match_info = min(final_candidates, key=lambda x: x['start'])\n",
    "        match = earliest_match_info['match_obj']\n",
    "        name = earliest_match_info['pattern_name']\n",
    "\n",
    "        if name == 'compound':\n",
    "            raw_code = match.group(1).upper()\n",
    "            parts = [p.strip() for p in raw_code.split('/')]\n",
    "            formatted_parts = [re.sub(r'([A-Z]+)(\\d)', r'\\1 \\2', p, 1) for p in parts]\n",
    "            return '/'.join(formatted_parts)\n",
    "        elif name == 'long_name':\n",
    "            return f\"{earliest_match_info['short_code']} {match.group(1).upper()}\"\n",
    "        elif name == 'all_caps':\n",
    "            dept = re.sub(r'\\s*\\/\\s*', '/', match.group(1))\n",
    "            num = re.sub(r'\\s*\\/\\s*', '/', match.group(2).upper())\n",
    "            return f\"{dept} {num}\"\n",
    "        elif name == 'mixed_case':\n",
    "            dept = match.group(1).upper()\n",
    "            num = re.sub(r'\\s*\\/\\s*', '/', match.group(2).upper())\n",
    "            return f\"{dept} {num}\"\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def _normalize_for_comparison(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts text to a 'canonical' form for robust comparison by lowercasing\n",
    "    and removing all non-alphanumeric characters.\n",
    "    \"\"\"\n",
    "    return \"\".join(char.lower() for char in text if char.isalnum())\n",
    "\n",
    "def refine_policy_with_ollama(context_block, model_name=\"deepseek-r1:14b\"):\n",
    "    \"\"\"\n",
    "    Asks the LLM to extract the core policy with retries, progressive prompting,\n",
    "    and robust, normalized verbatim quote verification.\n",
    "    Returns:\n",
    "        tuple: (result, flags)\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Handing off to Ollama model '{model_name}' to refine the policy block... ---\")\n",
    "    \n",
    "    SEPARATOR_START = \"[---POLICY_TEXT_START---]\"\n",
    "    SEPARATOR_END = \"[---POLICY_TEXT_END---]\"\n",
    "    MAX_RETRIES = 3\n",
    "\n",
    "    prompts = [\n",
    "        f\"\"\"Analyze the syllabus section to find the policy on \"Artificial Intelligence\".\n",
    "Instructions:\n",
    "1. Provide a brief, one-sentence explanation.\n",
    "2. Output the start separator: {SEPARATOR_START}\n",
    "3. Quote the complete AI policy verbatim.\n",
    "4. Output the end separator: {SEPARATOR_END}\n",
    "If no policy is found, respond ONLY with \"None\".\n",
    "Syllabus Section:\\n---\\n{context_block}\\n---\"\"\",\n",
    "\n",
    "        f\"\"\"RE-EVALUATION: The previous analysis was likely wrong. The text IS KNOWN to contain a policy on \"Artificial Intelligence\". Locate and extract it.\n",
    "Instructions:\n",
    "1. Explain your corrected finding in one sentence.\n",
    "2. Output the start separator: {SEPARATOR_START}\n",
    "3. Quote the policy verbatim. DO NOT summarize.\n",
    "4. Output the end separator: {SEPARATOR_END}\n",
    "Do not respond \"None\". Find the policy.\n",
    "Syllabus Section:\\n---\\n{context_block}\\n---\"\"\",\n",
    "\n",
    "        f\"\"\"FINAL ATTEMPT: You MUST extract the policy about \"Artificial Intelligence\". It is there. Your task is to EXTRACT it, not to decide if it exists.\n",
    "Instructions:\n",
    "1. Find the rules for using AI.\n",
    "2. Output: {SEPARATOR_START}\n",
    "3. Copy the paragraph(s) exactly.\n",
    "4. Output: {SEPARATOR_END}\n",
    "Extract it now.\n",
    "Syllabus Section:\\n---\\n{context_block}\\n---\"\"\"\n",
    "    ]\n",
    "    \n",
    "    normalized_context = _normalize_for_comparison(context_block)\n",
    "\n",
    "    try:\n",
    "        for attempt in range(MAX_RETRIES):\n",
    "            current_prompt = prompts[attempt]\n",
    "            print(f\"INFO: Attempt {attempt + 1} of {MAX_RETRIES} to query LLM...\")\n",
    "            \n",
    "            response = ollama.chat(model=model_name, messages=[{'role': 'user', 'content': current_prompt}])\n",
    "            llm_response = response['message']['content'].strip()\n",
    "\n",
    "            if (llm_response.strip().upper() == \"NONE\"):\n",
    "                print(f\"WARN: Attempt {attempt + 1} failed. LLM responded 'None'.\")\n",
    "                if attempt < MAX_RETRIES - 1: time.sleep(1)\n",
    "                continue\n",
    "\n",
    "            if SEPARATOR_START in llm_response and SEPARATOR_END in llm_response:\n",
    "                after_start = llm_response.split(SEPARATOR_START, 1)[1]\n",
    "                policy_text = after_start.split(SEPARATOR_END, 1)[0].strip()\n",
    "                \n",
    "                if not policy_text:\n",
    "                     print(f\"WARN: Attempt {attempt + 1} failed. LLM returned an empty policy.\")\n",
    "                     if attempt < MAX_RETRIES - 1: time.sleep(1)\n",
    "                     continue\n",
    "\n",
    "                normalized_policy = _normalize_for_comparison(policy_text)\n",
    "                if normalized_policy not in normalized_context:\n",
    "                    print(f\"WARN: Attempt {attempt + 1} failed. Normalized quote not in context (likely a summary).\")\n",
    "                    if attempt < MAX_RETRIES - 1: time.sleep(1)\n",
    "                    continue\n",
    "\n",
    "                print(\"INFO: LLM refinement successful and passed verification.\")\n",
    "                result = [{'text': policy_text, 'reason': f'Refined by {model_name} (Attempt {attempt+1})'}]\n",
    "                return (result, [])\n",
    "            else:\n",
    "                print(f\"WARN: Attempt {attempt + 1} failed. LLM did not use required separators.\")\n",
    "                if attempt < MAX_RETRIES - 1: time.sleep(1)\n",
    "        \n",
    "        return ([], ['LLM_FINAL_FAILURE'])\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"--- ERROR: Could not connect to Ollama: {e} ---\")\n",
    "        return ([], ['LLM_CONNECTION_ERROR'])\n",
    "\n",
    "def analyze_policy_with_clustering(paragraphs):\n",
    "    \"\"\"\n",
    "    Clusters mentions, finds the best block using a two-tier scoring system,\n",
    "    and returns it along with a flag if multiple clusters were found.\n",
    "    Returns:\n",
    "        tuple: (result, flags)\n",
    "    \"\"\"\n",
    "    flags = []\n",
    "    triggers = []\n",
    "    \n",
    "    for i, para in enumerate(paragraphs):\n",
    "        is_header = HEADER_PATTERN.match(para) and len(para.split()) < 15\n",
    "        contains_ai = AI_TRIGGER_PATTERN.search(para)\n",
    "        contains_policy = POLICY_PATTERN.search(para)\n",
    "        if is_header:\n",
    "            triggers.append({'index': i, 'type': 'header', 'weight': 10}) \n",
    "        elif contains_ai and contains_policy:\n",
    "            triggers.append({'index': i, 'type': 'strong_mention', 'weight': 3})\n",
    "        elif contains_ai:\n",
    "            triggers.append({'index': i, 'type': 'weak_mention', 'weight': 1})\n",
    "    \n",
    "    if not triggers:\n",
    "        return ([], [])\n",
    "\n",
    "    clusters = []\n",
    "    if triggers:\n",
    "        current_cluster = [triggers[0]]\n",
    "        for i in range(1, len(triggers)):\n",
    "            if triggers[i]['index'] - current_cluster[-1]['index'] <= 3:\n",
    "                current_cluster.append(triggers[i])\n",
    "            else:\n",
    "                clusters.append(current_cluster)\n",
    "                current_cluster = [triggers[i]]\n",
    "        clusters.append(current_cluster)\n",
    "\n",
    "    if len(clusters) > 1:\n",
    "        print(f\"INFO: Found {len(clusters)} distinct AI-related clusters. Flagging for review.\")\n",
    "        flags.append('MULTIPLE_CLUSTERS')\n",
    "    \n",
    "    # === MODIFICATION START: Two-tier scoring system ===\n",
    "    best_cluster_info = {'score': -1, 'policy_density': -1, 'block': []}\n",
    "    \n",
    "    for cluster in clusters:\n",
    "        min_index = min(t['index'] for t in cluster)\n",
    "        max_index = max(t['index'] for t in cluster)\n",
    "        \n",
    "        # --- Primary Score Calculation ---\n",
    "        has_header = any(t['type'] == 'header' for t in cluster) or (min_index > 0 and HEADER_PATTERN.match(paragraphs[min_index - 1]))\n",
    "        score = sum(t['weight'] for t in cluster) + (20 if has_header else 0)\n",
    "        \n",
    "        # --- Tie-breaker (Secondary Score) Calculation ---\n",
    "        start_idx_for_text = min_index - 1 if has_header and not any(t['type'] == 'header' for t in cluster) else min_index\n",
    "        end_idx_for_text = min(len(paragraphs), max_index + 1)\n",
    "        cluster_block_text = \"\\n\\n\".join(paragraphs[start_idx_for_text:end_idx_for_text])\n",
    "        num_policy_words = len(POLICY_PATTERN.findall(cluster_block_text))\n",
    "        policy_density = num_policy_words / (len(cluster_block_text.split()) + 1e-6) # Add epsilon to avoid division by zero\n",
    "        \n",
    "        # --- Update Best Cluster based on two-tier logic ---\n",
    "        is_best = False\n",
    "        if score > best_cluster_info['score']:\n",
    "            is_best = True\n",
    "        elif score == best_cluster_info['score'] and policy_density > best_cluster_info['policy_density']:\n",
    "            print(f\"INFO: Tie-breaker activated. New cluster with density {policy_density:.4f} is better than previous {best_cluster_info['policy_density']:.4f}.\")\n",
    "            is_best = True\n",
    "        \n",
    "        if is_best:\n",
    "            start_index = start_idx_for_text\n",
    "            end_index = end_idx_for_text\n",
    "            while end_index < len(paragraphs) and len(paragraphs[end_index].split()) > 5:\n",
    "                end_index += 1\n",
    "                \n",
    "            best_cluster_info = {\n",
    "                'score': score, \n",
    "                'policy_density': policy_density, \n",
    "                'block': paragraphs[start_index:end_index]\n",
    "            }\n",
    "    # === MODIFICATION END ===\n",
    "\n",
    "    if best_cluster_info['block']:\n",
    "        final_text = \"\\n\\n\".join(best_cluster_info['block'])\n",
    "        # Add policy_density to the reason for better debugging\n",
    "        reason_str = (\n",
    "            f\"Clustered Policy Block (Score: {best_cluster_info['score']}, \"\n",
    "            f\"Density: {best_cluster_info['policy_density']:.4f})\"\n",
    "        )\n",
    "        result = [{'text': final_text, 'reason': reason_str}]\n",
    "        return (result, flags)\n",
    "\n",
    "    return ([], flags)\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 4: MAIN CONTROLLER \n",
    "# ==============================================================================\n",
    "def analyze_syllabus(file_path, rule_based = True):\n",
    "    \"\"\"\n",
    "    Analyzes a syllabus and returns a structured list with the result and binary flags.\n",
    "    \n",
    "    Returns:\n",
    "        list: [course_code, policy_text, flag_multiple_clusters, flag_llm_failure]\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*20} Analyzing Syllabus: {os.path.basename(file_path)} {'='*20}\")\n",
    "    \n",
    "    flag_multiple_clusters = 0\n",
    "    flag_llm_failure = 0\n",
    "\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    paragraphs = []\n",
    "    if ext == '.doc': paragraphs = extract_paragraphs_from_doc(file_path)\n",
    "    elif ext == '.docx': paragraphs = extract_paragraphs_from_docx(file_path)\n",
    "    elif ext == '.pdf': paragraphs = extract_paragraphs_from_pdf(file_path)\n",
    "    else:\n",
    "        print(f\"Error: Unsupported file type '{ext}'.\")\n",
    "        return [None,'Unknown','Unknown', None, 0, 0]\n",
    "\n",
    "    if not paragraphs:\n",
    "        print(\"Could not extract any usable text.\")\n",
    "        return [None,'Unknown','Unknown', None, 0, 0]\n",
    "    \n",
    "    print(f\"Extracted {len(paragraphs)} distinct paragraphs. Analyzing with clustering engine...\")\n",
    "    if rule_based:\n",
    "        course_code = find_course_code_rule_based(paragraphs)\n",
    "    else:\n",
    "        course_code = find_course_code_position_based(paragraphs)\n",
    "    # ADDED: determine department & knowledge area\n",
    "    if course_code:\n",
    "        prefix = course_code.split()[0]\n",
    "        department = CODE_TO_DEPT.get(prefix, 'Unknown')\n",
    "        knowledge_area = KNOWLEDGE_AREA_MAP.get(department, 'Unknown')\n",
    "    else:\n",
    "        department = 'Unknown'\n",
    "        knowledge_area = 'Unknown'\n",
    "    #############\n",
    "    \n",
    "    ai_policy_sections, cluster_flags = analyze_policy_with_clustering(paragraphs)\n",
    "    if 'MULTIPLE_CLUSTERS' in cluster_flags:\n",
    "        flag_multiple_clusters = 1\n",
    "\n",
    "    policy_text = None\n",
    "    if ai_policy_sections:\n",
    "        policy_block = ai_policy_sections[0]['text']\n",
    "        max_word_count = 150\n",
    "\n",
    "        if len(policy_block.split()) > max_word_count:\n",
    "            print(f\"INFO: Policy block is long ({len(policy_block.split())} words). Engaging LLM.\")\n",
    "            refined_sections, llm_flags = refine_policy_with_ollama(policy_block)\n",
    "\n",
    "            if llm_flags: # This means the list is not empty, indicating a failure\n",
    "                flag_llm_failure = 1\n",
    "                policy_text = None \n",
    "            else:\n",
    "                if refined_sections:\n",
    "                    policy_text = refined_sections[0]['text']\n",
    "        else:\n",
    "            print(\"INFO: Clustered policy block passed quality checks.\")\n",
    "            policy_text = ai_policy_sections[0]['text']\n",
    "    \n",
    "    # flag = 1 if policy_text else 0\n",
    "    \n",
    "    print(\"--- Analysis processing complete. ---\")\n",
    "    return [course_code, department, knowledge_area, policy_text, flag_multiple_clusters, flag_llm_failure]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f0d860",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    base_dir = 'AY 2023-2024'\n",
    "    for i in range(1,16):\n",
    "        folder_name = f'part {i}'\n",
    "        dir = os.path.join(base_dir, folder_name)\n",
    "        results = []\n",
    "        for fname in os.listdir(dir):\n",
    "            path = os.path.join(dir, fname)\n",
    "            res = analyze_syllabus(path)\n",
    "            code, dept, area, policy, multi, llm_fail = res\n",
    "            results.append({\n",
    "                'File': os.path.basename(fname),\n",
    "                'Course Code': code,\n",
    "                'Department': dept,\n",
    "                'Knowledge Area': area,\n",
    "                'AI Policy': policy or '',\n",
    "                'Multiple Clusters': multi,\n",
    "                'LLM Failure': llm_fail\n",
    "            })\n",
    "            \n",
    "        output_dir = f'Policy text/{folder_name}'\n",
    "        os.makedirs(output_dir, exist_ok=True)  \n",
    "        csv_file = f'Policy text/{folder_name}/syllabus_summary_1.csv'\n",
    "        with open(csv_file, 'w', newline='', encoding='utf-8-sig') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=[\n",
    "                'File', 'Course Code', 'Department', 'Knowledge Area', 'AI Policy', 'Multiple Clusters', 'LLM Failure'\n",
    "            ])\n",
    "            writer.writeheader()\n",
    "            writer.writerows(results)\n",
    "        print(f\"Summary CSV written to {csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279ebe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    base_dir = 'AY 2023-2024'\n",
    "    for i in range(1,16):\n",
    "        folder_name = f'part {i}'\n",
    "        dir = os.path.join(base_dir, folder_name)\n",
    "        results = []\n",
    "        for fname in os.listdir(dir):\n",
    "            path = os.path.join(dir, fname)\n",
    "            res = analyze_syllabus(path, rule_based=False)  # this line is different from other 4 attempts\n",
    "            code, dept, area, policy, multi, llm_fail = res\n",
    "            results.append({\n",
    "                'File': os.path.basename(fname),\n",
    "                'Course Code': code,\n",
    "                'Department': dept,\n",
    "                'Knowledge Area': area,\n",
    "                'AI Policy': policy or '',\n",
    "                'Multiple Clusters': multi,\n",
    "                'LLM Failure': llm_fail\n",
    "            })\n",
    "            \n",
    "        output_dir = f'Policy text/{folder_name}'\n",
    "        os.makedirs(output_dir, exist_ok=True)  \n",
    "        csv_file = f'Policy text/{folder_name}/syllabus_summary_2.csv'\n",
    "        with open(csv_file, 'w', newline='', encoding='utf-8-sig') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=[\n",
    "                'File', 'Course Code', 'Department', 'Knowledge Area', 'AI Policy', 'Multiple Clusters', 'LLM Failure'\n",
    "            ])\n",
    "            writer.writeheader()\n",
    "            writer.writerows(results)\n",
    "        print(f\"Summary CSV written to {csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39df5597",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    base_dir = 'AY 2023-2024'\n",
    "    for i in range(1,16):\n",
    "        folder_name = f'part {i}'\n",
    "        dir = os.path.join(base_dir, folder_name)\n",
    "        results = []\n",
    "        for fname in os.listdir(dir):\n",
    "            path = os.path.join(dir, fname)\n",
    "            res = analyze_syllabus(path)\n",
    "            code, dept, area, policy, multi, llm_fail = res\n",
    "            results.append({\n",
    "                'File': os.path.basename(fname),\n",
    "                'Course Code': code,\n",
    "                'Department': dept,\n",
    "                'Knowledge Area': area,\n",
    "                'AI Policy': policy or '',\n",
    "                'Multiple Clusters': multi,\n",
    "                'LLM Failure': llm_fail\n",
    "            })\n",
    "            \n",
    "        output_dir = f'Policy text/{folder_name}'\n",
    "        os.makedirs(output_dir, exist_ok=True)  \n",
    "        csv_file = f'Policy text/{folder_name}/syllabus_summary_3.csv'\n",
    "        with open(csv_file, 'w', newline='', encoding='utf-8-sig') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=[\n",
    "                'File', 'Course Code', 'Department', 'Knowledge Area', 'AI Policy', 'Multiple Clusters', 'LLM Failure'\n",
    "            ])\n",
    "            writer.writeheader()\n",
    "            writer.writerows(results)\n",
    "        print(f\"Summary CSV written to {csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a131d4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    base_dir = 'AY 2023-2024'\n",
    "    for i in range(1,16):\n",
    "        folder_name = f'part {i}'\n",
    "        dir = os.path.join(base_dir, folder_name)\n",
    "        results = []\n",
    "        for fname in os.listdir(dir):\n",
    "            path = os.path.join(dir, fname)\n",
    "            res = analyze_syllabus(path)\n",
    "            code, dept, area, policy, multi, llm_fail = res\n",
    "            results.append({\n",
    "                'File': os.path.basename(fname),\n",
    "                'Course Code': code,\n",
    "                'Department': dept,\n",
    "                'Knowledge Area': area,\n",
    "                'AI Policy': policy or '',\n",
    "                'Multiple Clusters': multi,\n",
    "                'LLM Failure': llm_fail\n",
    "            })\n",
    "            \n",
    "        output_dir = f'Policy text/{folder_name}'\n",
    "        os.makedirs(output_dir, exist_ok=True)  \n",
    "        csv_file = f'Policy text/{folder_name}/syllabus_summary_4.csv'\n",
    "        with open(csv_file, 'w', newline='', encoding='utf-8-sig') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=[\n",
    "                'File', 'Course Code', 'Department', 'Knowledge Area', 'AI Policy', 'Multiple Clusters', 'LLM Failure'\n",
    "            ])\n",
    "            writer.writeheader()\n",
    "            writer.writerows(results)\n",
    "        print(f\"Summary CSV written to {csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dafc74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    base_dir = 'AY 2023-2024'\n",
    "    for i in range(1,16):\n",
    "        folder_name = f'part {i}'\n",
    "        dir = os.path.join(base_dir, folder_name)\n",
    "        results = []\n",
    "        for fname in os.listdir(dir):\n",
    "            path = os.path.join(dir, fname)\n",
    "            res = analyze_syllabus(path)\n",
    "            code, dept, area, policy, multi, llm_fail = res\n",
    "            results.append({\n",
    "                'File': os.path.basename(fname),\n",
    "                'Course Code': code,\n",
    "                'Department': dept,\n",
    "                'Knowledge Area': area,\n",
    "                'AI Policy': policy or '',\n",
    "                'Multiple Clusters': multi,\n",
    "                'LLM Failure': llm_fail\n",
    "            })\n",
    "            \n",
    "        output_dir = f'Policy text/{folder_name}'\n",
    "        os.makedirs(output_dir, exist_ok=True)  \n",
    "        csv_file = f'Policy text/{folder_name}/syllabus_summary_5.csv'\n",
    "        with open(csv_file, 'w', newline='', encoding='utf-8-sig') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=[\n",
    "                'File', 'Course Code', 'Department', 'Knowledge Area', 'AI Policy', 'Multiple Clusters', 'LLM Failure'\n",
    "            ])\n",
    "            writer.writeheader()\n",
    "            writer.writerows(results)\n",
    "        print(f\"Summary CSV written to {csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca33298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a csv for 1 particular part\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     base_dir = 'AY 2023-2024'\n",
    "#     folder_name = 'part 14' \n",
    "\n",
    "#     dir = os.path.join(base_dir, folder_name)\n",
    "#     results = []\n",
    "    \n",
    "#     for fname in os.listdir(dir):\n",
    "#         path = os.path.join(dir, fname)\n",
    "#         res = analyze_syllabus(path)\n",
    "#         code, dept, area, policy, multi, llm_fail = res\n",
    "#         results.append({\n",
    "#             'File': os.path.basename(fname),\n",
    "#             'Course Code': code,\n",
    "#             'Department': dept,\n",
    "#             'Knowledge Area': area,\n",
    "#             'AI Policy': policy or '',\n",
    "#             'Multiple Clusters': multi,\n",
    "#             'LLM Failure': llm_fail\n",
    "#         })\n",
    "            \n",
    "#     output_dir = f'Policy text/{folder_name}'\n",
    "#     os.makedirs(output_dir, exist_ok=True)  \n",
    "#     csv_file = f'Policy text/{folder_name}/syllabus_summary_1.csv'\n",
    "#     with open(csv_file, 'w', newline='', encoding='utf-8-sig') as f:\n",
    "#         writer = csv.DictWriter(f, fieldnames=[\n",
    "#             'File', 'Course Code', 'Department', 'Knowledge Area', 'AI Policy', 'Multiple Clusters', 'LLM Failure'\n",
    "#         ])\n",
    "#         writer.writeheader()\n",
    "#         writer.writerows(results)\n",
    "#     print(f\"Summary CSV written to {csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b48e97d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
