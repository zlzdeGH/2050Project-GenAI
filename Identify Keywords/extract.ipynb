{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebdb1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Method 1: TF-IDF Analysis ---\n",
      "Top 20 most important keywords from TF-IDF:\n",
      "             term      score\n",
      "0              ai  22.733130\n",
      "1             use  13.454727\n",
      "2            tool  12.402703\n",
      "3            work   8.554997\n",
      "4         ai tool   8.382823\n",
      "5         writing   7.994337\n",
      "6      generative   7.634102\n",
      "7         student   7.321167\n",
      "8      assignment   7.315885\n",
      "9   generative ai   6.840661\n",
      "10         course   6.819969\n",
      "11         use ai   6.639366\n",
      "12       academic   6.478842\n",
      "13        chatgpt   6.205947\n",
      "14         policy   6.063316\n",
      "15           used   5.903841\n",
      "16            may   5.855267\n",
      "17           must   5.073493\n",
      "18          using   4.870352\n",
      "19          class   4.677365\n",
      "\n",
      "--- Running Method 2: KeyBERT Analysis ---\n",
      "Top 20 most important keywords from KeyBERT:\n",
      "                                                 term   score\n",
      "0                                          student ai  0.5461\n",
      "1                                   essay constitutes  0.3613\n",
      "2   httpswwwnewcastleeduaucurrentstudentsstudyesse...  0.3270\n",
      "3                                  help documentation  0.3121\n",
      "4                                        class permit  0.3015\n",
      "5                                 conversation refine  0.2809\n",
      "6                                  code conductpolicy  0.2548\n",
      "7                            authorization generative  0.2543\n",
      "8                                  workflow potential  0.2514\n",
      "9                              discipline recommended  0.2511\n",
      "10                                   submission avoid  0.2424\n",
      "11                                   circumstance use  0.2349\n",
      "12                          industryspecific keywords  0.2320\n",
      "13                                      citation rule  0.2254\n",
      "14                                       provide bulk  0.2193\n",
      "15                               creativity awareness  0.2168\n",
      "16                                        msus policy  0.1957\n",
      "17                                   use autocomplete  0.1893\n",
      "18                                          use nonai  0.1851\n",
      "19                                   necessary intext  0.1839\n",
      "\n",
      "--- Running Method 3: LDA Topic Modeling ---\n",
      "Discovered 6 topics. Keywords for each topic:\n",
      "    topic_id        term\n",
      "0          0         use\n",
      "1          0     writing\n",
      "2          0        help\n",
      "3          0        tool\n",
      "4          0      course\n",
      "5          0  assignment\n",
      "6          0         ask\n",
      "7          0        make\n",
      "8          0         may\n",
      "9          0        work\n",
      "10         1         use\n",
      "11         1        tool\n",
      "12         1        work\n",
      "13         1  assignment\n",
      "14         1         may\n",
      "15         1        used\n",
      "16         1    academic\n",
      "17         1      policy\n",
      "18         1     student\n",
      "19         1     chatgpt\n",
      "20         2         use\n",
      "21         2        tool\n",
      "22         2     student\n",
      "23         2     writing\n",
      "24         2    academic\n",
      "25         2        work\n",
      "26         2      policy\n",
      "27         2        must\n",
      "28         2      course\n",
      "29         2  generative\n",
      "30         3     writing\n",
      "31         3     chatgpt\n",
      "32         3  technology\n",
      "33         3       class\n",
      "34         3  instructor\n",
      "35         3      expect\n",
      "36         3      course\n",
      "37         3        work\n",
      "38         3    creative\n",
      "39         3     process\n",
      "40         4         use\n",
      "41         4        tool\n",
      "42         4        work\n",
      "43         4      course\n",
      "44         4  assignment\n",
      "45         4     chatgpt\n",
      "46         4  generative\n",
      "47         4       using\n",
      "48         4     student\n",
      "49         4    academic\n",
      "50         5        tool\n",
      "51         5         use\n",
      "52         5        work\n",
      "53         5  generative\n",
      "54         5         may\n",
      "55         5      course\n",
      "56         5    academic\n",
      "57         5     chatgpt\n",
      "58         5  assignment\n",
      "59         5      policy\n",
      "\n",
      "--- Saving Results to Separate CSV Files ---\n",
      "Successfully saved results to CSV:\n",
      "- tfidf_keywords.csv\n",
      "- keybert_keywords.csv\n",
      "- lda_topic_keywords.csv\n",
      "Successfully saved all keyword analysis results to 'keyword_analysis_results.xlsx'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from docx import Document\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from keybert import KeyBERT\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# --- Download NLTK data \n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# --- Global Variables & Initializations ---\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "doc_path = r\"Syllabi Policies for AI Generative Tools.docx\"\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def extract_policies_from_docx(doc_path):\n",
    "    doc = Document(doc_path)\n",
    "    all_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        if para.text.strip():\n",
    "            all_text.append(para.text.strip())\n",
    "    for table in doc.tables:\n",
    "        for row in table.rows:\n",
    "            for cell in row.cells:\n",
    "                if cell.text.strip():\n",
    "                    all_text.append(cell.text.strip())\n",
    "    return all_text\n",
    "\n",
    "def clean_text_for_tfidf_keybert(text):\n",
    "    text = text.lower()  \n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  \n",
    "    text = re.sub(r'\\d+', '', text) \n",
    "    \n",
    "    tokens = nltk.word_tokenize(text) \n",
    "    \n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    \n",
    "    return \" \".join(lemmatized_tokens)\n",
    "\n",
    "def clean_and_tokenize_for_lda(text):\n",
    "    text = text.lower()  \n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  \n",
    "    text = re.sub(r'\\d+', '', text) \n",
    "    \n",
    "    tokens = nltk.word_tokenize(text) \n",
    "    \n",
    "    return [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words and len(token) > 2]\n",
    "\n",
    "# TF-IDF Analysis \n",
    "print(\"--- Running Method 1: TF-IDF Analysis ---\")\n",
    "all_text_raw = extract_policies_from_docx(doc_path)\n",
    "cleaned_blocks_tfidf = [clean_text_for_tfidf_keybert(block) for block in all_text_raw]\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=2, max_df=0.9)\n",
    "tfidf_matrix = vectorizer.fit_transform(cleaned_blocks_tfidf)\n",
    "tfidf_scores = tfidf_matrix.sum(axis=0).A1\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "tfidf_df = pd.DataFrame({'term': terms, 'score': tfidf_scores})\n",
    "tfidf_df = tfidf_df.sort_values(by='score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"Top 20 most important keywords from TF-IDF:\")\n",
    "print(tfidf_df.head(20))\n",
    "\n",
    "# KeyBERT Analysis \n",
    "print(\"\\n--- Running Method 2: KeyBERT Analysis ---\")\n",
    "kw_model = KeyBERT()\n",
    "policy_text_combined = \" \".join(cleaned_blocks_tfidf)\n",
    "\n",
    "keybert_results = kw_model.extract_keywords(policy_text_combined,\n",
    "                                          keyphrase_ngram_range=(1, 2),\n",
    "                                          stop_words='english',\n",
    "                                          use_mmr=True,\n",
    "                                          diversity=0.7,\n",
    "                                          top_n=50) # Extract more candidates\n",
    "keybert_df = pd.DataFrame(keybert_results, columns=['term', 'score'])\n",
    "print(\"Top 20 most important keywords from KeyBERT:\")\n",
    "print(keybert_df.head(20))\n",
    "\n",
    "# LDA Topic Modeling\n",
    "print(\"\\n--- Running Method 3: LDA Topic Modeling ---\")\n",
    "# LDA requires a list of token lists, so we use a different cleaner\n",
    "tokenized_docs = [clean_and_tokenize_for_lda(block) for block in all_text_raw]\n",
    "\n",
    "# Create Dictionary and Corpus\n",
    "dictionary = corpora.Dictionary(tokenized_docs)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
    "\n",
    "# Build LDA model - num_topics is a parameter you can tune. Let's start with 6.\n",
    "num_topics = 6\n",
    "lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15, random_state=42)\n",
    "\n",
    "# Get topics and format them into a DataFrame\n",
    "topics = lda_model.print_topics(num_words=10)\n",
    "lda_results = []\n",
    "for i, topic in enumerate(topics):\n",
    "    # Parse the string output from gensim\n",
    "    terms = re.findall(r'\"(.*?)\"', topic[1])\n",
    "    for term in terms:\n",
    "        lda_results.append({'topic_id': i, 'term': term})\n",
    "\n",
    "lda_df = pd.DataFrame(lda_results)\n",
    "\n",
    "print(f\"Discovered {num_topics} topics. Keywords for each topic:\")\n",
    "print(lda_df)\n",
    "\n",
    "#  Combine and Export All Results to Excel\n",
    "print(\"\\n--- Saving Results to Separate CSV Files ---\")\n",
    "\n",
    "\n",
    "tfidf_csv_path = \"tfidf_keywords.csv\"\n",
    "keybert_csv_path = \"keybert_keywords.csv\"\n",
    "lda_csv_path = \"lda_topic_keywords.csv\"\n",
    "\n",
    "tfidf_df.head(100).to_csv(tfidf_csv_path, index=False)\n",
    "keybert_df.head(100).to_csv(keybert_csv_path, index=False)\n",
    "lda_df.to_csv(lda_csv_path, index=False)\n",
    "\n",
    "print(\"Successfully saved results to CSV:\")\n",
    "print(f\"- {tfidf_csv_path}\")\n",
    "print(f\"- {keybert_csv_path}\")\n",
    "print(f\"- {lda_csv_path}\")\n",
    "\n",
    "\n",
    "print(f\"Successfully saved all keyword analysis results to '{output_excel_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550c1828",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
