{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61230e8e",
   "metadata": {},
   "source": [
    "## Keyword Strategy Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8a82fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "PRIMARY_KEYWORDS = [\n",
    "    'ai policy', 'aigenerated content', 'ai-generated', 'ai-assisted',\n",
    "    'generative ai', 'llm',\n",
    "]\n",
    "AI_CONTEXT_WORDS = [\n",
    "    'ai', 'artificial intelligence', 'generative', 'chatgpt', 'llm', 'copilot', 'bard', \n",
    "    'large language model' ,\n",
    "    'gemini', 'dall-e', 'gpt', 'midjourney', 'stable diffusion', 'ai tool', 'ai tools'\n",
    "]\n",
    "POLICY_KEYWORDS = [\n",
    "    'academic integrity', 'academic dishonesty', 'plagiarism', 'cheating', 'unauthorized',\n",
    "    'unauthorized use', 'unauthorized assistance', 'citation', 'cite', 'attribution',\n",
    "    'acknowledge', 'permitted', 'allowed', 'prohibited', 'forbidden', 'disclosure', \n",
    "    'ethical use', 'responsible use', 'use', 'using', 'assistance', 'help', 'guidance', \n",
    "    'policy', 'rule'\n",
    "]\n",
    "\n",
    "def find_ai_policy_paragraphs(paragraphs):\n",
    "    \"\"\"\n",
    "    KEEP a paragraph if it contains\n",
    "    a PRIMARY_KEYWORD\n",
    "    OR\n",
    "    (an AI_CONTEXT_WORD AND a POLICY_KEYWORD)\n",
    "    \"\"\"\n",
    "    found_policies = []\n",
    "    \n",
    "    # --- Pre-compile regex patterns for efficiency ---\n",
    "    ai_context_patterns = [\n",
    "        re.compile(r'\\b' + re.escape(kw) + r'\\b', re.IGNORECASE)\n",
    "        for kw in AI_CONTEXT_WORDS\n",
    "    ]\n",
    "\n",
    "    for para in paragraphs:\n",
    "        # Rule A: check PRIMARY keywords (mostly multi-word phrases) ---\n",
    "        para_lower = para.lower()\n",
    "        if any(kw in para_lower for kw in PRIMARY_KEYWORDS):\n",
    "            found_policies.append({\n",
    "                'text': para,\n",
    "                'reason': 'Matched a primary keyword.'\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Rule B: require an AI-context word via regex ---\n",
    "        contains_ai_context = any(pattern.search(para) for pattern in ai_context_patterns)\n",
    "        if not contains_ai_context:\n",
    "            continue\n",
    "            \n",
    "        # Policy keywords check (simple substring search is fine here)\n",
    "        contains_policy_context = any(kw in para_lower for kw in POLICY_KEYWORDS)\n",
    "\n",
    "        if contains_ai_context and contains_policy_context:\n",
    "            found_policies.append({\n",
    "                'text': para,\n",
    "                'reason': 'Matched the combination of AI-context and policy keywords.'\n",
    "            })\n",
    "            \n",
    "    return found_policies\n",
    "\n",
    "sample_paragraphs = [\n",
    "    \"The use of generative AI tools like ChatGPT is permitted for brainstorming, but all submitted work must be original.\",\n",
    "    \"Academic dishonesty, including plagiarism and cheating on exams, will result in a failing grade for the course.\",\n",
    "    \"Students must cite any assistance from AI, as failure to do so is a violation of academic integrity.\",\n",
    "    \"This course will include a lecture on the history of artificial intelligence and its impact on modern society.\",\n",
    "    \"All sources must be properly cited in APA format. Failure to attribute your sources constitutes plagiarism.\"\n",
    "]\n",
    "\n",
    "extracted_policies = find_ai_policy_paragraphs(sample_paragraphs)\n",
    "\n",
    "# --- Print results ---\n",
    "print(f\"--- Found {len(extracted_policies)} AI Policy Paragraphs ---\")\n",
    "for i, policy in enumerate(extracted_policies, 1):\n",
    "    print(f\"\\n{i}. Paragraph:\")\n",
    "    print(f\"   '{policy['text']}'\")\n",
    "    print(f\"   Reason for extraction: {policy['reason']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acb32af",
   "metadata": {},
   "source": [
    "## Original Version for Extracting text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7a2498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from docx import Document\n",
    "import pdfplumber\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "# --- For Windows users, you may need to specify the path to the Tesseract executable ---\n",
    "# Uncomment and update the line below if you are on Windows and Tesseract is not in your PATH\n",
    "# pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 1: KEYWORD DEFINITIONS & MAPPINGS\n",
    "# ==============================================================================\n",
    "PRIMARY_KEYWORDS = ['ai policy', 'aigenerated content', 'ai-generated', 'ai-assisted', 'generative ai']\n",
    "AI_CONTEXT_WORDS = ['ai', 'artificial intelligence', 'generative', 'chatgpt', 'llm', 'copilot', 'bard', 'large language model', 'gemini', 'dall-e', 'gpt', 'midjourney', 'stable diffusion', 'ai tool', 'ai tools']\n",
    "POLICY_KEYWORDS = ['academic integrity', 'academic dishonesty', 'plagiarism', 'cheating', 'unauthorized', 'unauthorized use', 'unauthorized assistance', 'citation', 'cite', 'attribution', 'acknowledge', 'permitted', 'allowed', 'prohibited', 'forbidden', 'disclosure', 'ethical use', 'responsible use', 'use', 'using', 'assistance', 'help', 'guidance', 'policy', 'rule']\n",
    "\n",
    "DEPARTMENT_MAP = {\n",
    "    'Economics': 'ECON', 'Computer Science': 'CSCI', 'Applied Mathematics': 'APMA',\n",
    "    'Public Health': 'PHP', 'Africana Studies': 'AFRI', \n",
    "    'Cognitive, Linguistic, and Psychological Sciences': 'CLPS',\n",
    "    # Add more mappings here for Phase 1 search\n",
    "}\n",
    "\n",
    "def build_checkers():\n",
    "    primary_checker = re.compile(r'|'.join([kw.replace(' ', r'\\s*') for kw in PRIMARY_KEYWORDS]), re.IGNORECASE)\n",
    "    ai_checker = re.compile(r'\\b(' + '|'.join(AI_CONTEXT_WORDS) + r')\\b', re.IGNORECASE)\n",
    "    policy_checker = re.compile(r'\\b(' + '|'.join(POLICY_KEYWORDS) + r')\\b', re.IGNORECASE)\n",
    "    return primary_checker, ai_checker, policy_checker\n",
    "\n",
    "PRIMARY_CHECKER, AI_CHECKER, POLICY_CHECKER = build_checkers()\n",
    "\n",
    "def is_policy_text(text):\n",
    "    if PRIMARY_CHECKER.search(text): return 'Matched a primary keyword.'\n",
    "    if AI_CHECKER.search(text) and POLICY_CHECKER.search(text): return 'Matched the combination of AI-context and policy keywords.'\n",
    "    return None\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 2: TEXT EXTRACTION\n",
    "# ==============================================================================\n",
    "def extract_paragraphs_from_docx(doc_path):\n",
    "    \"\"\"Extracts a clean list of paragraphs from a .docx file.\"\"\"\n",
    "    try:\n",
    "        doc = Document(doc_path); blocks = [p.text for p in doc.paragraphs if p.text.strip()]\n",
    "        for table in doc.tables:\n",
    "            for row in table.rows:\n",
    "                for cell in row.cells:\n",
    "                    if cell.text.strip(): blocks.append(cell.text)\n",
    "        return blocks\n",
    "    except Exception as e: print(f\"Error reading DOCX file {doc_path}: {e}\"); return []\n",
    "\n",
    "def extract_lines_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts a flat list of all text lines from a PDF for line-based analysis.\"\"\"\n",
    "    all_lines = []\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text(x_tolerance=2)\n",
    "                if page_text: all_lines.extend(page_text.split('\\n'))\n",
    "        if len(\"\".join(all_lines).strip()) > 100: print(\"Successfully extracted text using standard method.\"); return all_lines\n",
    "        print(\"Standard method yielded little text. Attempting OCR...\")\n",
    "    except Exception: print(\"Standard PDF reading failed. Attempting OCR...\")\n",
    "    try:\n",
    "        images = convert_from_path(pdf_path)\n",
    "        for image in images:\n",
    "            page_text = pytesseract.image_to_string(image)\n",
    "            all_lines.extend(page_text.split('\\n'))\n",
    "        if all_lines: print(\"Successfully extracted text using OCR.\")\n",
    "        return all_lines\n",
    "    except Exception as e: print(f\"OCR processing failed: {e}\"); return []\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 3: ANALYSIS LOGIC (With All Fixes)\n",
    "# ==============================================================================\n",
    "\n",
    "def find_course_code(paragraphs, search_limit=30):\n",
    "    \"\"\"FIXED: Finds course codes, including cross-listed ones, using a prioritized strategy.\"\"\"\n",
    "    # Phase 1: High-priority search for \"Full Department Name + Number\"\n",
    "    for long_name, short_code in DEPARTMENT_MAP.items():\n",
    "        pattern = re.compile(r'\\b' + re.escape(long_name) + r'\\s*(\\d{3,4}[A-Z]?)\\b', re.IGNORECASE)\n",
    "        for para in paragraphs[:search_limit]:\n",
    "            match = pattern.search(para)\n",
    "            if match: return f\"{short_code} {match.group(1)}\"\n",
    "\n",
    "    # Phase 2: Fallback search for \"ABBR/ABBR 1234\" format\n",
    "    # This pattern now handles single and multiple department codes\n",
    "    fallback_pattern = re.compile(r'\\b(([A-Z]{2,4}(\\s*/\\s*[A-Z]{2,4})*))\\s*(\\d{3,4}[A-Z]?)\\b')\n",
    "    for para in paragraphs[:search_limit]:\n",
    "        match = fallback_pattern.search(para)\n",
    "        if match:\n",
    "            dept_part = match.group(1).replace(\" \", \"\") # Remove spaces around slashes\n",
    "            num_part = match.group(4)\n",
    "            return f\"{dept_part} {num_part}\"\n",
    "    return None\n",
    "\n",
    "def analyze_docx_paragraphs(paragraphs):\n",
    "    \"\"\"Direct analysis for structured DOCX paragraphs.\"\"\"\n",
    "    found_policies = []\n",
    "    for para in paragraphs:\n",
    "        reason = is_policy_text(para)\n",
    "        if reason: found_policies.append({'text': para, 'reason': reason})\n",
    "    return found_policies\n",
    "\n",
    "def analyze_pdf_lines(lines, context_window=3):\n",
    "    \"\"\"RESTORED: Implements your superior 'scan-expand-trim' logic for PDFs.\"\"\"\n",
    "    found_policies = []; processed_indices = set()\n",
    "    trigger_regex = re.compile(r'\\b(' + '|'.join(AI_CONTEXT_WORDS) + r')\\b', re.IGNORECASE)\n",
    "    lines_of_interest = {i: trigger_regex.search(line) for i, line in enumerate(lines) if trigger_regex.search(line)}\n",
    "\n",
    "    for i in lines_of_interest:\n",
    "        if i in processed_indices: continue\n",
    "        start = max(0, i - context_window)\n",
    "        end = min(len(lines), i + context_window + 1)\n",
    "        context_block = \" \".join(line.strip() for line in lines[start:end])\n",
    "        \n",
    "        temp_trigger_pos = len(\" \".join(line.strip() for line in lines[start:i]))\n",
    "        start_sentence = context_block.rfind('.', 0, temp_trigger_pos) + 1\n",
    "        end_sentence = context_block.find('.', temp_trigger_pos)\n",
    "        end_sentence = len(context_block) if end_sentence == -1 else end_sentence + 1\n",
    "        candidate_sentence = context_block[start_sentence:end_sentence].strip()\n",
    "\n",
    "        if candidate_sentence:\n",
    "            reason = is_policy_text(candidate_sentence)\n",
    "            if reason:\n",
    "                found_policies.append({'text': candidate_sentence, 'reason': reason})\n",
    "                for j in range(start, end): processed_indices.add(j)\n",
    "    return found_policies\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 4: MAIN CONTROLLER\n",
    "# ==============================================================================\n",
    "\n",
    "def analyze_syllabus(file_path):\n",
    "    print(f\"\\n{'='*20} Analyzing Syllabus: {os.path.basename(file_path)} {'='*20}\")\n",
    "    if not os.path.exists(file_path): print(\"Error: File not found.\"); return\n",
    "\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    paragraphs, lines = None, None\n",
    "    if ext == '.docx':\n",
    "        paragraphs = extract_paragraphs_from_docx(file_path)\n",
    "    elif ext == '.pdf':\n",
    "        lines = extract_lines_from_pdf(file_path)\n",
    "        # For course code finding, we can treat lines as paragraphs\n",
    "        paragraphs = [line for line in lines if line.strip()] \n",
    "    else:\n",
    "        print(f\"Error: Unsupported file type '{ext}'.\"); return\n",
    "\n",
    "    if not paragraphs: print(\"Could not extract any usable text.\"); return\n",
    "        \n",
    "    print(f\"Extracted {len(paragraphs)} text blocks. Analyzing...\")\n",
    "    \n",
    "    course_code = find_course_code(paragraphs)\n",
    "    print(f\"--- Course Code: {course_code if course_code else 'Not Found'} ---\")\n",
    "\n",
    "    ai_policy_sections = []\n",
    "    if ext == '.docx':\n",
    "        ai_policy_sections = analyze_docx_paragraphs(paragraphs)\n",
    "    elif ext == '.pdf':\n",
    "        ai_policy_sections = analyze_pdf_lines(lines)\n",
    "\n",
    "    if not ai_policy_sections:\n",
    "        print(\"\\n--- No AI policy paragraphs were found in this document. ---\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n--- SUCCESS: Found {len(ai_policy_sections)} AI Policy Paragraph(s) ---\\n\")\n",
    "    unique_policies = {p['text']: p for p in ai_policy_sections}.values()\n",
    "    for i, policy in enumerate(unique_policies, 1):\n",
    "        print(f\"--- Relevant Paragraph {i} ---\"); print(policy['text']); print(f\"(Reason: {policy['reason']})\\n\")\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 5: EXECUTION\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    docx_file1 = \"8C2WFwQPrq6vcFU0Yml3vHcL1CYb5HnYcxp5s7A5.docx\" \n",
    "    analyze_syllabus(docx_file1)\n",
    "    print(\"\\n\" * 3)\n",
    "\n",
    "    docx_file2 = \"s5NpVVwHUlH3SB7MbcSmcgkfuHK796YdA4F94ZQd.docx\"\n",
    "    analyze_syllabus(docx_file2)\n",
    "    print(\"\\n\" * 3) \n",
    "\n",
    "    pdf_file = \"0Fslu7lGZ8dJG0OYQGf1TgzFMPyFDEv0n5Q96BNq.pdf\" \n",
    "    analyze_syllabus(pdf_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef37f20",
   "metadata": {},
   "source": [
    "## Final version\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabb2eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys \n",
    "from docx import Document\n",
    "import pdfplumber\n",
    "# import pytesseract\n",
    "# from pdf2image import convert_from_path\n",
    "\n",
    "# --- For Windows users, you may need to specify the path to the Tesseract executable ---\n",
    "# Uncomment and update the line below if you are on Windows and Tesseract is not in your PATH\n",
    "# pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "# Conditionally import the library for .doc files, only on Windows\n",
    "if sys.platform == 'win32':\n",
    "    try:\n",
    "        import win32com.client as win32\n",
    "    except ImportError:\n",
    "        print(\"Warning: The 'pywin32' library is not installed. .doc files cannot be processed.\")\n",
    "        print(\"To enable .doc support on Windows, run: pip install pywin32\")\n",
    "        win32 = None\n",
    "else:\n",
    "    win32 = None\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 1: KEYWORD DEFINITIONS & MAPPINGS\n",
    "# ==============================================================================\n",
    "PRIMARY_KEYWORDS = ['ai policy', 'aigenerated content', 'ai-generated', 'ai-assisted', 'generative ai']\n",
    "AI_CONTEXT_WORDS = ['ai', 'artificial intelligence', 'generative', 'chatgpt', 'llm', 'copilot', 'bard', 'large language model', 'gemini', 'dall-e', 'gpt', 'midjourney', 'stable diffusion', 'ai tool', 'ai tools']\n",
    "POLICY_KEYWORDS = ['academic integrity', 'academic dishonesty', 'plagiarism', 'cheating', 'unauthorized', 'unauthorized use', 'unauthorized assistance', 'citation', 'cite', 'attribution', 'acknowledge', 'permitted', 'allowed', 'prohibited', 'forbidden', 'disclosure', 'ethical use', 'responsible use', 'use', 'using', 'assistance', 'help', 'guidance', 'policy', 'rule']\n",
    "\n",
    "DEPARTMENT_MAP = {\n",
    "    'Economics': 'ECON', 'Computer Science': 'CSCI', 'Applied Mathematics': 'APMA',\n",
    "    'Public Health': 'PHP', 'Africana Studies': 'AFRI',\n",
    "    'Cognitive, Linguistic, and Psychological Sciences': 'CLPS',\n",
    "}\n",
    "\n",
    "def build_checkers():\n",
    "    primary_checker = re.compile(r'|'.join([kw.replace(' ', r'\\s*') for kw in PRIMARY_KEYWORDS]), re.IGNORECASE)\n",
    "    ai_checker = re.compile(r'\\b(' + '|'.join(AI_CONTEXT_WORDS) + r')\\b', re.IGNORECASE)\n",
    "    policy_checker = re.compile(r'\\b(' + '|'.join(POLICY_KEYWORDS) + r')\\b', re.IGNORECASE)\n",
    "    return primary_checker, ai_checker, policy_checker\n",
    "\n",
    "PRIMARY_CHECKER, AI_CHECKER, POLICY_CHECKER = build_checkers()\n",
    "\n",
    "def is_policy_text(text):\n",
    "    if PRIMARY_CHECKER.search(text): return 'Matched a primary keyword.'\n",
    "    if AI_CHECKER.search(text) and POLICY_CHECKER.search(text): return 'Matched the combination of AI-context and policy keywords.'\n",
    "    return None\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 2: TEXT EXTRACTION (NOW WITH .DOC SUPPORT)\n",
    "# ==============================================================================\n",
    "def extract_paragraphs_from_doc(doc_path):\n",
    "    \"\"\"\n",
    "    Extracts paragraphs from a .doc file using MS Word automation (Windows only).\n",
    "    \"\"\"\n",
    "    if not win32:\n",
    "        print(f\"Skipping .doc file '{os.path.basename(doc_path)}' as 'pywin32' is not available on this system.\")\n",
    "        print(\"Please manually save it as .docx or .pdf to analyze.\")\n",
    "        return []\n",
    "    \n",
    "    word = None\n",
    "    doc = None\n",
    "    try:\n",
    "        word = win32.Dispatch(\"Word.Application\")\n",
    "        word.Visible = False\n",
    "        # Get the full absolute path, which COM objects often require\n",
    "        abs_path = os.path.abspath(doc_path)\n",
    "        doc = word.Documents.Open(abs_path)\n",
    "        paragraphs = [p.Range.Text.strip() for p in doc.Paragraphs if p.Range.Text.strip()]\n",
    "        return paragraphs\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing .doc file with MS Word: {e}\")\n",
    "        return []\n",
    "    finally:\n",
    "        if doc:\n",
    "            doc.Close(False) # Close the document without saving changes\n",
    "        if word:\n",
    "            word.Quit() # Quit the Word application\n",
    "\n",
    "def extract_paragraphs_from_docx(doc_path):\n",
    "    try:\n",
    "        doc = Document(doc_path); blocks = [p.text for p in doc.paragraphs if p.text.strip()]\n",
    "        for table in doc.tables:\n",
    "            for row in table.rows:\n",
    "                for cell in row.cells:\n",
    "                    if cell.text.strip(): blocks.append(cell.text)\n",
    "        return blocks\n",
    "    except Exception as e: print(f\"Error reading DOCX file {doc_path}: {e}\"); return []\n",
    "\n",
    "def _reconstruct_paragraphs_from_page(page):\n",
    "    \"\"\"Helper function to reconstruct paragraphs on a single page using coordinates.\"\"\"\n",
    "    words = page.extract_words(keep_blank_chars=False, x_tolerance=2)\n",
    "    if not words: return []\n",
    "    lines = {};\n",
    "    for word in words:\n",
    "        line_top = round(word['top'], 2)\n",
    "        if line_top not in lines: lines[line_top] = []\n",
    "        lines[line_top].append(word)\n",
    "    for line_top in lines: lines[line_top].sort(key=lambda w: w['x0'])\n",
    "    sorted_lines = sorted(lines.items(), key=lambda item: item[0])\n",
    "    reconstructed_lines = []; line_heights = []; last_top = None\n",
    "    for top, words_in_line in sorted_lines:\n",
    "        text = \" \".join(w['text'] for w in words_in_line)\n",
    "        reconstructed_lines.append({'top': top, 'text': text})\n",
    "        if last_top is not None: line_heights.append(top - last_top)\n",
    "        last_top = top\n",
    "    if not reconstructed_lines: return []\n",
    "    avg_line_height = sum(line_heights) / len(line_heights) if line_heights else 12\n",
    "    paragraph_break_threshold = avg_line_height * 1.5\n",
    "    page_paragraphs = []; current_paragraph = reconstructed_lines[0]['text']\n",
    "    for i in range(1, len(reconstructed_lines)):\n",
    "        prev_line, curr_line = reconstructed_lines[i-1], reconstructed_lines[i]\n",
    "        if (curr_line['top'] - prev_line['top']) > paragraph_break_threshold:\n",
    "            page_paragraphs.append(current_paragraph)\n",
    "            current_paragraph = curr_line['text']\n",
    "        else:\n",
    "            current_paragraph += \" \" + curr_line['text']\n",
    "    page_paragraphs.append(current_paragraph)\n",
    "    return page_paragraphs\n",
    "\n",
    "def extract_paragraphs_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    FINAL PDF METHOD: Uses coordinate geometry and cross-page stitching\n",
    "    to perfectly reconstruct all paragraphs, even those split across pages.\n",
    "    \"\"\"\n",
    "    all_paragraphs = []\n",
    "    carry_over_paragraph = \"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                paragraphs_on_page = _reconstruct_paragraphs_from_page(page)\n",
    "                if not paragraphs_on_page: continue\n",
    "\n",
    "                # If there's a carry-over from the previous page, stitch it to the first paragraph\n",
    "                if carry_over_paragraph:\n",
    "                    paragraphs_on_page[0] = carry_over_paragraph + \" \" + paragraphs_on_page[0]\n",
    "                    carry_over_paragraph = \"\"\n",
    "\n",
    "                # Check if the last paragraph on THIS page is incomplete\n",
    "                last_para = paragraphs_on_page[-1]\n",
    "                # A simple but effective heuristic: if it doesn't end with punctuation, it's likely incomplete.\n",
    "                if not last_para.strip().endswith(('.', '?', '!', '\"', \"'\", ')', ':', ';')):\n",
    "                    carry_over_paragraph = paragraphs_on_page.pop()\n",
    "\n",
    "                all_paragraphs.extend(paragraphs_on_page)\n",
    "        \n",
    "        # Add any final carry-over from the very last page\n",
    "        if carry_over_paragraph:\n",
    "            all_paragraphs.append(carry_over_paragraph)\n",
    "\n",
    "        if all_paragraphs: print(\"Successfully extracted paragraphs using coordinate-based method.\"); return all_paragraphs\n",
    "        print(\"Coordinate-based method failed. Attempting OCR as last resort...\")\n",
    "    except Exception as e: print(f\"Coordinate-based parsing failed: {e}. Attempting OCR...\")\n",
    "    # try:\n",
    "    #     images = convert_from_path(pdf_path); full_text = \"\"\n",
    "    #     for image in images: full_text += pytesseract.image_to_string(image) + \"\\n\\n\"\n",
    "    #     return [p.strip().replace('\\n', ' ') for p in full_text.split('\\n\\n') if p.strip()]\n",
    "    # except Exception as e: print(f\"OCR processing failed: {e}\"); return []\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 3: ANALYSIS LOGIC\n",
    "# ==============================================================================\n",
    "def find_course_code(paragraphs, search_limit=30):\n",
    "    for long_name, short_code in DEPARTMENT_MAP.items():\n",
    "        pattern = re.compile(r'\\b' + re.escape(long_name) + r'\\s*(\\d{3,4}[A-Z]?)\\b', re.IGNORECASE)\n",
    "        for para in paragraphs[:search_limit]:\n",
    "            if match := pattern.search(para): return f\"{short_code} {match.group(1)}\"\n",
    "    fallback_pattern = re.compile(r'\\b(([A-Z]{2,4}(\\s*/\\s*[A-Z]{2,4})*))\\s*(\\d{3,4}[A-Z]?)\\b')\n",
    "    for para in paragraphs[:search_limit]:\n",
    "        if match := fallback_pattern.search(para):\n",
    "            dept_part = match.group(1).replace(\" \", \"\"); num_part = match.group(4)\n",
    "            return f\"{dept_part} {num_part}\"\n",
    "    return None\n",
    "\n",
    "def analyze_ai_policy(paragraphs):\n",
    "    found_policies = []\n",
    "    for para in paragraphs:\n",
    "        if reason := is_policy_text(para):\n",
    "            found_policies.append({'text': para, 'reason': reason})\n",
    "    return found_policies\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 4: MAIN CONTROLLER\n",
    "# ==============================================================================\n",
    "def analyze_syllabus(file_path):\n",
    "    print(f\"\\n{'='*20} Analyzing Syllabus: {os.path.basename(file_path)} {'='*20}\")\n",
    "    if not os.path.exists(file_path): print(\"Error: File not found.\"); return\n",
    "\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    paragraphs = []\n",
    "    if ext == '.docx': paragraphs = extract_paragraphs_from_docx(file_path)\n",
    "    elif ext == '.pdf': paragraphs = extract_paragraphs_from_pdf(file_path)\n",
    "    elif ext == '.doc': paragraphs = extract_paragraphs_from_doc(file_path) \n",
    "    else: print(f\"Error: Unsupported file type '{ext}'.\"); return\n",
    "\n",
    "    if not paragraphs: print(\"Could not extract any usable text.\"); return\n",
    "    \n",
    "    print(f\"Extracted {len(paragraphs)} distinct paragraphs. Analyzing...\")\n",
    "    course_code = find_course_code(paragraphs)\n",
    "    print(f\"--- Course Code: {course_code if course_code else 'Not Found'} ---\")\n",
    "\n",
    "    ai_policy_sections = analyze_ai_policy(paragraphs)\n",
    "    if not ai_policy_sections:\n",
    "        print(\"\\n--- No AI policy paragraphs were found in this document. ---\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n--- SUCCESS: Found {len(ai_policy_sections)} AI Policy Paragraph(s) ---\\n\")\n",
    "    unique_policies = {p['text']: p for p in ai_policy_sections}.values()\n",
    "    for i, policy in enumerate(unique_policies, 1):\n",
    "        print(f\"--- Relevant Paragraph {i} ---\"); print(policy['text']); print(f\"(Reason: {policy['reason']})\\n\")\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 5: EXECUTION\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    docx_file1 = \"8C2WFwQPrq6vcFU0Yml3vHcL1CYb5HnYcxp5s7A5.docx\" \n",
    "    analyze_syllabus(docx_file1)\n",
    "    print(\"\\n\" * 3)\n",
    "\n",
    "    docx_file2 = \"s5NpVVwHUlH3SB7MbcSmcgkfuHK796YdA4F94ZQd.docx\"\n",
    "    analyze_syllabus(docx_file2)\n",
    "    print(\"\\n\" * 3) \n",
    "\n",
    "    pdf_file = \"0Fslu7lGZ8dJG0OYQGf1TgzFMPyFDEv0n5Q96BNq.pdf\" \n",
    "    analyze_syllabus(pdf_file)\n",
    "    print(\"\\n\" * 3)\n",
    "\n",
    "    doc_file = \"7Y6H93I4L6F5bcplrUpPbf5AjQcypbJfrevTSiNf.doc\"\n",
    "    analyze_syllabus(doc_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff7f4bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
