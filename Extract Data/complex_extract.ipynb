{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57049977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from docx import Document\n",
    "import pdfplumber\n",
    "import ollama\n",
    "\n",
    "# --- For Windows users... ---\n",
    "if sys.platform == 'win32':\n",
    "    try: import win32com.client as win32\n",
    "    except ImportError: print(\"Warning: .doc support disabled.\"); win32 = None\n",
    "else: win32 = None\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 1: DEFINITIONS\n",
    "# ==============================================================================\n",
    "AI_CONTEXT_WORDS = ['ai', 'artificial intelligence', 'generative', 'chatgpt', 'llm', 'copilot', 'bard', 'large language model', 'gemini', 'dall-e', 'gpt', 'midjourney', 'stable diffusion', 'ai tool', 'ai tools']\n",
    "POLICY_KEYWORDS = ['academic integrity', 'academic dishonesty', 'plagiarism', 'cheating', 'unauthorized', 'unauthorized use', 'unauthorized assistance', 'citation', 'cite', 'attribution', 'acknowledge', 'permitted', 'allowed', 'prohibited', 'forbidden', 'disclosure', 'ethical use', 'responsible use', 'use', 'using', 'assistance', 'help', 'guidance', 'policy', 'rule']\n",
    "DEPARTMENT_MAP = {'Economics': 'ECON', 'Computer Science': 'CSCI', 'Applied Mathematics': 'APMA','Public Health': 'PHP', 'Africana Studies': 'AFRI','Cognitive, Linguistic, and Psychological Sciences': 'CLPS',}\n",
    "\n",
    "HEADER_PATTERN = re.compile(\n",
    "    r'.*\\b('\n",
    "    r'(ai|artificial\\sintelligence|generative\\s+ai)\\s+.*\\bpolicy'\n",
    "    r'|'\n",
    "    r'policy\\s+.*\\b(ai|artificial\\sintelligence|generative\\s+ai)'\n",
    "    r')\\b.*',\n",
    "    re.IGNORECASE\n",
    ")\n",
    "AI_TRIGGER_PATTERN = re.compile(r'\\b(' + '|'.join(AI_CONTEXT_WORDS) + r')\\b', re.IGNORECASE)\n",
    "POLICY_PATTERN = re.compile(r'\\b(' + '|'.join(POLICY_KEYWORDS) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 2: TEXT EXTRACTION \n",
    "# ==============================================================================\n",
    "\n",
    "def extract_paragraphs_from_doc(doc_path):\n",
    "    if not win32:\n",
    "        print(f\"Skipping .doc file '{os.path.basename(doc_path)}' as 'pywin32' is not available on this system.\")\n",
    "        return []\n",
    "    word = None; doc = None\n",
    "    try:\n",
    "        word = win32.Dispatch(\"Word.Application\")\n",
    "        word.Visible = False\n",
    "        doc = word.Documents.Open(os.path.abspath(doc_path))\n",
    "        return [p.Range.Text.strip() for p in doc.Paragraphs if p.Range.Text.strip()]\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing .doc file with MS Word: {e}\")\n",
    "        return []\n",
    "    finally:\n",
    "        if doc: doc.Close(False)\n",
    "        if word: word.Quit()\n",
    "\n",
    "def extract_paragraphs_from_docx(doc_path):\n",
    "    try:\n",
    "        doc = Document(doc_path)\n",
    "        blocks = [p.text for p in doc.paragraphs if p.text.strip()]\n",
    "        for table in doc.tables:\n",
    "            for row in table.rows:\n",
    "                for cell in row.cells:\n",
    "                    if cell.text.strip():\n",
    "                        blocks.append(cell.text)\n",
    "        return blocks\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading DOCX file {doc_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def _reconstruct_paragraphs_from_page(page):\n",
    "    words = page.extract_words(keep_blank_chars=False, x_tolerance=2)\n",
    "    if not words: return []\n",
    "    \n",
    "    lines = {}\n",
    "    for word in words:\n",
    "        line_top = round(word['top'], 2)\n",
    "        if line_top not in lines:\n",
    "            lines[line_top] = []\n",
    "        lines[line_top].append(word)\n",
    "\n",
    "    for line_top in lines:\n",
    "        lines[line_top].sort(key=lambda w: w['x0'])\n",
    "        \n",
    "    sorted_lines = sorted(lines.items(), key=lambda item: item[0])\n",
    "    \n",
    "    reconstructed_lines = []\n",
    "    line_heights = []\n",
    "    last_top = None\n",
    "    for top, words_in_line in sorted_lines:\n",
    "        text = \" \".join(w['text'] for w in words_in_line)\n",
    "        reconstructed_lines.append({'top': top, 'text': text})\n",
    "        if last_top is not None:\n",
    "            line_heights.append(top - last_top)\n",
    "        last_top = top\n",
    "\n",
    "    if not reconstructed_lines: return []\n",
    "\n",
    "    avg_line_height = sum(line_heights) / len(line_heights) if line_heights else 12\n",
    "    paragraph_break_threshold = avg_line_height * 1.5\n",
    "    \n",
    "    page_paragraphs = []\n",
    "    current_paragraph = reconstructed_lines[0]['text']\n",
    "    for i in range(1, len(reconstructed_lines)):\n",
    "        prev_line, curr_line = reconstructed_lines[i-1], reconstructed_lines[i]\n",
    "        if (curr_line['top'] - prev_line['top']) > paragraph_break_threshold:\n",
    "            page_paragraphs.append(current_paragraph)\n",
    "            current_paragraph = curr_line['text']\n",
    "        else:\n",
    "            current_paragraph += \" \" + curr_line['text']\n",
    "    page_paragraphs.append(current_paragraph)\n",
    "    \n",
    "    return page_paragraphs\n",
    "\n",
    "def extract_paragraphs_from_pdf(pdf_path):\n",
    "    all_paragraphs = []\n",
    "    carry_over_paragraph = \"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page_num, page in enumerate(pdf.pages):\n",
    "                paragraphs_on_page = _reconstruct_paragraphs_from_page(page)\n",
    "                if not paragraphs_on_page: continue\n",
    "                if carry_over_paragraph:\n",
    "                    paragraphs_on_page[0] = carry_over_paragraph + \" \" + paragraphs_on_page[0]\n",
    "                    carry_over_paragraph = \"\"\n",
    "                last_para = paragraphs_on_page[-1]\n",
    "                if not last_para.strip().endswith(('.', '?', '!', '\"', \"'\", ')', ':', ';')):\n",
    "                    carry_over_paragraph = paragraphs_on_page.pop()\n",
    "                all_paragraphs.extend(paragraphs_on_page)\n",
    "        if carry_over_paragraph:\n",
    "            all_paragraphs.append(carry_over_paragraph)\n",
    "        print(\"Successfully extracted paragraphs using coordinate-based method.\")\n",
    "        return all_paragraphs\n",
    "    except Exception as e:\n",
    "        print(f\"Coordinate-based parsing failed: {e}. No OCR fallback implemented.\")\n",
    "        return []\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 3: ANALYSIS LOGIC\n",
    "# ==============================================================================\n",
    "\n",
    "def find_course_code_rule_based(paragraphs, search_limit=30):\n",
    "    for long_name, short_code in DEPARTMENT_MAP.items():\n",
    "        pattern = re.compile(r'\\b' + re.escape(long_name) + r'\\s*(\\d{3,4}[A-Z]?)\\b', re.IGNORECASE)\n",
    "        for para in paragraphs[:search_limit]:\n",
    "            if match := pattern.search(para):\n",
    "                return f\"{short_code} {match.group(1)}\"\n",
    "\n",
    "    SEMESTER_TERMS = ['FALL', 'SPRING', 'WINTER', 'SUMMER']\n",
    "    exclude_pattern = '|'.join(SEMESTER_TERMS)\n",
    "    \n",
    "    fallback_pattern = re.compile(\n",
    "        r'\\b((?!' + exclude_pattern + r'\\b)[A-Z]{2,4}(\\s*/\\s*[A-Z]{2,4})*)\\s*(\\d{3,4}[A-Z]?)\\b'\n",
    "    )\n",
    "    for para in paragraphs[:search_limit]:\n",
    "        if match := fallback_pattern.search(para.upper()):\n",
    "            original_match_text = para[match.start():match.end()]\n",
    "            sub_match = fallback_pattern.match(original_match_text)\n",
    "            if sub_match:\n",
    "                 dept_part = sub_match.group(1).replace(\" \", \"\")\n",
    "                 num_part = sub_match.group(3)\n",
    "                 return f\"{dept_part} {num_part}\"\n",
    "    return None\n",
    "\n",
    "def _normalize_for_comparison(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts text to a 'canonical' form for robust comparison by lowercasing\n",
    "    and removing all non-alphanumeric characters.\n",
    "    \"\"\"\n",
    "    return \"\".join(char.lower() for char in text if char.isalnum())\n",
    "\n",
    "def refine_policy_with_ollama(context_block, model_name=\"deepseek-r1:14b\"):\n",
    "    \"\"\"\n",
    "    Asks the LLM to extract the core policy with retries, progressive prompting,\n",
    "    and robust, normalized verbatim quote verification.\n",
    "    Returns:\n",
    "        tuple: (result, flags)\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Handing off to Ollama model '{model_name}' to refine the policy block... ---\")\n",
    "    \n",
    "    SEPARATOR_START = \"[---POLICY_TEXT_START---]\"\n",
    "    SEPARATOR_END = \"[---POLICY_TEXT_END---]\"\n",
    "    MAX_RETRIES = 3\n",
    "\n",
    "    prompts = [\n",
    "        f\"\"\"Analyze the syllabus section to find the policy on \"Artificial Intelligence\".\n",
    "Instructions:\n",
    "1. Provide a brief, one-sentence explanation.\n",
    "2. Output the start separator: {SEPARATOR_START}\n",
    "3. Quote the complete AI policy verbatim.\n",
    "4. Output the end separator: {SEPARATOR_END}\n",
    "If no policy is found, respond ONLY with \"None\".\n",
    "Syllabus Section:\\n---\\n{context_block}\\n---\"\"\",\n",
    "\n",
    "        f\"\"\"RE-EVALUATION: The previous analysis was likely wrong. The text IS KNOWN to contain a policy on \"Artificial Intelligence\". Locate and extract it.\n",
    "Instructions:\n",
    "1. Explain your corrected finding in one sentence.\n",
    "2. Output the start separator: {SEPARATOR_START}\n",
    "3. Quote the policy verbatim. DO NOT summarize.\n",
    "4. Output the end separator: {SEPARATOR_END}\n",
    "Do not respond \"None\". Find the policy.\n",
    "Syllabus Section:\\n---\\n{context_block}\\n---\"\"\",\n",
    "\n",
    "        f\"\"\"FINAL ATTEMPT: You MUST extract the policy about \"Artificial Intelligence\". It is there. Your task is to EXTRACT it, not to decide if it exists.\n",
    "Instructions:\n",
    "1. Find the rules for using AI.\n",
    "2. Output: {SEPARATOR_START}\n",
    "3. Copy the paragraph(s) exactly.\n",
    "4. Output: {SEPARATOR_END}\n",
    "Extract it now.\n",
    "Syllabus Section:\\n---\\n{context_block}\\n---\"\"\"\n",
    "    ]\n",
    "    \n",
    "    normalized_context = _normalize_for_comparison(context_block)\n",
    "\n",
    "    try:\n",
    "        for attempt in range(MAX_RETRIES):\n",
    "            current_prompt = prompts[attempt]\n",
    "            print(f\"INFO: Attempt {attempt + 1} of {MAX_RETRIES} to query LLM...\")\n",
    "            \n",
    "            response = ollama.chat(model=model_name, messages=[{'role': 'user', 'content': current_prompt}])\n",
    "            llm_response = response['message']['content'].strip()\n",
    "\n",
    "            if (llm_response.strip().upper() == \"NONE\"):\n",
    "                print(f\"WARN: Attempt {attempt + 1} failed. LLM responded 'None'.\")\n",
    "                if attempt < MAX_RETRIES - 1: time.sleep(1)\n",
    "                continue\n",
    "\n",
    "            if SEPARATOR_START in llm_response and SEPARATOR_END in llm_response:\n",
    "                after_start = llm_response.split(SEPARATOR_START, 1)[1]\n",
    "                policy_text = after_start.split(SEPARATOR_END, 1)[0].strip()\n",
    "                \n",
    "                if not policy_text:\n",
    "                     print(f\"WARN: Attempt {attempt + 1} failed. LLM returned an empty policy.\")\n",
    "                     if attempt < MAX_RETRIES - 1: time.sleep(1)\n",
    "                     continue\n",
    "\n",
    "                normalized_policy = _normalize_for_comparison(policy_text)\n",
    "                if normalized_policy not in normalized_context:\n",
    "                    print(f\"WARN: Attempt {attempt + 1} failed. Normalized quote not in context (likely a summary).\")\n",
    "                    if attempt < MAX_RETRIES - 1: time.sleep(1)\n",
    "                    continue\n",
    "\n",
    "                print(\"INFO: LLM refinement successful and passed verification.\")\n",
    "                result = [{'text': policy_text, 'reason': f'Refined by {model_name} (Attempt {attempt+1})'}]\n",
    "                return (result, [])\n",
    "            else:\n",
    "                print(f\"WARN: Attempt {attempt + 1} failed. LLM did not use required separators.\")\n",
    "                if attempt < MAX_RETRIES - 1: time.sleep(1)\n",
    "        \n",
    "        return ([], ['LLM_FINAL_FAILURE'])\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"--- ERROR: Could not connect to Ollama: {e} ---\")\n",
    "        return ([], ['LLM_CONNECTION_ERROR'])\n",
    "\n",
    "def analyze_policy_with_clustering(paragraphs):\n",
    "    \"\"\"\n",
    "    Clusters mentions, finds the best block using a two-tier scoring system,\n",
    "    and returns it along with a flag if multiple clusters were found.\n",
    "    Returns:\n",
    "        tuple: (result, flags)\n",
    "    \"\"\"\n",
    "    flags = []\n",
    "    triggers = []\n",
    "    \n",
    "    for i, para in enumerate(paragraphs):\n",
    "        is_header = HEADER_PATTERN.match(para) and len(para.split()) < 15\n",
    "        contains_ai = AI_TRIGGER_PATTERN.search(para)\n",
    "        contains_policy = POLICY_PATTERN.search(para)\n",
    "        if is_header:\n",
    "            triggers.append({'index': i, 'type': 'header', 'weight': 10}) \n",
    "        elif contains_ai and contains_policy:\n",
    "            triggers.append({'index': i, 'type': 'strong_mention', 'weight': 3})\n",
    "        elif contains_ai:\n",
    "            triggers.append({'index': i, 'type': 'weak_mention', 'weight': 1})\n",
    "    \n",
    "    if not triggers:\n",
    "        return ([], [])\n",
    "\n",
    "    clusters = []\n",
    "    if triggers:\n",
    "        current_cluster = [triggers[0]]\n",
    "        for i in range(1, len(triggers)):\n",
    "            if triggers[i]['index'] - current_cluster[-1]['index'] <= 3:\n",
    "                current_cluster.append(triggers[i])\n",
    "            else:\n",
    "                clusters.append(current_cluster)\n",
    "                current_cluster = [triggers[i]]\n",
    "        clusters.append(current_cluster)\n",
    "\n",
    "    if len(clusters) > 1:\n",
    "        print(f\"INFO: Found {len(clusters)} distinct AI-related clusters. Flagging for review.\")\n",
    "        flags.append('MULTIPLE_CLUSTERS')\n",
    "    \n",
    "    # === MODIFICATION START: Two-tier scoring system ===\n",
    "    best_cluster_info = {'score': -1, 'policy_density': -1, 'block': []}\n",
    "    \n",
    "    for cluster in clusters:\n",
    "        min_index = min(t['index'] for t in cluster)\n",
    "        max_index = max(t['index'] for t in cluster)\n",
    "        \n",
    "        # --- Primary Score Calculation ---\n",
    "        has_header = any(t['type'] == 'header' for t in cluster) or (min_index > 0 and HEADER_PATTERN.match(paragraphs[min_index - 1]))\n",
    "        score = sum(t['weight'] for t in cluster) + (20 if has_header else 0)\n",
    "        \n",
    "        # --- Tie-breaker (Secondary Score) Calculation ---\n",
    "        start_idx_for_text = min_index - 1 if has_header and not any(t['type'] == 'header' for t in cluster) else min_index\n",
    "        end_idx_for_text = min(len(paragraphs), max_index + 1)\n",
    "        cluster_block_text = \"\\n\\n\".join(paragraphs[start_idx_for_text:end_idx_for_text])\n",
    "        num_policy_words = len(POLICY_PATTERN.findall(cluster_block_text))\n",
    "        policy_density = num_policy_words / (len(cluster_block_text.split()) + 1e-6) # Add epsilon to avoid division by zero\n",
    "        \n",
    "        # --- Update Best Cluster based on two-tier logic ---\n",
    "        is_best = False\n",
    "        if score > best_cluster_info['score']:\n",
    "            is_best = True\n",
    "        elif score == best_cluster_info['score'] and policy_density > best_cluster_info['policy_density']:\n",
    "            print(f\"INFO: Tie-breaker activated. New cluster with density {policy_density:.4f} is better than previous {best_cluster_info['policy_density']:.4f}.\")\n",
    "            is_best = True\n",
    "        \n",
    "        if is_best:\n",
    "            start_index = start_idx_for_text\n",
    "            end_index = end_idx_for_text\n",
    "            while end_index < len(paragraphs) and len(paragraphs[end_index].split()) > 5:\n",
    "                end_index += 1\n",
    "                \n",
    "            best_cluster_info = {\n",
    "                'score': score, \n",
    "                'policy_density': policy_density, \n",
    "                'block': paragraphs[start_index:end_index]\n",
    "            }\n",
    "    # === MODIFICATION END ===\n",
    "\n",
    "    if best_cluster_info['block']:\n",
    "        final_text = \"\\n\\n\".join(best_cluster_info['block'])\n",
    "        # Add policy_density to the reason for better debugging\n",
    "        reason_str = (\n",
    "            f\"Clustered Policy Block (Score: {best_cluster_info['score']}, \"\n",
    "            f\"Density: {best_cluster_info['policy_density']:.4f})\"\n",
    "        )\n",
    "        result = [{'text': final_text, 'reason': reason_str}]\n",
    "        return (result, flags)\n",
    "\n",
    "    return ([], flags)\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 4: MAIN CONTROLLER \n",
    "# ==============================================================================\n",
    "def analyze_syllabus(file_path):\n",
    "    \"\"\"\n",
    "    Analyzes a syllabus and returns a structured list with the result and binary flags.\n",
    "    \n",
    "    Returns:\n",
    "        list: [course_code, policy_text, flag_multiple_clusters, flag_llm_failure]\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*20} Analyzing Syllabus: {os.path.basename(file_path)} {'='*20}\")\n",
    "    \n",
    "    flag_multiple_clusters = 0\n",
    "    flag_llm_failure = 0\n",
    "\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    paragraphs = []\n",
    "    if ext == '.doc': paragraphs = extract_paragraphs_from_doc(file_path)\n",
    "    elif ext == '.docx': paragraphs = extract_paragraphs_from_docx(file_path)\n",
    "    elif ext == '.pdf': paragraphs = extract_paragraphs_from_pdf(file_path)\n",
    "    else:\n",
    "        print(f\"Error: Unsupported file type '{ext}'.\")\n",
    "        return [None, None, 0, 0]\n",
    "\n",
    "    if not paragraphs:\n",
    "        print(\"Could not extract any usable text.\")\n",
    "        return [None, None, 0, 0]\n",
    "    \n",
    "    print(f\"Extracted {len(paragraphs)} distinct paragraphs. Analyzing with clustering engine...\")\n",
    "    course_code = find_course_code_rule_based(paragraphs)\n",
    "    \n",
    "    ai_policy_sections, cluster_flags = analyze_policy_with_clustering(paragraphs)\n",
    "    if 'MULTIPLE_CLUSTERS' in cluster_flags:\n",
    "        flag_multiple_clusters = 1\n",
    "\n",
    "    policy_text = None\n",
    "    if ai_policy_sections:\n",
    "        policy_block = ai_policy_sections[0]['text']\n",
    "        max_word_count = 150\n",
    "\n",
    "        if len(policy_block.split()) > max_word_count:\n",
    "            print(f\"INFO: Policy block is long ({len(policy_block.split())} words). Engaging LLM.\")\n",
    "            refined_sections, llm_flags = refine_policy_with_ollama(policy_block)\n",
    "\n",
    "            if llm_flags: # This means the list is not empty, indicating a failure\n",
    "                flag_llm_failure = 1\n",
    "                policy_text = None \n",
    "            else:\n",
    "                if refined_sections:\n",
    "                    policy_text = refined_sections[0]['text']\n",
    "        else:\n",
    "            print(\"INFO: Clustered policy block passed quality checks.\")\n",
    "            policy_text = ai_policy_sections[0]['text']\n",
    "\n",
    "    print(\"--- Analysis processing complete. ---\")\n",
    "    return [course_code, policy_text, flag_multiple_clusters, flag_llm_failure]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1af97ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file = \"0GIzUorcaUlFtsvHBQmWIgLxidFQogW1Q3jaQJlX.pdf\" \n",
    "analyze_syllabus(pdf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39be1cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file = \"DIoCCJg4XeH5rJ0HuN9MqZiRNenyFUxa121Il04r.pdf\" \n",
    "analyze_syllabus(pdf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d2c154",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file = \"jnlQIFYUgZhHZIZyILiFAEr02KEuyWJYkquXUvJD.pdf\" \n",
    "analyze_syllabus(pdf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1992a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file = \"PeB4sO9tTTZxAYrxncoLPKoAIOtRa1ew5spr4lCw.pdf\" \n",
    "analyze_syllabus(pdf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2381482f",
   "metadata": {},
   "outputs": [],
   "source": [
    "docx_file = \"HNPK3m8hfi0bcJ1sSa2LHeMNQ3DAxjJBXkSTtomA.docx\" \n",
    "analyze_syllabus(docx_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56a5c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "docx_file = \"T8hj3wlKrzFUjYtz61tDDFxzIIW5B7MFAdgNzy4X.docx\" \n",
    "analyze_syllabus(docx_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3328ea2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_clusters_in_file(file_path):\n",
    "    \"\"\"\n",
    "    An independent function to analyze a file, find all AI-related text clusters,\n",
    "    and print them with their scores for debugging and inspection.\n",
    "    This function does NOT proceed to LLM refinement.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*20} Debugging Clusters for: {os.path.basename(file_path)} {'='*20}\")\n",
    "    \n",
    "    # --- Step 1: Extract Text  \n",
    "    if not os.path.exists(file_path):\n",
    "        print(\"Error: File not found.\")\n",
    "        return\n",
    "\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    paragraphs = []\n",
    "    if ext == '.doc': paragraphs = extract_paragraphs_from_doc(file_path)\n",
    "    elif ext == '.docx': paragraphs = extract_paragraphs_from_docx(file_path)\n",
    "    elif ext == '.pdf': paragraphs = extract_paragraphs_from_pdf(file_path)\n",
    "    else:\n",
    "        print(f\"Error: Unsupported file type '{ext}'.\")\n",
    "        return\n",
    "\n",
    "    if not paragraphs:\n",
    "        print(\"Could not extract any usable text from the file.\")\n",
    "        return\n",
    "        \n",
    "    print(f\"Successfully extracted {len(paragraphs)} paragraphs.\")\n",
    "\n",
    "    # --- Step 2: Find all clusters and their details \n",
    "    triggers = []\n",
    "    for i, para in enumerate(paragraphs):\n",
    "        is_header = HEADER_PATTERN.match(para) and len(para.split()) < 15\n",
    "        contains_ai = AI_TRIGGER_PATTERN.search(para)\n",
    "        contains_policy = POLICY_PATTERN.search(para)\n",
    "        if is_header:\n",
    "            triggers.append({'index': i, 'type': 'header', 'weight': 10}) \n",
    "        elif contains_ai and contains_policy:\n",
    "            triggers.append({'index': i, 'type': 'strong_mention', 'weight': 3})\n",
    "        elif contains_ai:\n",
    "            triggers.append({'index': i, 'type': 'weak_mention', 'weight': 1})\n",
    "    \n",
    "    if not triggers:\n",
    "        print(\"\\n--- No AI-related triggers found in this document. ---\")\n",
    "        return\n",
    "\n",
    "    clusters = []\n",
    "    if triggers:\n",
    "        current_cluster = [triggers[0]]\n",
    "        for i in range(1, len(triggers)):\n",
    "            if triggers[i]['index'] - current_cluster[-1]['index'] <= 3:\n",
    "                current_cluster.append(triggers[i])\n",
    "            else:\n",
    "                clusters.append(current_cluster)\n",
    "                current_cluster = [triggers[i]]\n",
    "        clusters.append(current_cluster)\n",
    "        \n",
    "    if not clusters:\n",
    "        print(\"\\n--- Could not form any clusters from the triggers. ---\")\n",
    "        return\n",
    "\n",
    "    # --- Step 3: Calculate scores and format for printing\n",
    "    all_clusters_details = []\n",
    "    for cluster in clusters:\n",
    "        min_index = min(t['index'] for t in cluster)\n",
    "        max_index = max(t['index'] for t in cluster)\n",
    "        has_header = any(t['type'] == 'header' for t in cluster) or (min_index > 0 and HEADER_PATTERN.match(paragraphs[min_index - 1]))\n",
    "        score = sum(t['weight'] for t in cluster) + (20 if has_header else 0)\n",
    "        \n",
    "        start_index = min_index - 1 if has_header and not any(t['type'] == 'header' for t in cluster) else min_index\n",
    "        end_index = min(len(paragraphs), max_index + 1)\n",
    "        while end_index < len(paragraphs) and len(paragraphs[end_index].split()) > 5:\n",
    "            end_index += 1\n",
    "        \n",
    "        cluster_text = \"\\n\\n\".join(paragraphs[start_index:end_index])\n",
    "        all_clusters_details.append({'score': score, 'text': cluster_text})\n",
    "\n",
    "    # --- Step 4: Print the results beautifully \n",
    "    print(f\"\\n--- Found {len(all_clusters_details)} AI-related cluster(s). Details below: ---\")\n",
    "    sorted_clusters = sorted(all_clusters_details, key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    for i, cluster_info in enumerate(sorted_clusters, 1):\n",
    "        print(f\"\\n--- Cluster #{i} (Score: {cluster_info['score']}) ---\")\n",
    "        print(cluster_info['text'])\n",
    "        print(\"-\" * (25 + len(str(i)) + len(str(cluster_info['score']))))\n",
    "\n",
    "\n",
    "pdf_file = \"0GIzUorcaUlFtsvHBQmWIgLxidFQogW1Q3jaQJlX.pdf\" \n",
    "debug_clusters_in_file(pdf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb77490",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file = \"DIoCCJg4XeH5rJ0HuN9MqZiRNenyFUxa121Il04r.pdf\" \n",
    "debug_clusters_in_file(pdf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2773cf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file = \"jnlQIFYUgZhHZIZyILiFAEr02KEuyWJYkquXUvJD.pdf\" \n",
    "debug_clusters_in_file(pdf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b472f31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
