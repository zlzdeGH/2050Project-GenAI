{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeef911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1480"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import ollama\n",
    "from tqdm.auto import tqdm\n",
    "import glob\n",
    "from collections import Counter\n",
    "\n",
    "df = pd.read_csv('new_final_2024.csv')\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c998db19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Software\\Anaconda\\envs\\genai\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "OLLAMA_MODEL = \"gpt-oss:20b\" \n",
    "\n",
    "\n",
    "POLICY_CATEGORIES = [\n",
    "    \"Permissive\",         # Explicitly encourages or allows wide AI use.\n",
    "    \"Guideline-Based\",    # Allows AI use but requires adherence to strict citation/disclosure rules.\n",
    "    \"Restrictive\",        # Strictly limits or completely prohibits AI use for assignments.\n",
    "    \"Case-by-Case\",       # Defers the policy to the instructor on a per-assignment basis.\n",
    "    \"Ambiguous\",          # Mentions AI but the rules are unclear or vague.\n",
    "    \"No Policy Found\",    # Use this for empty/irrelevant text.\n",
    "    \"Analysis Error\"      # Use this if the model fails.\n",
    "]\n",
    "\n",
    "def create_prompt(policy_text):\n",
    "    \"\"\"Creates a clear, concise prompt for the LLM.\"\"\"\n",
    "    return f\"\"\"\n",
    "Analyze the following academic AI policy and classify it into ONE of the following categories:\n",
    "\n",
    "- Permissive: Explicitly encourages or allows wide AI use.\n",
    "- Guideline-Based: Allows AI use but requires adherence to strict citation/disclosure rules.\n",
    "- Restrictive: Strictly limits or completely prohibits AI use for assignments.\n",
    "- Case-by-Case: Defers the policy to the instructor on a per-assignment basis.\n",
    "- Ambiguous: Mentions AI but the rules are unclear or vague.\n",
    "\n",
    "Your response MUST BE ONLY ONE of the category labels listed above. Do not add explanations.\n",
    "\n",
    "--- POLICY TEXT ---\n",
    "{policy_text}\n",
    "--- END OF TEXT ---\n",
    "\n",
    "Category:\"\"\"\n",
    "\n",
    "\n",
    "def analyze_policy(policy_text):\n",
    "    \"\"\"\n",
    "    Sends the policy text to Ollama and returns a single category label.\n",
    "    \"\"\"\n",
    "    if not isinstance(policy_text, str) or not policy_text.strip():\n",
    "        return \"No Policy Found\"\n",
    "\n",
    "    prompt = create_prompt(policy_text)\n",
    "    \n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=OLLAMA_MODEL,\n",
    "            messages=[{'role': 'user', 'content': prompt}]\n",
    "        )\n",
    "        # Extract the model's response and clean it up.\n",
    "        category = response['message']['content'].strip().replace(\"'\", \"\").replace('\"', '')\n",
    "\n",
    "        # Validate that the model's response is one of our expected categories\n",
    "        if category in POLICY_CATEGORIES:\n",
    "            return category\n",
    "        else:\n",
    "            print(f\"\\nWarning: Model returned an unexpected category: '{category}'. Marking as Ambiguous.\")\n",
    "            return \"Ambiguous\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR: Could not connect to or get a valid response from Ollama. Reason: {e}\")\n",
    "        return \"Analysis Error\"\n",
    "\n",
    "def process_csv_with_ollama(csv_path, out_suffix='_with_sentiment_1.csv'):\n",
    "    \"\"\"\n",
    "    Reads a CSV, analyzes the 'AI Policy' column using an LLM, adds the\n",
    "    results, and saves to a NEW file.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Processing file: {os.path.basename(csv_path)} ---\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  ERROR: File not found. Skipping.\")\n",
    "        return\n",
    "\n",
    "    if 'AI Policy' not in df.columns:\n",
    "        print(\"  ERROR: 'AI Policy' column not found. Skipping.\")\n",
    "        return\n",
    "    \n",
    "    # Identify which rows actually need processing\n",
    "    rows_to_process = df[df['AI Policy'].notna() & (df['AI Policy'].str.strip() != '')]\n",
    "    \n",
    "    if rows_to_process.empty:\n",
    "        print(\"  INFO: No AI policies found to analyze in this file.\")\n",
    "        return\n",
    "\n",
    "    print(f\"  Found {len(rows_to_process)} policies to analyze. Starting analysis...\")\n",
    "    \n",
    "    # Initialize the new column with a default value\n",
    "    df['Policy Category'] = \"No Policy Found\"\n",
    "    \n",
    "    # Use tqdm for a progress bar as this will be slow\n",
    "    for index, row in tqdm(rows_to_process.iterrows(), total=len(rows_to_process), desc=\"Analyzing Policies\"):\n",
    "        policy_text = row['AI Policy']\n",
    "        category = analyze_policy(policy_text)\n",
    "        # Use .at for efficient cell setting\n",
    "        df.at[index, 'Policy Category'] = category\n",
    "    \n",
    "    # --- Save to a NEW file for safety ---\n",
    "    output_path = csv_path.replace(\".csv\", out_suffix)\n",
    "    try:\n",
    "        df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\n  SUCCESS: Analysis complete. Results saved to:\\n  {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n  ERROR: Could not save the updated file. Reason: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472b2fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Starting AI Policy Analysis with Ollama (gpt-oss:20b)\n",
      "==============================\n",
      "\n",
      "--- Processing file: new_final_2024.csv ---\n",
      "  Found 510 policies to analyze. Starting analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Policies: 100%|██████████| 510/510 [2:57:20<00:00, 20.86s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  SUCCESS: Analysis complete. Results saved to:\n",
      "  new_final_2024_with_sentiment_1.csv\n",
      "\n",
      "==============================\n",
      "Analysis script finished.\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    print(f\"{'='*30}\\nStarting AI Policy Analysis with Ollama ({OLLAMA_MODEL})\\n{'='*30}\")\n",
    "\n",
    "    target_csv_file = 'new_final_2024.csv'\n",
    "    process_csv_with_ollama(target_csv_file)\n",
    "        \n",
    "    print(f\"\\n{'='*30}\\nAnalysis script finished.\\n{'='*30}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b78381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Starting AI Policy Analysis with Ollama (gpt-oss:20b)\n",
      "==============================\n",
      "\n",
      "--- Processing file: new_final_2024.csv ---\n",
      "  Found 510 policies to analyze. Starting analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Policies: 100%|██████████| 510/510 [2:57:03<00:00, 20.83s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  SUCCESS: Analysis complete. Results saved to:\n",
      "  new_final_2024_with_sentiment_2.csv\n",
      "\n",
      "==============================\n",
      "Analysis script finished.\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    print(f\"{'='*30}\\nStarting AI Policy Analysis with Ollama ({OLLAMA_MODEL})\\n{'='*30}\")\n",
    "\n",
    "    target_csv_file = 'new_final_2024.csv'\n",
    "    process_csv_with_ollama(target_csv_file,out_suffix='_with_sentiment_2.csv')\n",
    "        \n",
    "    print(f\"\\n{'='*30}\\nAnalysis script finished.\\n{'='*30}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ec404a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Starting AI Policy Analysis with Ollama (gpt-oss:20b)\n",
      "==============================\n",
      "\n",
      "--- Processing file: new_final_2024.csv ---\n",
      "  Found 510 policies to analyze. Starting analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Policies: 100%|██████████| 510/510 [2:54:32<00:00, 20.53s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  SUCCESS: Analysis complete. Results saved to:\n",
      "  new_final_2024_with_sentiment_3.csv\n",
      "\n",
      "==============================\n",
      "Analysis script finished.\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    print(f\"{'='*30}\\nStarting AI Policy Analysis with Ollama ({OLLAMA_MODEL})\\n{'='*30}\")\n",
    "\n",
    "    target_csv_file = 'new_final_2024.csv'\n",
    "    process_csv_with_ollama(target_csv_file,out_suffix='_with_sentiment_3.csv')\n",
    "        \n",
    "    print(f\"\\n{'='*30}\\nAnalysis script finished.\\n{'='*30}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a376f6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Starting AI Policy Analysis with Ollama (gpt-oss:20b)\n",
      "==============================\n",
      "\n",
      "--- Processing file: new_final_2024.csv ---\n",
      "  Found 510 policies to analyze. Starting analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Policies: 100%|██████████| 510/510 [3:08:14<00:00, 22.15s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  SUCCESS: Analysis complete. Results saved to:\n",
      "  new_final_2024_with_sentiment_4.csv\n",
      "\n",
      "==============================\n",
      "Analysis script finished.\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    print(f\"{'='*30}\\nStarting AI Policy Analysis with Ollama ({OLLAMA_MODEL})\\n{'='*30}\")\n",
    "\n",
    "    target_csv_file = 'new_final_2024.csv'\n",
    "    process_csv_with_ollama(target_csv_file,out_suffix='_with_sentiment_4.csv')\n",
    "        \n",
    "    print(f\"\\n{'='*30}\\nAnalysis script finished.\\n{'='*30}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc2a184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Starting AI Policy Analysis with Ollama (gpt-oss:20b)\n",
      "==============================\n",
      "\n",
      "--- Processing file: new_final_2024.csv ---\n",
      "  Found 510 policies to analyze. Starting analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Policies: 100%|██████████| 510/510 [3:10:43<00:00, 22.44s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  SUCCESS: Analysis complete. Results saved to:\n",
      "  new_final_2024_with_sentiment_5.csv\n",
      "\n",
      "==============================\n",
      "Analysis script finished.\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    print(f\"{'='*30}\\nStarting AI Policy Analysis with Ollama ({OLLAMA_MODEL})\\n{'='*30}\")\n",
    "\n",
    "    target_csv_file = 'new_final_2024.csv'\n",
    "    process_csv_with_ollama(target_csv_file,out_suffix='_with_sentiment_5.csv')   \n",
    "      \n",
    "    print(f\"\\n{'='*30}\\nAnalysis script finished.\\n{'='*30}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551156ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Aggregating results for: new_final_2024.csv ---\n",
      "  Found 5 result files to aggregate.\n",
      "  Calculating final classifications and sentiment confidence scores...\n",
      "\n",
      "  SUCCESS: Aggregation complete. Report with new naming and order saved to:\n",
      "  new_final_2024_final_aggregated.csv\n"
     ]
    }
   ],
   "source": [
    "SENTIMENT_MAP = {\n",
    "    'Permissive': 'Positive',\n",
    "    'Guideline-Based': 'Neutral',\n",
    "    'Case-by-Case': 'Neutral',\n",
    "    'Ambiguous': 'Neutral',\n",
    "    'Restrictive': 'Negative',\n",
    "    'No Policy Found': 'No Policy', \n",
    "    'Analysis Error': 'Error'       \n",
    "}\n",
    "\n",
    "def calculate_final_sentiment(row):\n",
    "    \"\"\"\n",
    "    Analyzes the classification results from all runs for a single row.\n",
    "    Returns the mode of the classification, the mode of the broad sentiment,\n",
    "    and the confidence score for that sentiment.\n",
    "    \"\"\"\n",
    "    classifications = [row[f'Run_{i}_Result'] for i in range(1, 6)]\n",
    "    \n",
    "    sentiments = [SENTIMENT_MAP.get(cat, 'Unknown') for cat in classifications]\n",
    "    \n",
    "    if not sentiments:\n",
    "        return pd.Series(['No Policy Found', 'No Policy', 0.0], index=['Final Classification', 'Broad Sentiment', 'Sentiment Confidence'])\n",
    "    \n",
    "    sentiment_counts = Counter(sentiments)\n",
    "    final_sentiment = sentiment_counts.most_common(1)[0][0]\n",
    "    \n",
    "    confidence_score = sentiment_counts[final_sentiment] / len(sentiments)\n",
    "    \n",
    "    classification_counts = Counter(classifications)\n",
    "    final_classification = classification_counts.most_common(1)[0][0]\n",
    "\n",
    "    return pd.Series([final_classification, final_sentiment, confidence_score], \n",
    "                     index=['Final Classification', 'Broad Sentiment', 'Sentiment Confidence'])\n",
    "\n",
    "\n",
    "def aggregate_sentiment_runs(source_csv_path, run_files_pattern):\n",
    "    \"\"\"\n",
    "    Reads all run files, aggregates them, calculates sentiment confidence,\n",
    "    and creates a final summary file with clear column names and logical order.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Aggregating results for: {os.path.basename(source_csv_path)} ---\")\n",
    "    \n",
    "    run_files = sorted(glob.glob(run_files_pattern))\n",
    "    \n",
    "    if len(run_files) < 2:\n",
    "        print(f\"  ERROR: Found only {len(run_files)} result file(s). Need at least 2. Halting.\")\n",
    "        return\n",
    "        \n",
    "    print(f\"  Found {len(run_files)} result files to aggregate.\")\n",
    "    \n",
    "    try:\n",
    "        final_df = pd.read_csv(source_csv_path)\n",
    "        run_dfs = [pd.read_csv(f) for f in run_files]\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR: Could not read a required file. Reason: {e}\")\n",
    "        return\n",
    "\n",
    "    for i, file_path in enumerate(run_files):\n",
    "        final_df[f'Run_{i+1}_Result'] = run_dfs[i]['Policy Category']\n",
    "\n",
    "    print(\"  Calculating final classifications and sentiment confidence scores...\")\n",
    "    analysis_results = final_df.apply(calculate_final_sentiment, axis=1)\n",
    "    \n",
    "    final_df = final_df.join(analysis_results)\n",
    "    \n",
    "    original_cols = list(pd.read_csv(source_csv_path, nrows=0).columns)\n",
    "    \n",
    "    new_analysis_cols = ['Final Classification', 'Broad Sentiment', 'Sentiment Confidence']\n",
    "    run_cols = [f'Run_{i}_Result' for i in range(1, len(run_files) + 1)]\n",
    "\n",
    "    try:\n",
    "        policy_index = original_cols.index('AI Policy')\n",
    "        final_col_order = (original_cols[:policy_index+1] + \n",
    "                           new_analysis_cols + \n",
    "                           original_cols[policy_index+1:] +\n",
    "                           run_cols)\n",
    "    except ValueError:\n",
    "        final_col_order = original_cols + new_analysis_cols + run_cols\n",
    "\n",
    "    final_df = final_df[final_col_order]\n",
    "\n",
    "    final_output_path = source_csv_path.replace(\".csv\", \"_final_aggregated.csv\")\n",
    "    try:\n",
    "        final_df.to_csv(final_output_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\n  SUCCESS: Aggregation complete. Report with new naming and order saved to:\\n  {final_output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n  ERROR: Could not save the final aggregated file. Reason: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    original_csv_file = 'new_final_2024.csv'\n",
    "    results_pattern = 'new_final_2024_with_sentiment_*.csv'\n",
    "    \n",
    "    aggregate_sentiment_runs(original_csv_file, results_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa27aaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
